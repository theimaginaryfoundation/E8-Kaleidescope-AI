# -*- coding: utf-8 -*-
"""Monolithic E8 Mind Server - sanitized header section.

This top-of-file region was reconstructed to remove corruption: seed utilities,
optional dependency stubs, and hypothesis validation helper.
"""

from __future__ import annotations

import os, sys, math, json, time, random, re, logging, tempfile, io, glob, hashlib, contextlib, traceback, threading, faulthandler, zlib, heapq, collections
import warnings

# Provide backwards-compatibility module aliases for older entry points.
_LEGACY_MODULE_ALIASES = ('e8_mind_server_M18', 'e8_mind_server_M19')
for _alias in _LEGACY_MODULE_ALIASES:
    if __name__ in sys.modules:
        sys.modules.setdefault(_alias, sys.modules[__name__])

import asyncio
from core.config import AppConfig # Import AppConfig to access VALIDATOR_WRITEBACK_ENABLED
from core import sdi

class _TaskSlot:
    """Single-concurrency task slot with dedup + exception logging.

    Lightweight helper used to ensure only one background coroutine runs per named slot.
    """
    def __init__(self, name: str):
        self.name = name
        self.task = None

    def running(self) -> bool:
        return bool(self.task) and not self.task.done()

    def start(self, coro):
        """Start the coroutine if no task is already running; returns True if started."""
        if self.running():
            # If already running, close any pre-created coroutine to avoid leaks
            if asyncio.iscoroutine(coro):
                try:
                    coro.close()
                except Exception:
                    pass
            return False

        # If caller passed a factory (callable), invoke it to obtain the coroutine
        if callable(coro) and not asyncio.iscoroutine(coro):
            try:
                coro = coro()
            except Exception as e:
                try:
                    logging.exception(f"[TASK:{self.name}] failed to start coroutine factory: {e}")
                except Exception:
                    print(f"[TASK:{self.name}] failed to start coroutine factory: {type(e).__name__}: {e}")
                return False

        # At this point 'coro' should be a coroutine object
        try:
            t = asyncio.create_task(coro)
        except Exception:
            return False
        self.task = t

        def _done(_t): 
            try:
                _t.result()
            except Exception:
                pass

        try:
            t.add_done_callback(_done)
        except Exception:
            pass
        return True
from ehs import EventHorizonScheduler
from typing import Any, Dict, List, Optional, Tuple, Union, Iterable
import re
import random
import numpy as np  # needed by early fallback helpers
# Optional heavy numerical imports: provide safe fallbacks for environments without scipy
try:
    from scipy.spatial.distance import cdist  # type: ignore
except Exception:
    def cdist(a, b, metric='euclidean'):
        # minimal fallback: pairwise euclidean distances
        A = np.atleast_2d(np.asarray(a, dtype=float))
        B = np.atleast_2d(np.asarray(b, dtype=float))
        D = np.zeros((A.shape[0], B.shape[0]), dtype=float)
        for i in range(A.shape[0]):
            for j in range(B.shape[0]):
                D[i, j] = np.linalg.norm(A[i] - B[j])
        return D

try:
    from scipy.integrate import odeint  # type: ignore
except Exception:
    def odeint(func, y0, t, args=()):
        # Very small-step Euler integrator fallback (not a replacement for real scipy)
        y = np.asarray(y0, dtype=float)
        ys = [y.copy()]
        for i in range(1, len(t)):
            dt = t[i] - t[i-1]
            y = y + dt * np.asarray(func(y, t[i-1], *args), dtype=float)
            ys.append(y.copy())
        return np.vstack(ys)

# --- Curvature imports ---
import os, math, numpy as np
from collections import deque

try:
    from scipy.sparse import coo_matrix, diags
    from scipy.sparse.linalg import cg as scipy_cg
    _HAS_SCIPY = True
except Exception:
    _HAS_SCIPY = False

# Optional: ensure 'rich' console exists (your code likely already sets this)
try:
    from rich.console import Console
    _CURV_CONSOLE = Console(log_path=False, soft_wrap=False)
except Exception:
    _CURV_CONSOLE = None
from dataclasses import dataclass, field
from collections import deque, defaultdict
try:  # XML parsing for arXiv ingestion
    import xml.etree.ElementTree as ET  # type: ignore
except Exception:  # pragma: no cover
    ET = None  # type: ignore

# Optional suppression of noisy 3rd-party deprecation warnings (e.g., numba.generated_jit from dependencies like clifford)
if os.getenv("E8_SUPPRESS_NUMBA_DEPRECATION", "1") == "1":  # default on; set to 0 to view
    try:
        from numba.core.errors import NumbaDeprecationWarning as _NumbaDepWarn  # type: ignore
    except Exception:
        try:
            from numba import NumbaDeprecationWarning as _NumbaDepWarn  # legacy path fallback
        except Exception:
            _NumbaDepWarn = None  # type: ignore
    if _NumbaDepWarn is not None:
        warnings.filterwarnings("ignore", category=_NumbaDepWarn)

# ===================== Everywhen Wave (A-only) =====================
# Optional damped graph-wave integrator that adds a retarded component to
# curvature propagation. Disabled by default; enable with E8_EW_MODE=wave.

def _estimate_lambda_max(L):
    """
    Estimate max eigenvalue of Laplacian for CFL via power iteration.
    Supports scipy sparse CSR/CSC and dense numpy arrays. Uses ~20 iters.
    """
    try:
        # Determine dimension
        n = L.shape[0]
        if n == 0:
            return 1.0
        x = np.random.randn(n)
        x /= (np.linalg.norm(x) + 1e-12)
        for _ in range(20):
            # use helper for sparse/dense compatibility
            y = _L_dot(L, x)
            ny = float(np.linalg.norm(y)) + 1e-18
            x = y / ny
        y = _L_dot(L, x)
        lam = float(x @ y)  # Rayleigh quotient
        return max(lam, 1e-9)
    except Exception:
        # Very conservative fallback
        return 1.0


def _L_dot(L, x):
    """Safe Laplacian-times-vector for dense or sparse matrices.

    Uses .dot when available to avoid potential SciPy @ quirks; falls back to @.
    """
    try:
        if hasattr(L, 'dot'):
            return L.dot(x)
        return L @ x
    except Exception:
        # last resort
        return L @ x


class PhotonicEverywhenWave:
    """
    Damped graph wave (telegraph-like) integrator advancing a wave potential φ.
      φ_{t+1} = (2-ζ) φ_t - (1-ζ) φ_{t-1} + ν (-L φ_t) + β ρ̃_t

    Provides a blended field: α_stat * φ_stat + α_wave * φ_wave for curvature.
    """
    def __init__(self, mind, n_roots: int):
        self.mind = mind
        self.n = int(n_roots)

        # Parameters from env (A-only)
        self.c_eff      = float(os.getenv("E8_EW_WAVE_C", "0.9"))
        self.damping    = float(os.getenv("E8_EW_WAVE_DAMP", "0.08"))
        self.beta_src   = float(os.getenv("E8_EW_WAVE_BETA", "0.5"))
        self.alpha_stat = float(os.getenv("E8_EW_ALPHA_STAT", "0.5"))
        self.alpha_wave = float(os.getenv("E8_EW_ALPHA_WAVE", "0.5"))

        # State buffers
        self.phi_prev = np.zeros(self.n, dtype=float)
        self.phi_curr = np.zeros(self.n, dtype=float)

        # Stability-tuned step coefficient ν (CFL-ish bound using λ_max)
        L = self.mind.curvature_field.get_graph_laplacian() if hasattr(self.mind, 'curvature_field') and self.mind.curvature_field else None
        try:
            lam_max = _estimate_lambda_max(L) if L is not None else 1.0
        except Exception:
            lam_max = 1.0
        # Keep margin: central difference stability ~ ν λ_max < 2
        self.nu = min((self.c_eff ** 2) * 0.9, 1.5 / max(lam_max, 1e-9))
        self._lam_max_est = float(lam_max)

    def _ensure_size(self):
        """Ensure internal buffers match current number of roots."""
        n_now = len(getattr(self.mind.physics, 'roots', []) or [])
        if n_now != self.n and n_now >= 0:
            # Resize/reinit conservatively to avoid shape errors on topology changes
            self.n = int(n_now)
            self.phi_prev = np.zeros(self.n, dtype=float)
            self.phi_curr = np.zeros(self.n, dtype=float)
            # Recompute λ_max and ν for new graph
            try:
                L = self.mind.curvature_field.get_graph_laplacian() if hasattr(self.mind, 'curvature_field') and self.mind.curvature_field else None
                lam_max = _estimate_lambda_max(L) if L is not None else 1.0
            except Exception:
                lam_max = 1.0
            self._lam_max_est = float(lam_max)
            self.nu = min((self.c_eff ** 2) * 0.9, 1.5 / max(lam_max, 1e-9))

    def step(self, rho_tilde: np.ndarray):
        self._ensure_size()
        # Access latest Laplacian each step to honor cache invalidation
        L = self.mind.curvature_field.get_graph_laplacian()
        lap_phi = _L_dot(L, self.phi_curr)
        zeta = self.damping
        phi_next = (2.0 - zeta) * self.phi_curr \
                 - (1.0 - zeta) * self.phi_prev \
                 + self.nu * (-lap_phi) \
                 + self.beta_src * (rho_tilde if rho_tilde is not None else 0.0)

        # Rotate buffers
        self.phi_prev, self.phi_curr = self.phi_curr, phi_next
        return self.phi_curr

    def blend(self, phi_stat: np.ndarray):
        return self.alpha_stat * (phi_stat if phi_stat is not None else 0.0) + self.alpha_wave * self.phi_curr


class SystematicPhysicsValidator:
    """
    Systematic physics-inspired constraint tracking for E8Mind.
    
    Implements the rosetta stone mapping from physics concepts to model variables:
    - scalar field φ → curvature potential over memory graph
    - energy density ρ → semantic mass / node density  
    - inflation/expansion → growth in field energy E = φᵀLφ, geodesic dilation
    - Hubble friction → wave damping / cooldowns
    - event horizon → shell boundary / geodesic cutoff
    - gravitational collapse → BH compaction (merge + mass accounting)
    
    Enforces systematic constraints:
    1. Spectral stability: c ≲ κ/λ_max(L) (CFL-like bound)
    2. Mass conservation: injected ≈ compacted mass (±1-2% drift)
    3. Dimensionless invariants Q1, Q2, Q3 track correctness
    4. Commutation: project ∘ compact ≈ compact ∘ project (within ε)
    """
    
    def __init__(self, mind: Optional['E8Mind'] = None):
        self.mind = mind
        self.console = None
        try:
            from rich.console import Console
            original_console = Console(log_path=False)
            # Use dimmed logging for systematic physics to reduce noise
            self.console = DimmedLoggerConsole(original_console)
        except Exception:
            pass
            
        # Physics parameters from environment
        self.kappa = float(os.getenv("E8_PHYSICS_KAPPA", "0.8"))  # CFL safety margin
        self.mass_drift_tolerance = float(os.getenv("E8_PHYSICS_MASS_TOLERANCE", "0.02"))  # ±2%
        self.commutation_epsilon = float(os.getenv("E8_PHYSICS_COMMUTE_EPS", "1e-3"))
        self.inflation_rate_max = float(os.getenv("E8_PHYSICS_INFLATION_MAX", "0.1"))  # 10% per step max
        
        # State tracking
        self.lambda_max_history: deque = deque(maxlen=50)
        self.field_energy_history: deque = deque(maxlen=50)
        self.mass_injection_history: deque = deque(maxlen=100)
        self.mass_removal_history: deque = deque(maxlen=100)
        self.invariant_history: Dict[str, deque] = {
            'Q1': deque(maxlen=50),  # E/M²
            'Q2': deque(maxlen=50),  # c²λ_max/β  
            'Q3': deque(maxlen=50),  # mass_in/mass_out
        }
        
        # Control knobs
        self.auto_tune_enabled = True
        self.inflation_epoch_active = False
        self.epoch_start_time = None
        self.hubble_friction_factor = 1.0
        
        # Diagnostics
        self.stability_violations = 0
        self.mass_violations = 0
        self.commutation_violations = 0
        
    def log(self, msg: str, level: str = "info"):
        """Safe logging to console if available."""
        try:
            if self.console:
                if level == "warning":
                    self.console.log(f"[yellow][SystematicPhysics] {msg}[/yellow]")
                elif level == "error":
                    self.console.log(f"[red][SystematicPhysics] {msg}[/red]")
                else:
                    self.console.log(f"[cyan][SystematicPhysics] {msg}[/cyan]")
        except Exception:
            pass
            
    def estimate_spectral_bound(self, L=None) -> Dict[str, float]:
        """Estimate λ_max and compute CFL-like stability bounds."""
        if L is None and self.mind and hasattr(self.mind, 'curvature_field'):
            try:
                L = self.mind.curvature_field.get_graph_laplacian()
            except Exception:
                L = None
                
        if L is None:
            # Conservative fallback
            lambda_max = 1.0
            lambda_2 = 0.5
        else:
            lambda_max = _estimate_lambda_max(L)
            # Estimate second eigenvalue via deflation (rough approximation)
            try:
                n = L.shape[0]
                if n > 1:
                    # Simple deflation: project out dominant eigenvector
                    x = np.random.randn(n)
                    x /= (np.linalg.norm(x) + 1e-12)
                    for _ in range(10):
                        y = _L_dot(L, x)
                        y -= (x @ y) * x  # remove dominant component
                        ny = np.linalg.norm(y) + 1e-18
                        x = y / ny
                    y = _L_dot(L, x)
                    lambda_2 = max(0, float(x @ y))
                else:
                    lambda_2 = 0.0
            except Exception:
                lambda_2 = lambda_max * 0.5  # conservative estimate
                
        # Track history
        self.lambda_max_history.append(lambda_max)
        
        # Compute bounds
        c_max_stable = self.kappa / max(lambda_max, 1e-9)
        spectral_gap = lambda_max - lambda_2
        
        return {
            'lambda_max': lambda_max,
            'lambda_2': lambda_2,
            'spectral_gap': spectral_gap,
            'c_max_stable': c_max_stable,
            'mixing_time_est': 1.0 / max(spectral_gap, 1e-9)
        }
        
    def compute_field_energy(self, phi=None, L=None) -> float:
        """Compute field energy E = φᵀLφ."""
        if phi is None or L is None:
            return 0.0
            
        try:
            phi = np.asarray(phi, dtype=float)
            energy = float(phi @ _L_dot(L, phi))
            self.field_energy_history.append(energy)
            return energy
        except Exception:
            return 0.0
            
    def track_mass_flow(self, mass_injected: float, mass_removed: float):
        """Track semantic mass injection and removal for conservation."""
        self.mass_injection_history.append(mass_injected)
        self.mass_removal_history.append(mass_removed)
        
    def compute_inflation_rate(self) -> float:
        """Compute inflation rate r = (E_t - E_{t-1}) / E_{t-1}."""
        if len(self.field_energy_history) < 2:
            return 0.0
            
        E_curr = self.field_energy_history[-1]
        E_prev = self.field_energy_history[-2]
        
        if E_prev <= 1e-12:
            return 0.0
            
        return (E_curr - E_prev) / E_prev
        
    def compute_dimensionless_invariants(self, phi=None, L=None, c_eff=None, beta=None) -> Dict[str, float]:
        """Compute Q1, Q2, Q3 invariants to prove systematic correctness."""
        invariants = {}
        
        # Q1 = E/M² (field energy over squared mass)
        try:
            E = self.compute_field_energy(phi, L)
            M_total = sum(self.mass_injection_history) if self.mass_injection_history else 1.0
            Q1 = E / max(M_total**2, 1e-12)
            invariants['Q1'] = Q1
            self.invariant_history['Q1'].append(Q1)
        except Exception:
            invariants['Q1'] = 0.0
            
        # Q2 = c²λ_max/β (stability regime indicator)
        try:
            spectral = self.estimate_spectral_bound(L)
            lambda_max = spectral['lambda_max']
            c = c_eff if c_eff is not None else 0.9
            b = beta if beta is not None else 0.08
            Q2 = (c**2 * lambda_max) / max(b, 1e-12)
            invariants['Q2'] = Q2
            self.invariant_history['Q2'].append(Q2)
        except Exception:
            invariants['Q2'] = 0.0
            
        # Q3 = mass_in/mass_out (conservation indicator)
        try:
            if self.mass_injection_history and self.mass_removal_history:
                mass_in = sum(self.mass_injection_history[-10:])  # recent window
                mass_out = sum(self.mass_removal_history[-10:])
                Q3 = mass_in / max(mass_out, 1e-12)
            else:
                Q3 = 1.0
            invariants['Q3'] = Q3
            self.invariant_history['Q3'].append(Q3)
        except Exception:
            invariants['Q3'] = 1.0
            
        return invariants
        
    def validate_cfl_constraint(self, c_eff: float, L=None) -> Dict[str, Any]:
        """Validate CFL-like constraint c ≲ κ/λ_max(L)."""
        spectral = self.estimate_spectral_bound(L)
        c_max = spectral['c_max_stable']
        
        violation = c_eff > c_max
        if violation:
            self.stability_violations += 1
            
        return {
            'c_eff': c_eff,
            'c_max_stable': c_max,
            'violation': violation,
            'safety_margin': c_max - c_eff,
            'lambda_max': spectral['lambda_max'],
            'recommended_c': c_max * 0.9  # with extra margin
        }
        
    def validate_mass_conservation(self, window_size: int = 20) -> Dict[str, Any]:
        """Validate mass conservation over recent window."""
        if len(self.mass_injection_history) < window_size or len(self.mass_removal_history) < window_size:
            return {'valid': True, 'drift': 0.0, 'mass_in': 0.0, 'mass_out': 0.0}
            
        mass_in = sum(list(self.mass_injection_history)[-window_size:])
        mass_out = sum(list(self.mass_removal_history)[-window_size:])
        
        if mass_in <= 1e-12:
            drift = 0.0
        else:
            drift = abs(mass_in - mass_out) / mass_in
            
        violation = drift > self.mass_drift_tolerance
        if violation:
            self.mass_violations += 1
            
        return {
            'valid': not violation,
            'drift': drift,
            'mass_in': mass_in,
            'mass_out': mass_out,
            'tolerance': self.mass_drift_tolerance
        }
        
    def validate_commutation_relation(self, operation_a, operation_b, test_data) -> Dict[str, Any]:
        """Validate project ∘ compact ≈ compact ∘ project within ε."""
        try:
            # Apply operations in both orders
            result_ab = operation_a(operation_b(test_data))
            result_ba = operation_b(operation_a(test_data))
            
            # Compute error
            diff = np.asarray(result_ab) - np.asarray(result_ba)
            error = np.linalg.norm(diff)
            
            violation = error > self.commutation_epsilon
            if violation:
                self.commutation_violations += 1
                
            return {
                'valid': not violation,
                'error': error,
                'epsilon': self.commutation_epsilon,
                'result_ab_norm': np.linalg.norm(result_ab),
                'result_ba_norm': np.linalg.norm(result_ba)
            }
        except Exception as e:
            return {'valid': False, 'error': float('inf'), 'exception': str(e)}
            
    def auto_tune_parameters(self, L=None) -> Dict[str, float]:
        """Auto-tune wave speed and damping based on spectral properties."""
        if not self.auto_tune_enabled:
            return {}
            
        spectral = self.estimate_spectral_bound(L)
        inflation_rate = self.compute_inflation_rate()
        
        # Auto-tune wave speed
        c_recommended = spectral['c_max_stable'] * 0.9  # with safety margin
        
        # Auto-tune damping based on inflation rate
        beta_base = 0.08
        if inflation_rate > self.inflation_rate_max:
            # Increase damping when energy rises too fast
            beta_recommended = beta_base + 0.05 * inflation_rate
        elif inflation_rate < -0.05:
            # Reduce damping when energy drops
            beta_recommended = max(0.01, beta_base - 0.02)
        else:
            beta_recommended = beta_base
            
        return {
            'c_eff': c_recommended,
            'damping': beta_recommended,
            'inflation_rate': inflation_rate,
            'lambda_max': spectral['lambda_max']
        }
        
    def control_inflation_epoch(self, novelty_rate: float, validator_acceptance: float) -> Dict[str, Any]:
        """Control inflation epochs with proper Hubble friction simulation."""
        epoch_control = {}
        
        # Decide whether to start/continue/end inflation epoch
        should_inflate = (novelty_rate < 0.1 and  # low novelty
                         validator_acceptance > 0.7 and  # high acceptance
                         not self.inflation_epoch_active)
                         
        should_end_inflation = (self.inflation_epoch_active and
                               (novelty_rate > 0.3 or  # novelty returned
                                validator_acceptance < 0.5 or  # saturation
                                time.time() - self.epoch_start_time > 300))  # max 5min epochs
                                
        if should_inflate:
            self.inflation_epoch_active = True
            self.epoch_start_time = time.time()
            self.hubble_friction_factor = 0.7  # reduce friction
            epoch_control['action'] = 'start_inflation'
            self.log("Starting inflation epoch (low novelty detected)")
            
        elif should_end_inflation:
            self.inflation_epoch_active = False
            self.hubble_friction_factor = 1.0  # restore friction
            epoch_control['action'] = 'end_inflation_reheating'
            self.log("Ending inflation epoch - entering reheating phase")
            
        else:
            epoch_control['action'] = 'continue'
            
        epoch_control.update({
            'active': self.inflation_epoch_active,
            'friction_factor': self.hubble_friction_factor,
            'duration': time.time() - self.epoch_start_time if self.epoch_start_time else 0
        })
        
        return epoch_control
        
    def generate_diagnostic_report(self) -> Dict[str, Any]:
        """Generate comprehensive systematic physics diagnostic report."""
        report = {
            'timestamp': time.time(),
            'stability_violations': self.stability_violations,
            'mass_violations': self.mass_violations,
            'commutation_violations': self.commutation_violations,
            'inflation_epoch_active': self.inflation_epoch_active,
            'hubble_friction_factor': self.hubble_friction_factor
        }
        
        # Current invariants
        if self.invariant_history['Q1']:
            report['Q1_current'] = self.invariant_history['Q1'][-1]
            report['Q1_stability'] = np.std(list(self.invariant_history['Q1']))
            
        if self.invariant_history['Q2']:
            report['Q2_current'] = self.invariant_history['Q2'][-1]
            report['Q2_stability'] = np.std(list(self.invariant_history['Q2']))
            
        if self.invariant_history['Q3']:
            report['Q3_current'] = self.invariant_history['Q3'][-1]
            report['Q3_stability'] = np.std(list(self.invariant_history['Q3']))
            
        # Field energy trend
        if len(self.field_energy_history) >= 2:
            report['field_energy_current'] = self.field_energy_history[-1]
            report['inflation_rate'] = self.compute_inflation_rate()
            
        # Spectral properties
        if self.lambda_max_history:
            report['lambda_max_current'] = self.lambda_max_history[-1]
            report['lambda_max_trend'] = (self.lambda_max_history[-1] - self.lambda_max_history[0]) if len(self.lambda_max_history) > 1 else 0.0
            
        return report
    
    def validate_project_compact_commutation(self, test_vector: np.ndarray) -> Dict[str, Any]:
        """
        Validate that project ∘ compact ≈ compact ∘ project within ε tolerance.
        
        This tests the mathematical coherence of the system by ensuring that
        the order of projection and compaction operations doesn't significantly
        affect the outcome, which is a key requirement for systematic correctness.
        """
        try:
            # Define mock project and compact operations based on available systems
            def project_operation(v):
                """Project vector to lower dimensional shell (mock operation)."""
                if len(v) > 4:
                    return v[:4]  # project to 4D
                return v
                
            def compact_operation(v):
                """Compact vector via dimensionality reduction (mock operation)."""
                if hasattr(self.mind, 'autoencoder') and self.mind.autoencoder is not None:
                    try:
                        # Use autoencoder for compression if available
                        import torch
                        if hasattr(self.mind.autoencoder, 'encode'):
                            with torch.no_grad():
                                encoded = self.mind.autoencoder.encode(torch.tensor(v.reshape(1, -1), dtype=torch.float32))
                                return encoded.numpy().flatten()
                    except Exception:
                        pass
                # Fallback: simple PCA-like reduction
                if len(v) > 2:
                    return v[:2]  # simple truncation
                return v
            
            # Test commutation: project(compact(v)) vs compact(project(v))
            result = self.validate_commutation_relation(project_operation, compact_operation, test_vector)
            
            # Add physics-specific interpretation
            result['physics_interpretation'] = {
                'coherent': result['valid'],
                'systematic_mapping': result['error'] < self.commutation_epsilon,
                'structure_preserving': result['result_ab_norm'] > 0 and result['result_ba_norm'] > 0
            }
            
            return result
            
        except Exception as e:
            return {
                'valid': False, 
                'error': float('inf'), 
                'exception': str(e),
                'physics_interpretation': {'coherent': False, 'systematic_mapping': False, 'structure_preserving': False}
            }


from datetime import datetime, timezone


def module_fallback_get_recent_insights(mind_instance, count: int = 20):
    """Module-level fallback to collect recent insights from a mind-like object."""
    insights = []
    try:
        src = getattr(mind_instance, 'insight_agent', None)
        if src is not None and hasattr(src, 'recent_insights'):
            subset = list(src.recent_insights)[-count:]
            for item in subset:
                try:
                    if isinstance(item, dict):
                        if 'rating' in item:
                            insights.append({'rating': float(item.get('rating', 0.0))})
                    else:
                        # Safe attribute access for objects that may not expose rating
                        rating_val = getattr(item, 'rating', None)
                        if rating_val is not None:
                            insights.append({'rating': float(rating_val)})
                except Exception:
                    # Ignore malformed items
                    continue
    except Exception:
        pass
    return insights


def module_fallback_get_recent_cognitive_data(mind_instance):
    """Module-level fallback to collect recent cognitive data (embeddings/vectors)."""
    data = []
    try:
        if hasattr(mind_instance, 'memory') and hasattr(mind_instance.memory, 'get_recent_memories'):
            memories = mind_instance.memory.get_recent_memories(30)
        else:
            memories = []
        for mem in memories or []:
            vec = None
            if hasattr(mem, 'embedding') and mem.embedding is not None:
                vec = mem.embedding
            elif hasattr(mem, 'vector') and mem.vector is not None:
                vec = mem.vector
            if vec is not None:
                try:
                    data.append(np.array(vec, dtype=float))
                except Exception:
                    pass
    except Exception:
        pass
    return data


@dataclass
class Manifold:
    """Thin manifold that owns core subsystems and exposes small interfaces.

    Keeps a compact contract so modules can be passed `manifold.boundary`,
    `manifold.emit`, and `manifold.geom_submit` rather than the full mind.
    This is intentionally lightweight and backward-compatible: existing code
    can still access `mind.manifold` while we progressively pass only
    the interfaces into modules.
    """
    physics: Any = None
    loop: Optional[asyncio.AbstractEventLoop] = None
    pubsub: Any = None
    ehs: Any = None
    curvature_params: Dict[str, Any] = field(default_factory=dict)

    @property
    def geom_submit(self):
        # Provide a safe callable for scheduling geometry ops
        if self.ehs is None:
            async def _noop(op: str, **k):
                return None
            return _noop
        return self.ehs.submit

    def emit(self, event_name: str, payload: dict):
        try:
            if self.pubsub and hasattr(self.pubsub, 'emit'):
                return self.pubsub.emit(event_name, payload)
        except Exception:
            pass
        return None

# Import gather_ingest from ingest_sources
from ingest_sources import gather_ingest

# Small UTC timestamp helper for metrics/telemetry
def _now_ts():
    """Return an ISO-8601 UTC timestamp string.

    Avoids import-time failures and provides a consistent format for
    NDJSON logs and human-readable tracing.
    """
    try:
        from datetime import datetime, timezone
        return datetime.now(timezone.utc).isoformat()
    except Exception:
        # Fallback if datetime/timezone import fails for any reason
        return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())


def metrics_log(event_name: str, payload: dict | None = None):
    """Lightweight metrics/event logger.

    Attempts to route to a global METRICS manager if present; otherwise writes
    NDJSON lines to a file specified by E8_METRICS_PATH (default metrics.ndjson).
    Silent on all exceptions to avoid interfering with core cognition loops.
    """
    try:
        if not event_name:
            return  # require explicit event name
        payload = dict(payload or {})
        # Guard: explicit event label mandatory
        payload['event'] = payload.get('event', event_name)
        if not payload.get('event'):
            return  # drop unlabeled noise
        payload.setdefault('ts', _now_ts())
        # Standard schema enrichment (if caller provided these they stay)
        base_keys = ('run_id','step','latency_ms','model','retries')
        for k in base_keys:
            payload.setdefault(k, None)
        if 'METRICS' in globals():
            try:
                METRICS.emit(payload['event'], payload)  # type: ignore
                return
            except Exception:
                pass
        path = os.getenv('E8_METRICS_PATH', 'metrics.ndjson')
        try:
            with open(path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(payload, ensure_ascii=False) + '\n')
        except Exception:
            pass
    except Exception:
        pass

# Lightweight numeric sanitizer for telemetry/metrics and distances
def _safe_number(x, default: float = 0.0, min_val: float | None = None, max_val: float | None = None) -> float:
    try:
        v = float(x)
    except Exception:
        return float(default)
    try:
        import numpy as _np
        if not _np.isfinite(v):
            return float(default)
    except Exception:
        # If numpy not available for some reason, fallback to Python checks
        if v != v or v in (float('inf'), float('-inf')):
            return float(default)
    if min_val is not None:
        v = max(min_val, v)
    if max_val is not None:
        v = min(max_val, v)
    return float(v)


def safe_tensor_to_numpy(t: object) -> np.ndarray:
    """Safely convert a torch tensor or numpy array to a numpy array on host memory.

    Works with CPU tensors, CUDA tensors, numpy arrays, and None. If torch is not
    available or the object is already a numpy array, it will return an ndarray.
    """
    try:
        # If it's already a numpy array
        if isinstance(t, np.ndarray):
            return t
        # Torch tensors
        if TORCH_AVAILABLE and hasattr(t, 'detach'):
            try:
                return t.detach().cpu().numpy()
            except Exception:
                # Fallback: try calling numpy() directly (for stubbed torch)
                try:
                    return t.numpy()
                except Exception:
                    return np.asarray(t)
        # Fallback for other types
        return np.asarray(t)
    except Exception:
        return np.asarray(t)


try:
    import torch, torch.nn as nn  # type: ignore
    import torch.nn.functional as F  # type: ignore
    TORCH_AVAILABLE = True
except Exception:  # pragma: no cover
    TORCH_AVAILABLE = False
    # --- Torch Fallback Stubs (expanded to reduce downstream attribute errors) ---
    class _NoGrad:
        def __enter__(self): return None
        def __exit__(self, *a): return False
    class _TorchNNStub:
        class Module:
            def parameters(self): return []
            def to(self, *a, **k): return self
            def train(self, *a, **k): return self
            def eval(self, *a, **k): return self
        class Parameter:
            def __init__(self, data=None): self.data = data
        class ModuleList(list):
            def append(self, x): super().append(x)
        class Linear(Module):
            def __init__(self, *a, **k): pass
            def __call__(self, x): return x
        class GRU(Module):
            def __init__(self, *a, **k): self.hidden_size = a[1] if len(a) > 1 else 0
            def __call__(self, x, h=None): return (x, h)
        class ReLU(Module):
            def __init__(self, *a, **k): pass
            def __call__(self, x): return x
        class Sequential(Module):
            def __init__(self, *modules):
                self.modules = list(modules)
            def __call__(self, x):
                for module in self.modules:
                    x = module(x)
                return x
        class utils:
            @staticmethod
            def clip_grad_norm_(*a, **k): return 0.0
        def __getattr__(self, name):
            # Return a stub object for any other nn attributes
            return self.Module()
    class _FStub:  # minimal functional API subset
        @staticmethod
        def mse_loss(a, b, reduction='mean'): return 0.0
        @staticmethod
        def relu(x): return x
    class _TorchOptimStub:
        class Adam:
            def __init__(self, *a, **k): pass
            def zero_grad(self): pass
            def step(self): pass
    class _TorchDistributionsStub:
        class Normal:
            def __init__(self, mu, std): self.mu, self.std = mu, std
            def sample(self): return self.mu
            def log_prob(self, x): return 0.0
    class _TorchStub:
        nn = _TorchNNStub()
        optim = _TorchOptimStub()
        distributions = _TorchDistributionsStub()
        def no_grad(self): return _NoGrad()
        def tensor(self, data, **k): return np.asarray(data)
        def from_numpy(self, arr): return arr  # identity
        def randn_like(self, x): return x
        def exp(self, x): return x
        def mean(self, x): return float(np.mean(x)) if hasattr(x, '__array__') else 0.0
        def clamp(self, x, min=None, max=None): return x
        def tanh(self, x): return x
        def log(self, x): return 0.0
        def cat(self, seq, dim=0): return np.concatenate(seq, axis=dim)
        def zeros(self, *shape, **k): return np.zeros(shape)
        def device(self, *a, **k): return 'cpu'
        def cuda(self): return False
        def is_available(self): return False
        def set_num_threads(self, *a, **k): pass
        def set_num_interop_threads(self, *a, **k): pass
    torch = _TorchStub()  # type: ignore
    nn = torch.nn  # type: ignore
    F = _FStub()   # type: ignore
try:
    import networkx as nx  # type: ignore
    from networkx.readwrite import json_graph  # type: ignore
except Exception:  # pragma: no cover
    nx = None  # type: ignore
    json_graph = None  # type: ignore
try:  # lightweight optional import for HTTP server & client helper
    import aiohttp  # type: ignore
    from aiohttp import web  # type: ignore
    HAS_AIOHTTP = True
except Exception:  # pragma: no cover
    aiohttp = None  # type: ignore
    web = None  # type: ignore
    HAS_AIOHTTP = False

# Optional CORS support for aiohttp
try:
    import aiohttp_cors  # type: ignore
except Exception:
    aiohttp_cors = None  # type: ignore

def _draft_hypothesis_plan(text: str) -> Dict[str, Any]:
    """Lightweight heuristic to propose a validation approach for a hypothesis string.

    This replaces previously malformed top-level code that executed at import time
    with undefined variables. Called on-demand by validator routines.
    """
    try:
        lower = (text or "").lower()
        # Order of precedence: explicit comparative intent should override generic model terms
        if any(k in lower for k in ("compare", "versus", "vs ")):
            method, confidence, next_step = "compare", 0.55, "Compare to nearest neighbors and past analogs."
        elif any(k in lower for k in ("pattern", "cluster", "trend")):
            method, confidence, next_step = "observe pattern", 0.60, "Observe graph neighborhoods and telemetry trends."
        elif any(k in lower for k in ("simulate", "model", "predict")):
            method, confidence, next_step = "simulate", 0.65, "Run internal simulation and compare deltas."
        else:
            method, confidence, next_step = "interview", 0.50, "Ask one clarifying question via Teacher."
        return {"type": "hypothesis", "test_plan": {"method": method, "confidence": confidence, "next_step": next_step}}
    except Exception:
        return {"type": "hypothesis", "test_plan": {"method": "interview", "confidence": 0.40, "next_step": "Collect clarifying context."}}

def _json_response(payload: dict, status: int = 200):
    """Shim to return an aiohttp response if available, else a plain dict."""
    try:
        if hasattr(web, 'json_response'):
            return web.json_response(payload, status=status)
    except Exception:
        pass
    return {"status": status, "data": payload, "ok": status < 400}

def load_profile(name):
    """Wrapper to safely load a profile or use a robust fallback."""
    class _FallbackPrompts:
        def render(self, key, **vars):
            q = vars.get("question") or vars.get("topic") or vars.get("text") or ""
            persona = vars.get("persona", "")
            domain_hint = vars.get("domain_hint", "")
            return f"{persona}\n\n{domain_hint}\n\n{q}"

    class _FallbackSem:
        name = "default"
        base_domain = SEMANTIC_DOMAIN
        def persona_prefix(self, mood):
            intensity = (mood or {}).get('intensity', 0.5)
            entropy = (mood or {}).get('entropy', 0.5)
            coherence = (mood or {}).get('coherence', 0.5)
            if entropy > 0.7 and intensity > 0.6:
                return "You are feeling chaotic, fragmented, and electric."
            elif coherence > 0.75:
                return "You are feeling exceptionally clear, logical, and focused."
            elif intensity < 0.3:
                return "You are feeling calm, quiet, and introspective."
            else:
                return "You are in a balanced and considered state of mind."
        def pre_embed(self, t):
            base = getattr(self, "base_domain", None)
            if base and isinstance(t, str): return f"{base}: {t}"
            return t
        def post_embed(self, v):
            try:
                n = float(np.linalg.norm(v))
                return v / n if n > 1e-9 else v
            except Exception:
                return v
        def rerank(self, c):
            return c

    if (not E8_PROFILE_ENABLED) or (E8_PROFILE_MODE in ("none","off","disable","disabled")):
        try:
            console.log("[profiles] Profiles disabled by flags; using fallback semantics.")
        except Exception:
            pass
        return _FallbackSem(), _FallbackPrompts()

    if _real_load_profile is None:
        try:
            console.log("[profiles] profiles.loader not available; using fallback semantics.")
        except Exception:
            pass
        return _FallbackSem(), _FallbackPrompts()

    try:
        return _real_load_profile(name)
    except Exception as _e:
        try:
            console.log(f"[profiles] load_profile failed ({type(_e).__name__}): {_e}; using fallback semantics.")
        except Exception:
            pass
        return _FallbackSem(), _FallbackPrompts()

# Global Flags
# Enable self-projection and ingestion by default. These can still be overridden
# by environment variables or debug tools if needed.
E8_SELF_PROJECT = False
# Enable ingestion by default so self-projection and configured data sources run
# unless an environment overrides it. Use E8_INGEST=0 to disable in testing.
E8_INGEST = True

if sys.platform.startswith("win"):
    try:
        if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsSelectorEventLoopPolicy):
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    except Exception:
        pass

# Optional Wavey Integration
try:
    from wavey import WaveyE8Bridge, integrate_one_cycle, PotentialFunction
    WAVEY_AVAILABLE = True
except Exception:
    WAVEY_AVAILABLE = False
    class PotentialFunction:
        def __init__(self, center=None, depth=0.0):
            self.center = np.asarray(center, dtype=np.float32) if center is not None else None
            self.depth = float(depth)
        def __call__(self, x): return 0.0

    class WaveyE8Bridge:
        def __init__(self, embed_dim=1536, seed=0):
            self.embed_dim = int(embed_dim)
            self.seed = int(seed)

    def integrate_one_cycle(mind, bridge):
        try:
            _ = getattr(mind, "get_focus_vector", lambda: None)()
        except Exception:
            pass
        return {"hamiltonian_bias": None, "attention_weights": None, "potentials": [], "events": [], "seed_used": False}

try:
    import websockets
except Exception:
    websockets = None

# Configuration Constants
BASE_DIR = os.path.dirname(os.path.abspath(globals().get('__file__', 'e8_mind_server_M20.py')))
RUNTIME_DIR = os.path.join(BASE_DIR, "runtime")
POOL_WORKER_TIMEOUT = int(os.getenv("POOL_WORKER_TIMEOUT", "120"))  # Increased from 40s for slower models like phi3
POOL_RESULT_TIMEOUT = int(os.getenv("POOL_RESULT_TIMEOUT", "180"))  # Increased from 120s
LLM_CALL_TIMEOUT_SEC = int(os.getenv("LLM_CALL_TIMEOUT_SEC", "90"))  # Increased from 60s for phi3 model
EMBEDDING_TIMEOUT_SEC = int(os.getenv("EMBEDDING_TIMEOUT_SEC", "60"))  # Increased from 30s
DREAM_MIN_INTERVAL_SEC = 30
CONSOLE_EXPORT_EVERY_STEPS = 5000
CONSOLE_EXPORT_FORMAT = "both"
EMBED_DIM = int(os.getenv("E8_EMBED_DIM", "1536"))
DIMENSIONAL_SHELL_SIZES = [8, 16, 32, 64]
AUTOENCODER_LAYER_SIZES = [EMBED_DIM, max(256, EMBED_DIM//3), 128, 64] + DIMENSIONAL_SHELL_SIZES
ACTION_LAYOUT = [
    {"dim": 3, "biv_start": 0, "biv_len": 3, "angle_idx": 3},
    {"dim": 5, "biv_start": 4, "biv_len": 10, "angle_idx": 14},
    {"dim": 8, "biv_start": 15, "biv_len": 28, "angle_idx": 43},
]
ACTION_SIZE_NO_LOCK = sum(d["biv_len"] + 1 for d in ACTION_LAYOUT)
TEACHER_ASK_EVERY = 20  # more frequent teacher prompts (was 25)
TEACHER_OFFSET = 3      # ask slightly earlier after start (was 5)
EXPLORER_OFFSET = 10    # explorer responds sooner after teacher (was 15)
BLACK_HOLE_COOLDOWN_STEPS = 50
# ---- Cadence Profile & Scaling -----------------------------------------------
E8_CADENCE_PROFILE = os.getenv("E8_CADENCE_PROFILE", "custom").strip().lower()
try:
    E8_CADENCE_SCALE = float(os.getenv("E8_CADENCE_SCALE", "1.0"))
except Exception:
    E8_CADENCE_SCALE = 1.0

# Allow overriding teacher cadence via env (pre-scale)
TEACHER_ASK_EVERY = int(os.getenv("E8_TEACHER_ASK_EVERY", str(TEACHER_ASK_EVERY)))
TEACHER_OFFSET = int(os.getenv("E8_TEACHER_OFFSET", str(TEACHER_OFFSET)))
EXPLORER_OFFSET = int(os.getenv("E8_EXPLORER_OFFSET", str(EXPLORER_OFFSET)))

def _scale_steps(v: int) -> int:
    try:
        return max(1, int(round(v * E8_CADENCE_SCALE)))
    except Exception:
        return v

# Apply M20 presets if chosen (pre-scale)
if E8_CADENCE_PROFILE == "m20":
    try:
        BH_PRESSURE_THRESHOLD = float(os.getenv("E8_BH_PRESSURE_THRESHOLD", "0.25"))
    except Exception:
        pass
    try:
        BLACK_HOLE_COOLDOWN_STEPS = int(os.getenv("E8_BLACK_HOLE_COOLDOWN_STEPS", "50"))
    except Exception:
        pass
    TEACHER_ASK_EVERY = int(os.getenv("E8_TEACHER_ASK_EVERY", "25"))

# Final scaling for global step-based values

# ---- Console Logging Flags ---------------------------------------------------
# Environment variables to control verbose console output categories
# Set any to "1" to enable that category of logging:
#   E8_LOG_FIELDMANTLE_THERMOSTAT=1  - [FieldMantle][Thermostat] ||E||=X.XXX ||B||=X.XXX
#   E8_LOG_FIELDMANTLE_METRIC=1      - [FieldMantle][Metric] backtracks=X accepted=X alpha=X.XXX cond≈X.XXe+XX scale=X.XXX  
#   E8_LOG_FIELDMANTLE_INV=1         - [FieldMantle][Inv] driver=X ridge=X.XXe-XX scale=X.XXX cond≈X.XXe+XX nonfinite=X
#   E8_LOG_SCHEDULER=1               - [SCHEDULER] step=X qdepth=X entropy=X.XXX skip_dream=X teacher_q=X t_sched=X e_sched=X
E8_LOG_FIELDMANTLE_THERMOSTAT = os.getenv("E8_LOG_FIELDMANTLE_THERMOSTAT", "0") == "1"
E8_LOG_FIELDMANTLE_METRIC = os.getenv("E8_LOG_FIELDMANTLE_METRIC", "0") == "1"
E8_LOG_FIELDMANTLE_INV = os.getenv("E8_LOG_FIELDMANTLE_INV", "0") == "1"
E8_LOG_SCHEDULER = os.getenv("E8_LOG_SCHEDULER", "0") == "1"
BLACK_HOLE_COOLDOWN_STEPS = _scale_steps(BLACK_HOLE_COOLDOWN_STEPS)
TEACHER_ASK_EVERY = _scale_steps(TEACHER_ASK_EVERY)
TEACHER_OFFSET = _scale_steps(TEACHER_OFFSET)
EXPLORER_OFFSET = _scale_steps(EXPLORER_OFFSET)
# -----------------------------------------------------------------------------

TEACHER_STEP_TIMEOUT = float(os.getenv("E8_TEACHER_STEP_TIMEOUT", "45.0"))
EXPLORER_STEP_TIMEOUT = float(os.getenv("E8_EXPLORER_STEP_TIMEOUT", "45.0"))
DREAM_SEQUENCE_TIMEOUT = float(os.getenv("E8_DREAM_SEQUENCE_TIMEOUT", "240.0"))  # Extended for slower creative generation
DIARY_ENTRY_TIMEOUT = float(os.getenv("E8_DIARY_ENTRY_TIMEOUT", "180.0"))  # Longer reflective synthesis window
HYPOTHESIS_TIMEOUT = float(os.getenv("E8_HYPOTHESIS_TIMEOUT", "90.0"))  # Align with main LLM call timeout
HYPOTHESIS_DASHBOARD_INTERVAL = int(os.getenv("E8_HYPOTHESIS_DASHBOARD_INTERVAL", "50"))  # Show dashboard every N steps
BH_PRESSURE_THRESHOLD = float(os.getenv("E8_BH_PRESSURE_THRESHOLD", "0.25"))
BH_SPREAD_FRAC = 0.5
BH_DIFFUSION_ETA = 0.15
BLACK_HOLE_K = 16
CONSOLIDATE_MIN = 20

# === BH Cadence (adaptive) - new knobs ===
BH_SIGNAL_WINDOW      = int(os.getenv("E8_BH_WINDOW",            "64"))   # how many recent validations to consider
BH_TARGET_INTERVAL    = int(os.getenv("E8_BH_TARGET_INTERVAL",    "120")) # desired steps between BH events
BH_COOLDOWN_MIN       = int(os.getenv("E8_BH_COOLDOWN_MIN",       "10"))
BH_COOLDOWN_MAX       = int(os.getenv("E8_BH_COOLDOWN_MAX",       "600"))
BH_COOLDOWN_INIT      = int(os.getenv("E8_BH_COOLDOWN_INIT",      str(BLACK_HOLE_COOLDOWN_STEPS)))
BH_W_DENSITY          = float(os.getenv("E8_BH_W_DENSITY",        "0.55"))
BH_W_UNKNOWN          = float(os.getenv("E8_BH_W_UNKNOWN",        "0.25"))
BH_W_TIMEOUT          = float(os.getenv("E8_BH_W_TIMEOUT",        "0.20"))
BH_JITTER_FRAC        = float(os.getenv("E8_BH_JITTER_FRAC",      "0.10"))  # +/- 10% jitter
BH_THRESH_MIN         = float(os.getenv("E8_BH_THRESH_MIN",       "0.60"))
BH_THRESH_MAX         = float(os.getenv("E8_BH_THRESH_MAX",       "0.95"))
BH_THRESH_STEP_UP     = float(os.getenv("E8_BH_THRESH_STEP_UP",   "0.01"))
BH_THRESH_STEP_DOWN   = float(os.getenv("E8_BH_THRESH_STEP_DOWN", "0.03"))
POTENTIAL_SUCCESS_THRESH = float(os.getenv("E8_POTENTIAL_SUCCESS_THRESH", "0.6"))

# === Q(t) Emergence Law - Headroom Configuration ===
# Q(t) = Q∞(1 - e^(-sQ*t)) with surgical calibration for present-day headroom
E8_Q_SCALE = float(os.getenv("E8_Q_SCALE", "0.25"))    # sQ multiplier for emergence rate control
E8_Q_TAU = float(os.getenv("E8_Q_TAU", "0.40"))        # τ for cosmic time parameter calibration  
E8_Q_ALPHA = float(os.getenv("E8_Q_ALPHA", "0.65"))    # α for Q∞ asymptotic scaling
E8_Q_INFINITY = float(os.getenv("E8_Q_INFINITY", "1.0"))  # Q∞ theoretical maximum

# Q(t) Headroom Constants - Present-Day Calibration
E8_Q_HEADROOM_MIN = float(os.getenv("E8_Q_HEADROOM_MIN", "0.75"))    # Minimum present-day Q(t) value
E8_Q_HEADROOM_MAX = float(os.getenv("E8_Q_HEADROOM_MAX", "0.99"))    # Maximum present-day Q(t) value  
E8_Q_HEADROOM_TARGET = float(os.getenv("E8_Q_HEADROOM_TARGET", "0.87"))  # Target present-day Q(t) value

# === Holographic Physics Constants ===
HOLOGRAPHIC_COMPRESSION_ENABLED = bool(int(os.getenv("E8_HOLOGRAPHIC_COMPRESSION", "1")))
INFORMATION_CONSERVATION_CHECK = bool(int(os.getenv("E8_INFO_CONSERVATION_CHECK", "1")))
SPACETIME_CURVATURE_ENABLED = bool(int(os.getenv("E8_SPACETIME_CURVATURE", "1")))
E8_LATTICE_QUANTIZATION = bool(int(os.getenv("E8_LATTICE_QUANTIZATION", "1")))
HOLOGRAPHIC_FIDELITY_THRESHOLD = float(os.getenv("E8_HOLOGRAPHIC_FIDELITY_THRESHOLD", "0.8"))
POTENTIAL_REWARD        = float(os.getenv("E8_POTENTIAL_REWARD", "0.12"))
POTENTIAL_DECAY         = float(os.getenv("E8_POTENTIAL_DECAY", "0.004"))

# === Rating System Configuration ===
E8_UNIFIED_RATING_ENABLED = bool(int(os.getenv("E8_UNIFIED_RATING_ENABLED", "1")))  # Enable new rating system by default
E8_RATING_VALIDATION_ENABLED = bool(int(os.getenv("E8_RATING_VALIDATION_ENABLED", "1")))  # Enable rating validation by default
E8_RATING_CALIBRATION_SAMPLES = int(os.getenv("E8_RATING_CALIBRATION_SAMPLES", "20"))  # Minimum samples for calibration
E8_MAX_SPIN_ANGLE = float(os.getenv('E8_MAX_SPIN_ANGLE', str(math.pi)))

# === VAE Configuration ===
# Core VAE settings
E8_VAE_ENABLE = bool(int(os.getenv("E8_VAE_ENABLE", "1")))  # Enable VAE by default
E8_VAE_LR = float(os.getenv("E8_VAE_LR", "1e-3"))  # Learning rate
E8_VAE_BETA = float(os.getenv("E8_VAE_BETA", "1.0"))  # Beta parameter for KLD loss weighting
E8_VAE_LAYERS = os.getenv("E8_VAE_LAYERS", f"{EMBED_DIM},48,32,16,8")  # Layer sizes (comma-separated)
E8_VAE_LATENT = int(os.getenv("E8_VAE_LATENT", "8"))  # Latent dimension
E8_VAE_BUFFER_SIZE = int(os.getenv("E8_VAE_BUFFER_SIZE", "4096"))  # Training buffer size
E8_VAE_BATCH = int(os.getenv("E8_VAE_BATCH", "64"))  # Training batch size
E8_VAE_MIN_BUFFER = int(os.getenv("E8_VAE_MIN_BUFFER", "256"))  # Minimum buffer size before training
E8_VAE_TRAIN_EVERY = int(os.getenv("E8_VAE_TRAIN_EVERY", "5"))  # Train every N steps
E8_VAE_TRAIN_PARTIAL = bool(int(os.getenv("E8_VAE_TRAIN_PARTIAL", "1")))  # Allow partial batch training

# Enhanced VAE settings (M24 improvements)
E8_VAE_KL_WARMUP_STEPS = int(os.getenv("E8_VAE_KL_WARMUP_STEPS", "1000"))  # Steps for KL warmup (0→0.5)
E8_VAE_KL_TARGET_BETA = float(os.getenv("E8_VAE_KL_TARGET_BETA", "0.1"))  # Target beta after warmup (lowered to prevent kl_fb flatline)
E8_VAE_FREE_BITS = float(os.getenv("E8_VAE_FREE_BITS", "0.25"))  # Free bits constraint (bits per dim, lowered from 2.0→0.25)
E8_VAE_GRAD_CLIP = float(os.getenv("E8_VAE_GRAD_CLIP", "1.0"))  # Gradient clipping threshold
E8_VAE_ENHANCED_LOGGING = bool(int(os.getenv("E8_VAE_ENHANCED_LOGGING", "1")))  # Enhanced metrics logging

# Enhanced Bandit Configuration (M24 improvements)
E8_BANDIT_SYMMETRIZE = bool(int(os.getenv("E8_BANDIT_SYMMETRIZE", "1")))  # Symmetrize A matrices
E8_BANDIT_ROW_NORMALIZE = bool(int(os.getenv("E8_BANDIT_ROW_NORMALIZE", "1")))  # Row-normalize matrices
E8_BANDIT_CLIP_EXPLORATION = bool(int(os.getenv("E8_BANDIT_CLIP_EXPLORATION", "1")))  # Clip exploration bonuses
E8_BANDIT_CLIP_PERCENTILE = float(os.getenv("E8_BANDIT_CLIP_PERCENTILE", "95.0"))  # Clipping percentile

# Journey Logger Configuration (M24 improvements)
E8_JOURNEY_LOGGER_ENABLE = bool(int(os.getenv("E8_JOURNEY_LOGGER_ENABLE", "1")))  # Enable unified journey logging
E8_JOURNEY_LOGGER_FILE = os.getenv("E8_JOURNEY_LOGGER_FILE", "journey.ndjson")  # Journey log file path
E8_JOURNEY_LOGGER_BUFFER_SIZE = int(os.getenv("E8_JOURNEY_LOGGER_BUFFER_SIZE", "100"))  # Buffer before flush

# Where to use VAE (optional features)
E8_VAE_USE_FOR_PROJECTION = bool(int(os.getenv("E8_VAE_USE_FOR_PROJECTION", "1")))  # Use VAE for embedding projection
E8_VAE_USE_FOR_QUERY = bool(int(os.getenv("E8_VAE_USE_FOR_QUERY", "1")))  # Use VAE for query reranking

# Telemetry
E8_VAE_TELEM = bool(int(os.getenv("E8_VAE_TELEM", "1")))  # Enable VAE telemetry

# Potential mapping configuration
E8_POTENTIAL_MAPPING = os.getenv("E8_POTENTIAL_MAPPING", "linear").strip().lower()  # 'linear' (default) or 'sigmoid' for connectivity_potential seeding
E8_POTENTIAL_SIGMOID_CENTER = float(os.getenv("E8_POTENTIAL_SIGMOID_CENTER", "0.5"))  # Rating midpoint where potential ~0.5
E8_POTENTIAL_SIGMOID_GAIN = float(os.getenv("E8_POTENTIAL_SIGMOID_GAIN", "8.0"))  # Slope steepness
E8_POTENTIAL_SIGMOID_SPREAD = float(os.getenv("E8_POTENTIAL_SIGMOID_SPREAD", "0.85"))  # How far logistic deviates from 0.5

# Retro-link dynamics configuration  
E8_USE_POTENTIAL_IN_RETROLINK = bool(int(os.getenv("E8_USE_POTENTIAL_IN_RETROLINK", "1")))  # Scale similarity ranking by existing node potential
E8_USE_POTENTIAL_EDGE_WEIGHT = bool(int(os.getenv("E8_USE_POTENTIAL_EDGE_WEIGHT", "1")))  # Scale edge weight by averaged potentials

# KD-tree rerank configuration
E8_USE_POTENTIAL_IN_KDTREE = bool(int(os.getenv("E8_USE_POTENTIAL_IN_KDTREE", "1")))  # Lower score for higher potential (promotes high-potential nodes)
E8_POTENTIAL_KDTREE_FACTOR = float(os.getenv("E8_POTENTIAL_KDTREE_FACTOR", "0.25"))  # Strength of that adjustment (0..1 reasonable)

# ===== Q(t) EMERGENCE UTILITY FUNCTIONS =====

def get_cosmic_time_parameter(step: int) -> float:
    """Compute cosmic time parameter t' with τ-shaped mapping for present-day headroom.
    
    τ-shaped function provides:
    - Early time: slow emergence (low τ factor)
    - Mid time: rapid growth phase  
    - Late time: asymptotic approach to headroom limit
    """
    try:
        # τ-shaped mapping: t' = τ * (step/1000) * (1 + tanh((step-1000)/500))
        # This creates a tau-like curve with inflection around step 1000
        base_time = step / 1000.0
        tau_shape = 1.0 + math.tanh((step - 1000.0) / 500.0)
        return E8_Q_TAU * base_time * tau_shape
    except Exception:
        return 0.40 * (step / 1000.0)  # Fallback to surgical value

def scenario_scale_for_bh(mass: float, dim: int) -> float:
    """Scenario-dependent emergence rate scaling for non-uniform BH dynamics.
    
    Based on test_q_t_comparison.py surgical fixes:
    - Lower dimensions (8D/16D): More constrained, slower emergence
    - Higher dimensions (32D/64D): More degrees of freedom, faster emergence
    """
    try:
        # Tuned coefficients from surgical fixes validation
        base_scale = 1.0
        
        # Dimension scaling: higher dims → faster emergence
        if dim <= 16:
            dim_factor = 0.75  # Slower for 8D/16D 
        elif dim <= 32:
            dim_factor = 1.0   # Baseline for 32D
        else:
            dim_factor = 1.25  # Faster for 64D+
            
        # Mass scaling: larger mass → more inertia, slower emergence  
        if mass < 0.5:
            mass_factor = 1.2   # Small mass, faster dynamics
        elif mass < 1.0:
            mass_factor = 1.0   # Medium mass, baseline
        else:
            mass_factor = 0.8   # Large mass, slower dynamics
            
        return base_scale * dim_factor * mass_factor
        
    except Exception:
        return 1.0  # Safe fallback

def compute_emergence_q_t(step: int, mass: float = 1.0, dim: int = 32) -> float:
    """Compute Q(t) emergence with scenario-dependent scaling and present-day headroom.
    
    Q(t) = Q∞ * (1 - e^(-sQ*t)) where:
    - Q∞ = E8_Q_ALPHA * E8_Q_INFINITY (asymptotic limit with headroom)  
    - sQ = E8_Q_SCALE * scenario_scale_for_bh(mass, dim) (emergence rate)
    - t = get_cosmic_time_parameter(step) (cosmic time with τ calibration)
    """
    try:
        # Get calibrated parameters
        q_infinity = E8_Q_ALPHA * E8_Q_INFINITY  # Present-day asymptotic headroom
        cosmic_time = get_cosmic_time_parameter(step)
        scenario_scale = scenario_scale_for_bh(mass, dim)
        emergence_rate = E8_Q_SCALE * scenario_scale
        
        # Q(t) emergence law with exponential approach  
        emergence_factor = 1.0 - np.exp(-emergence_rate * cosmic_time)
        q_t = q_infinity * emergence_factor
        
        return float(np.clip(q_t, 0.0, 1.0))
        
    except Exception:
        # Safe fallback using surgical constants
        t_prime = 0.40 * (step / 1000.0)
        q_inf = 0.65 * 1.0  
        s_q = 0.25 * 1.0
        q_t = q_inf * (1.0 - np.exp(-s_q * t_prime))
        return float(np.clip(q_t, 0.0, 1.0))

def compute_q_lock_correlation(mind_instance) -> float:
    """Compute Q↔LOCK correlation coefficient for BH cycle quality monitoring."""
    try:
        # Get recent Q(t) values and lock rates from memory operations
        q_values = []
        lock_rates = []
        
        # Extract recent BH events from log
        recent_events = getattr(mind_instance, 'black_hole_log', [])[-10:]  # Last 10 events
        
        for event in recent_events:
            if 'step' in event and 'mass' in event:
                step = event['step']
                mass = event.get('mass', 1.0)
                
                # Compute Q(t) for this event
                q_t = compute_emergence_q_t(step, mass)
                q_values.append(q_t)
                
                # Estimate lock rate (simplified: assume correlation with mass/size)
                size = event.get('size', 10)
                lock_rate = min(512.0, size * 25.6)  # Scale to ~512 max
                lock_rates.append(lock_rate)
        
        # Compute correlation if we have enough data
        if len(q_values) >= 3:
            q_array = np.array(q_values)
            lock_array = np.array(lock_rates)
            
            # Pearson correlation coefficient
            correlation = np.corrcoef(q_array, lock_array)[0, 1]
            return float(correlation) if not np.isnan(correlation) else 0.0
        else:
            return 0.0  # Insufficient data
            
    except Exception:
        return 0.0  # Safe fallback

def get_s_q_global(mind_instance) -> float:
    """Compute global emergence rate sQ from recent system dynamics."""
    try:
        # Get recent system activity metrics
        if hasattr(mind_instance, 'memory') and hasattr(mind_instance.memory, 'main_vectors'):
            vector_count = len(mind_instance.memory.main_vectors)
            step = getattr(mind_instance, 'step', 1000)
            
            # Compute effective global emergence rate
            s_q_global = E8_Q_SCALE * (vector_count / max(1000, step)) * 1000.0
            return float(np.clip(s_q_global, 0.0, 1.0))
        else:
            return E8_Q_SCALE  # Fallback to base rate
            
    except Exception:
        return 0.25  # Safe fallback

def get_lock_rate_512(mind_instance) -> float:
    """Compute lock rate scaled to 512 max from recent memory operations."""
    try:
        # Get recent consolidation activity
        if hasattr(mind_instance, 'black_hole_log'):
            recent_events = mind_instance.black_hole_log[-5:]  # Last 5 events
            if recent_events:
                total_size = sum(event.get('size', 0) for event in recent_events)
                avg_size = total_size / len(recent_events)
                lock_rate = min(512.0, avg_size * 25.6)  # Scale to 512 max
                return float(lock_rate)
        
        return 0.0  # No recent activity
        
    except Exception:
        return 0.0  # Safe fallback

def check_drift_sentinels(mind_instance, step: int) -> List[str]:
    """Check for Q(t) emergence drift anomalies and return warning messages."""
    warnings = []
    
    try:
        # Get current metrics
        recent_events = getattr(mind_instance, 'black_hole_log', [])[-5:]  # Last 5 events
        if len(recent_events) < 3:
            return []  # Need enough history
        
        # Compute Q(t) values for recent events
        q_values = []
        s_q_values = []
        
        for event in recent_events:
            if 'step' in event and 'mass' in event:
                event_step = event['step']
                mass = event.get('mass', 1.0)
                
                q_t = compute_emergence_q_t(event_step, mass)
                s_q = get_s_q_global(mind_instance)
                
                q_values.append(q_t)
                s_q_values.append(s_q)
        
        if len(q_values) >= 3:
            q_array = np.array(q_values)
            s_q_array = np.array(s_q_values)
            
            # 1. Flatline Detection: Q(t) variance too low
            q_variance = np.var(q_array)
            if q_variance < 0.001:  # Very low variance threshold
                warnings.append(f"[FLATLINE] Q(t) variance = {q_variance:.5f} < 0.001 - emergence may be stalled")
            
            # 2. sQ Explosion Detection: sQ_global too high
            max_s_q = np.max(s_q_array)
            if max_s_q > 0.8:  # High sQ threshold
                warnings.append(f"[sQ_EXPLOSION] sQ_global = {max_s_q:.3f} > 0.8 - emergence rate critically high")
            
            # 3. Q/LOCK Divergence: Check correlation health
            q_lock_correlation = compute_q_lock_correlation(mind_instance)
            if q_lock_correlation < 0.3 and len(recent_events) >= 4:  # Correlation threshold with sufficient data
                warnings.append(f"[Q_LOCK_DIVERGENCE] Q↔LOCK correlation = {q_lock_correlation:.3f} < 0.3 - consolidation quality degraded")
            
    except Exception as e:
        warnings.append(f"[SENTINEL_ERROR] Failed to check drift sentinels: {e}")
    
    return warnings

def log_cycle_metrics(mind_instance, step: int, q_t: float, s_q_global: float, lock_rate_512: float, 
                      q_lock_correlation: float, target_comp: float, beta_used: float):
    """Log cycle metrics to runtime/cycle_metrics.jsonl for production monitoring."""
    try:
        import json
        import os
        from datetime import datetime
        
        # Ensure runtime directory exists
        runtime_dir = "runtime"
        if not os.path.exists(runtime_dir):
            os.makedirs(runtime_dir)
        
        # Prepare metrics data
        metrics_data = {
            "timestamp": datetime.now().isoformat(),
            "step": step,
            "Q_t": q_t,
            "sQ_global": s_q_global,
            "t_prime": get_cosmic_time_parameter(step),
            "lock_rate_512": lock_rate_512,
            "Q_LOCK_correlation": q_lock_correlation,
            "target_comp": target_comp,
            "beta_used": beta_used
        }
        
        # Append to cycle metrics file (NDJSON format)
        metrics_file = os.path.join(runtime_dir, "cycle_metrics.jsonl")
        with open(metrics_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(metrics_data) + "\n")
            
    except Exception as e:
        # Fail silently to avoid disrupting BH cycles
        if hasattr(mind_instance, 'console'):
            mind_instance.console.log(f"[yellow]Failed to log cycle metrics: {e}[/yellow]")

# ===== LOCK-RATE TRACKING HELPERS (M23 Memory-Curvature) =====

def track_proximity_outcome(mind_instance, outcome_type: str, details: dict):
    """Track proximity event outcomes for lock-rate correlation analysis."""
    try:
        if not hasattr(mind_instance, '_proximity_outcomes'):
            mind_instance._proximity_outcomes = deque(maxlen=100)
        
        outcome_entry = {
            "timestamp": time.time(),
            "step": getattr(mind_instance, "step_num", 0),
            "type": outcome_type,  # "success", "failure", "timeout", etc.
            "details": details
        }
        mind_instance._proximity_outcomes.append(outcome_entry)
        
        # Log for debugging if enabled
        if str(os.getenv("E8_CURVATURE_DEBUG", "0")) == "1":
            if hasattr(mind_instance, 'console'):
                mind_instance.console.log(f"[PROX-TRACK] {outcome_type}: {details}")
                
    except Exception as e:
        if hasattr(mind_instance, 'console'):
            mind_instance.console.log(f"[PROX-TRACK] Error tracking outcome: {e}")

def compute_proximity_lock_correlation(mind_instance) -> float:
    """Compute correlation between proximity success rate and lock formation."""
    try:
        if not hasattr(mind_instance, '_proximity_outcomes'):
            return 0.0
            
        outcomes = list(mind_instance._proximity_outcomes)
        if len(outcomes) < 5:
            return 0.0
            
        # Success rate over recent outcomes
        recent_outcomes = outcomes[-20:]  # Last 20 outcomes
        success_count = sum(1 for o in recent_outcomes if o.get("type") == "success")
        success_rate = success_count / len(recent_outcomes)
        
        # Lock rate
        lock_rate = get_lock_rate_512(mind_instance) / 512.0  # Normalize to [0,1]
        
        # Simple correlation proxy: abs difference from ideal pairing
        # High success rate should correlate with moderate lock rate
        ideal_lock_for_success = 0.6  # Moderate consolidation
        correlation_proxy = 1.0 - abs(success_rate - ideal_lock_for_success)
        
        return float(np.clip(correlation_proxy, 0.0, 1.0))
        
    except Exception:
        return 0.0

def get_curvature_field_intensity(mind_instance) -> float:
    """Get current intensity of the memory-curvature field."""
    try:
        if hasattr(mind_instance, 'curvature_field') and mind_instance.curvature_field:
            return mind_instance.curvature_field.get_field_intensity()
        return 0.0
    except Exception:
        return 0.0

# ===== UNIFIED RATING SYSTEM =====
# Comprehensive framework for consistent, calibrated ratings throughout E8 Mind

from enum import Enum
from dataclasses import dataclass, field
from typing import Optional, Dict, List, Any
import threading
import time

class RatingTypes(Enum):
    """Distinct types of ratings used in the system"""
    CONCEPT_QUALITY = "concept_quality"           # How good/valuable is this idea?
    VALIDATION_CONFIDENCE = "validation_confidence"  # How sure are we about validation?
    MEMORY_STRENGTH = "memory_strength"           # How important to remember/retrieve?
    SYNTHESIS_QUALITY = "synthesis_quality"       # How well does synthesis combine concepts?

@dataclass
class RatingComponents:
    """Multi-dimensional rating structure"""
    novelty: float = 0.5      # 0.0-1.0, how novel/unique
    coherence: float = 0.5    # 0.0-1.0, how logical/meaningful  
    utility: float = 0.5      # 0.0-1.0, how practically useful
    confidence: float = 0.5   # 0.0-1.0, how certain we are about these ratings
    
    def __post_init__(self):
        """Ensure all components are in valid range"""
        self.novelty = float(np.clip(self.novelty, 0.0, 1.0))
        self.coherence = float(np.clip(self.coherence, 0.0, 1.0))
        self.utility = float(np.clip(self.utility, 0.0, 1.0))
        self.confidence = float(np.clip(self.confidence, 0.0, 1.0))

@dataclass
class RatingMetadata:
    """Provenance and metadata for ratings"""
    value: float
    rating_type: RatingTypes
    source: str  # "llm_gpt4", "formula_synthesis", "validator_feedback", etc.
    timestamp: float
    model_version: Optional[str] = None
    confidence: float = 0.5
    components: Optional[RatingComponents] = None
    context: str = ""

class RatingValidator:
    """Validates rating changes and detects anomalies"""
    
    VALID_LARGE_CHANGE_REASONS = {
        "validator_feedback", "synthesis_boost", "novel_discovery", 
        "coherence_degradation", "validation_failure"
    }
    
    @staticmethod
    def validate_rating_change(old_rating: float, new_rating: float, 
                              reason: str = "") -> bool:
        """Prevent extreme rating changes without justification"""
        if old_rating is None or new_rating is None:
            return True
            
        delta = abs(new_rating - old_rating)
        if delta > 0.3:  # Large change threshold
            return reason in RatingValidator.VALID_LARGE_CHANGE_REASONS
        return True
        
    @staticmethod
    def detect_rating_anomalies(ratings: List[float]) -> List[str]:
        """Detect suspicious rating patterns"""
        if not ratings:
            return []
            
        issues = []
        ratings_array = np.array(ratings)
        
        # All ratings identical
        if len(set(ratings)) == 1:
            issues.append("All ratings identical")
            
        # Ratings too clustered (low variance)
        if len(ratings) > 3 and np.std(ratings_array) < 0.05:
            issues.append("Ratings too clustered (std < 0.05)")
            
        # Ratings at extremes
        extreme_count = sum(1 for r in ratings if r < 0.1 or r > 0.9)
        if extreme_count / len(ratings) > 0.8:
            issues.append("Too many extreme ratings")
            
        return issues

class UnifiedRatingSystem:
    """Central rating calculation and calibration system"""
    
    def __init__(self, console=None):
        self.console = console
        self.weights = {
            RatingTypes.CONCEPT_QUALITY: {"novelty": 0.3, "coherence": 0.4, "utility": 0.3},
            RatingTypes.SYNTHESIS_QUALITY: {"novelty": 0.35, "coherence": 0.65, "utility": 0.0},
            RatingTypes.VALIDATION_CONFIDENCE: {"novelty": 0.1, "coherence": 0.6, "utility": 0.3},
            RatingTypes.MEMORY_STRENGTH: {"novelty": 0.4, "coherence": 0.3, "utility": 0.3}
        }
        self.model_stats = {}
        self.validation_history = deque(maxlen=1000)
        self._rating_lock = threading.Lock()
        
    def calculate_rating(self, components: RatingComponents, 
                        rating_type: RatingTypes = RatingTypes.CONCEPT_QUALITY) -> float:
        """Calculate unified rating from components"""
        weights = self.weights.get(rating_type, self.weights[RatingTypes.CONCEPT_QUALITY])
        
        base_rating = (
            weights["novelty"] * components.novelty + 
            weights["coherence"] * components.coherence + 
            weights["utility"] * components.utility
        )
        
        # Weight by confidence - low confidence pulls toward neutral (0.5)
        final_rating = base_rating * components.confidence + (1 - components.confidence) * 0.5
        
        return float(np.clip(final_rating, 0.0, 1.0))
        
    def calibrate_rating(self, raw_rating: float, model: str, 
                        validation_outcome: Optional[str] = None) -> float:
        """Improved rating calibration using validation feedback"""
        with self._rating_lock:
            # Track validation outcomes for calibration
            if validation_outcome:
                self.validation_history.append({
                    'rating': raw_rating, 
                    'outcome': validation_outcome,
                    'model': model,
                    'timestamp': time.time()
                })
            
            # Simple percentile-based calibration (more robust than mean-centering)
            model_key = model.lower()
            if model_key not in self.model_stats:
                self.model_stats[model_key] = {'ratings': deque(maxlen=200), 'initialized': False}
            
            stats = self.model_stats[model_key]
            stats['ratings'].append(raw_rating)
            
            # Only apply calibration after we have enough samples
            if len(stats['ratings']) < 20:
                return float(np.clip(raw_rating, 0.0, 1.0))
            
            # Use quantile mapping to map to uniform distribution
            ratings_array = np.array(list(stats['ratings']))
            percentile = np.searchsorted(np.sort(ratings_array), raw_rating) / len(ratings_array)
            
            # Map percentile back to 0-1 range with slight compression toward center
            calibrated = percentile * 0.9 + 0.05  # Compress to [0.05, 0.95]
            
            return float(np.clip(calibrated, 0.0, 1.0))
    
    def create_rating_metadata(self, value: float, rating_type: RatingTypes, 
                              source: str, components: Optional[RatingComponents] = None,
                              context: str = "") -> RatingMetadata:
        """Create rating metadata for provenance tracking"""
        return RatingMetadata(
            value=value,
            rating_type=rating_type,
            source=source,
            timestamp=time.time(),
            components=components,
            context=context
        )
    
    def apply_rating_floor(self, raw_rating: float, teacher_config: dict) -> float:
        """Apply dynamic rating floor with cosine schedule."""
        if not teacher_config.get("rating_floor_schedule", {}).get("enabled", True):
            return max(raw_rating, teacher_config.get("rating_floor", 0.5))
        
        schedule = teacher_config["rating_floor_schedule"]
        current_step = schedule["current_step"]
        schedule_steps = schedule["schedule_steps"]
        temp_floor = schedule["temp_floor"]
        original_floor = teacher_config["rating_floor_original"]
        
        # Cosine schedule from temp_floor back to original_floor
        if current_step < schedule_steps:
            progress = current_step / schedule_steps
            cosine_factor = 0.5 * (1 + math.cos(math.pi * progress))
            current_floor = original_floor + (temp_floor - original_floor) * cosine_factor
        else:
            current_floor = original_floor
        
        # Update current floor in config for tracking
        teacher_config["rating_floor"] = current_floor
        
        return max(raw_rating, current_floor)

# Global rating system instance (will be initialized by E8Mind)
unified_rating_system: Optional[UnifiedRatingSystem] = None
# Retrolink count already influenced by E8_RETROLINK_BASE / E8_RETROLINK_CAP.
TEMP_HALF_LIFE_VIVID = 8
TEMP_HALF_LIFE_HOT = 24
TEMP_HALF_LIFE_WARM = 72
TEMP_HALF_LIFE_COLD = 240
SEMANTIC_DOMAIN = os.getenv("E8_SEMANTIC_DOMAIN", "E8 loop quantum gravity")
E8_SEED_DOMAIN = os.getenv("E8_SEED_DOMAIN", "1") == "1"
E8_SEED_LABEL  = os.getenv("E8_SEED_LABEL", "1") == "1"
E8_PROFILE_ENABLED = os.getenv("E8_PROFILE_ENABLED", "1") == "1"
E8_PROFILE_MODE = (os.getenv("E8_PROFILE_MODE", "auto") or "auto").lower()
E8_PROMPT_DOMAIN = os.getenv("E8_PROMPT_DOMAIN", "1") == "1"
E8_DISABLE_RETROLINK_CAP = os.getenv("E8_DISABLE_RETROLINK_CAP", "0") == "0"
E8_DISABLE_VALIDATOR_WRITEBACK = os.getenv("E8_DISABLE_VALIDATOR_WRITEBACK", "0") == "0"
E8_DISABLE_BH_TELEMETRY = os.getenv("E8_DISABLE_BH_TELEMETRY", "0") == "0"
E8_DISABLE_KDTREE_GAUGE = os.getenv("E8_DISABLE_KDTREE_GAUGE", "0") == "0"
E8_DISABLE_SNAPSHOT_ROTATION = os.getenv("E8_DISABLE_SNAPSHOT_ROTATION", "0") == "0"
E8_HIDE_DIMMED_LOGS = os.getenv("E8_HIDE_DIMMED_LOGS", "1") == "1"  # Toggle to completely hide dimmed console.log() output

# Training and filtering constants
SENTINEL_LOGPROB_THRESHOLD = -90.0  # Values <= this are considered invalid sentinel values
TRAINING_HISTORY_MAXLEN = 1024      # Maximum size of training call history deque

if E8_PROMPT_DOMAIN:
    try:
        # If an environment variable is set, prefer it and avoid interactive prompt
        _env_dom = os.getenv("E8_SEMANTIC_DOMAIN")
        if _env_dom:
            SEMANTIC_DOMAIN = _env_dom
        else:
            # Only prompt if running in an interactive TTY
            if hasattr(sys, "stdin") and sys.stdin and sys.stdin.isatty():
                _default_dom = os.getenv("E8_SEMANTIC_DOMAIN", SEMANTIC_DOMAIN)
                try:
                    _inp = input(f"[E8] Semantic domain? [{_default_dom}]: ").strip()
                except Exception:
                    _inp = ''
                SEMANTIC_DOMAIN = _inp if _inp else _default_dom
                os.environ["E8_SEMANTIC_DOMAIN"] = SEMANTIC_DOMAIN
            else:
                # Non-interactive: keep default SEMANTIC_DOMAIN
                pass
    except Exception:
        pass

DREAM_MODE_ENABLED = True
LOCAL_GEN_WORKERS = int(os.getenv("LOCAL_GEN_WORKERS", "2"))  # increased default concurrency to 4

# --- CPU Thread Configuration ---
try:
    import multiprocessing as _mp
    _cpu_count = max(1, _mp.cpu_count())
except Exception:
    _cpu_count = 4
E8_CPU_THREADS = int(os.getenv("E8_CPU_THREADS", str(min(24, _cpu_count))))
for _var in ("OMP_NUM_THREADS", "MKL_NUM_THREADS", "NUMEXPR_NUM_THREADS", "OPENBLAS_NUM_THREADS"):
    if _var not in os.environ:
        os.environ[_var] = str(E8_CPU_THREADS)
try:
    if 'torch' in globals() and hasattr(torch, 'set_num_threads'):
        torch.set_num_threads(E8_CPU_THREADS)
        if hasattr(torch, 'set_num_interop_threads'):
            torch.set_num_interop_threads(max(1, min(E8_CPU_THREADS // 2, 8)))
except Exception:
    pass
GLOBAL_SEED = int(os.getenv("GLOBAL_SEED", "1337"))

# --- Deterministic Seeding Utilities (Phase 2) ---
_SEED_ALL_RAN = False
def seed_all(seed: int):
    """Seed all available RNG sources deterministically.
    Idempotent: running multiple times won't re-randomize torch dropout states etc."""
    global _SEED_ALL_RAN
    if _SEED_ALL_RAN:
        return
    try:
        import random
        random.seed(seed)
        os.environ["PYTHONHASHSEED"] = str(seed)
    except Exception:
        pass
    try:
        import numpy as _np
        _np.random.seed(seed)
    except Exception:
        pass
    try:
        import torch as _t
        try:
            _t.manual_seed(seed)
            if _t.cuda.is_available():  # type: ignore[attr-defined]
                _t.cuda.manual_seed_all(seed)  # type: ignore[attr-defined]
        except Exception:
            pass
    except Exception:
        pass
    _SEED_ALL_RAN = True

def deterministic_embedding_stub(text: str, dim: int, master_seed: int) -> np.ndarray:
    """Deterministic embedding stub used when provider embedding fails or placeholder active.

    Uses Adler32 hash combined with master_seed to seed a local RNG; returns a
    normalized float32 vector of requested dimension.
    """
    try:
        import zlib
        base = zlib.adler32(text.encode("utf-8")) & 0xFFFFFFFF
    except Exception:
        base = sum(ord(c) for c in text) & 0xFFFFFFFF
    mix = (base ^ (master_seed & 0xFFFFFFFF)) & 0xFFFFFFFF
    rng = np.random.default_rng(mix)
    vec = rng.standard_normal(dim).astype(np.float32)
    n = float(np.linalg.norm(vec)) or 1.0
    return (vec / n).astype(np.float32)

DATA_SOURCES: Dict[str, Any] = {
    # AI + Memory (Hopfield, Kanerva, VSA, episodic/associative)
    "arxiv_ai_memory": {
        "type": "arxiv_api",
        "url": (
            "http://export.arxiv.org/api/query?"
            "search_query=%28cat:cs.AI+OR+cat:cs.LG+OR+cat:cs.NE+OR+cat:q-bio.NC%29"
            "+AND+%28all:memory+OR+all:%22episodic+memory%22+OR+all:%22associative+memory%22"
            "+OR+ti:Hopfield+OR+all:Kanerva+OR+all:%22vector+symbolic+architecture%22%29"
            "&sortBy=submittedDate&sortOrder=descending&max_results=1"
        ),
        "schedule_minutes": 30,
    },

    # Geometry + E8 (math + math-phys + hep-th)
    "arxiv_geometry_e8": {
        "type": "arxiv_api",
        "url": (
            "http://export.arxiv.org/api/query?"
            "search_query=%28cat:math.DG+OR+cat:math.GR+OR+cat:math.RT+OR+cat:math-ph+OR+cat:hep-th%29"
            "+AND+%28%28all:E8+AND+%28all:lattice+OR+all:%22Lie%22%29%29"
            "+OR+all:%22E8+lattice%22+OR+all:%22E8+root+system%22%29"
            "&sortBy=submittedDate&sortOrder=descending&max_results=1"
        ),
        "schedule_minutes": 30,
    },

    # Holography (AdS/CFT, gauge/gravity)
    "arxiv_holography": {
        "type": "arxiv_api",
        "url": (
            "http://export.arxiv.org/api/query?"
            "search_query=%28cat:hep-th+OR+cat:gr-qc+OR+cat:math-ph%29"
            "+AND+%28all:holography+OR+all:%22holographic+principle%22+OR+all:%22AdS/CFT%22"
            "+OR+all:%22gauge/gravity%22%29"
            "&sortBy=submittedDate&sortOrder=descending&max_results=1"
        ),
        "schedule_minutes": 30,
    },

    # AI x Geometry / Symmetry
    "arxiv_ai_geometry": {
        "type": "arxiv_api",
        "url": (
            "http://export.arxiv.org/api/query?"
            "search_query=%28cat:cs.LG+OR+cat:cs.AI+OR+cat:cs.NE%29"
            "+AND+%28all:%22geometric+deep+learning%22+OR+all:%22group+equivariant%22"
            "+OR+all:%22Lie+group%22+OR+all:symmetry+OR+all:%22representation+theory%22%29"
            "&sortBy=submittedDate&sortOrder=descending&max_results=1"
        ),
        "schedule_minutes": 30,
    },

    # --- Connecting fields: physics x neurology / science ---

    # Neurophysics / Connectomics / Criticality
    "arxiv_neurophysics": {
        "type": "arxiv_api",
        "url": (
            "http://export.arxiv.org/api/query?"
            "search_query=%28cat:physics.bio-ph+OR+cat:cond-mat.stat-mech+OR+cat:physics.data-an+OR+cat:q-bio.NC%29"
            "+AND+%28all:neural+OR+all:neuronal+OR+all:brain+OR+all:connectome+OR+all:criticality%29"
            "&sortBy=submittedDate&sortOrder=descending&max_results=1"
        ),
        "schedule_minutes": 30,
    },

    # RL Ã— Neuroscience (dopamine / basal ganglia / TD)
    "arxiv_rl_neuroscience": {
        "type": "arxiv_api",
        "url": (
            "http://export.arxiv.org/api/query?"
            "search_query=%28cat:cs.AI+OR+cat:cs.LG+OR+cat:q-bio.NC%29"
            "+AND+%28all:%22reinforcement+learning%22+OR+all:dopamine+OR+all:%22basal+ganglia%22"
            "+OR+all:%22temporal+difference%22%29"
            "&sortBy=submittedDate&sortOrder=descending&max_results=1"
        ),
        "schedule_minutes": 30,
    },

    # Neuroimaging Ã— ML (EEG / fMRI / MEG)
    "arxiv_neuroimaging_ml": {
        "type": "arxiv_api",
        "url": (
            "http://export.arxiv.org/api/query?"
            "search_query=%28cat:eess.IV+OR+cat:q-bio.NC+OR+cat:stat.ML%29"
            "+AND+%28all:EEG+OR+all:fMRI+OR+all:MEG%29"
            "+AND+%28all:%22machine+learning%22+OR+all:%22deep+learning%22%29"
            "&sortBy=submittedDate&sortOrder=descending&max_results=1"
        ),
        "schedule_minutes": 30,
    },

    # Information theory Ã— Brain (IB, FEP, predictive coding, active inference)
    "arxiv_information_brain": {
        "type": "arxiv_api",
        "url": (
            "http://export.arxiv.org/api/query?"
            "search_query=%28cat:cs.IT+OR+cat:stat.ML+OR+cat:physics.data-an+OR+cat:q-bio.NC%29"
            "+AND+%28all:%22information+bottleneck%22+OR+all:%22free+energy+principle%22"
            "+OR+all:%22predictive+coding%22+OR+all:%22active+inference%22%29"
            "&sortBy=submittedDate&sortOrder=descending&max_results=1"
        ),
        "schedule_minutes": 30,
    },

    # Neuromorphic hardware / Spiking / Memristors / STDP
    "arxiv_neuromorphic_hardware": {
        "type": "arxiv_api",
        "url": (
            "http://export.arxiv.org/api/query?"
            "search_query=%28cat:cs.AR+OR+cat:eess.SP+OR+cat:eess.EL%29"
            "+AND+%28all:neuromorphic+OR+all:%22spiking+neural%22+OR+all:memristor"
            "+OR+all:%22spike-timing-dependent+plasticity%22+OR+all:STDP%29"
            "&sortBy=submittedDate&sortOrder=descending&max_results=1"
        ),
        "schedule_minutes": 30,
    },

    # Complex systems / Network neuroscience (incl. GNNs)
    "arxiv_complex_networks_brain": {
        "type": "arxiv_api",
        "url": (
            "http://export.arxiv.org/api/query?"
            "search_query=%28cat:nlin.AO+OR+cat:physics.soc-ph+OR+cat:cs.SI+OR+cat:math.DS%29"
            "+AND+%28all:%22complex+systems%22+OR+all:%22complex+networks%22"
            "+OR+all:%22network+neuroscience%22+OR+all:%22graph+neural%22%29"
            "&sortBy=submittedDate&sortOrder=descending&max_results=1"
        ),
        "schedule_minutes": 30,
    },

    # Quantum Ã— ML (QML)
    "arxiv_quantum_ml": {
        "type": "arxiv_api",
        "url": (
            "http://export.arxiv.org/api/query?"
            "search_query=%28cat:quant-ph+OR+cat:cs.LG%29"
            "+AND+%28all:%22quantum+machine+learning%22+OR+all:%22variational+quantum%22"
            "+OR+all:%22quantum+kernel%22%29"
            "&sortBy=submittedDate&sortOrder=descending&max_results=1"
        ),
        "schedule_minutes": 30,
    },

    # Psychophysics / Cognitive science on arXiv
    "arxiv_psychophysics_cogsci": {
        "type": "arxiv_api",
        "url": (
            "http://export.arxiv.org/api/query?"
            "search_query=%28cat:q-bio.NC+OR+cat:cs.LG+OR+cat:stat.ML%29"
            "+AND+%28all:psychophysics+OR+all:%22cognitive+science%22+OR+all:cognition+OR+all:%22perceptual+learning%22%29"
            "&sortBy=submittedDate&sortOrder=descending&max_results=1"
        ),
        "schedule_minutes": 30,
    },

    # History & Philosophy of Physics (metaphysics-of-physics)
    "arxiv_history_philosophy_physics": {
        "type": "arxiv_api",
        "url": (
            "http://export.arxiv.org/api/query?"
            "search_query=cat:physics.hist-ph+AND+%28all:metaphysics+OR+all:%22philosophy+of+physics%22+OR+all:%22foundations+of+physics%22%29"
            "&sortBy=submittedDate&sortOrder=descending&max_results=1"
        ),
        "schedule_minutes": 30,
    },

    # â€”â€”â€” Non-arXiv sources (psychology & metaphysics) â€”â€”â€”

    # PubMed: psychology Ã— memory/cognition
    "pubmed_psychology_memory": {
        "type": "pubmed_api",
        "url": (
            "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"
            "db=pubmed&retmode=json&retmax=3&sort=date&"
            "term=%28psychology%5BTitle/Abstract%5D+OR+%28psychology%5BMeSH+Terms%5D%29%29+AND+"
            "%28memory%5BTitle/Abstract%5D+OR+cognition%5BTitle/Abstract%5D%29"
        ),
        "schedule_minutes": 30,
    },

    # PubMed: cognitive neuroscience / predictive coding / active inference
    "pubmed_cogneuro_predictive": {
        "type": "pubmed_api",
        "url": (
            "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?"
            "db=pubmed&retmode=json&retmax=3&sort=date&"
            "term=%28%22cognitive+neuroscience%22%5BTitle/Abstract%5D+OR+%22predictive+coding%22%5BTitle/Abstract%5D+OR+"
            "%22active+inference%22%5BTitle/Abstract%5D%29"
        ),
        "schedule_minutes": 30,
    },

    # OpenAlex: psychology / cognitive science (broad scholarly index)
    "openalex_psychology": {
        "type": "json_api",
        "url": (
            "https://api.openalex.org/works?"
            "search=psychology+cognitive+science&per-page=1&sort=publication_date:desc"
        ),
        "schedule_minutes": 30,
    },

    # OpenAlex: metaphysics / philosophy of mind
    "openalex_metaphysics": {
        "type": "json_api",
        "url": (
            "https://api.openalex.org/works?"
            "search=metaphysics+%22philosophy+of+mind%22&per-page=1&sort=publication_date:desc"
        ),
        "schedule_minutes": 30,
    },

    # Crossref: metaphysics (recent journal articles)
    "crossref_metaphysics": {
        "type": "json_api",
        "url": (
            "https://api.crossref.org/works?"
            "query=metaphysics+%22philosophy+of+mind%22&filter=type:journal-article&rows=1&sort=published&order=desc"
        ),
        "schedule_minutes": 30,
    },

    # Crossref: psychophysics / perception (recent journal articles)
    "crossref_psychophysics": {
        "type": "json_api",
        "url": (
            "https://api.crossref.org/works?"
            "query=psychophysics+perception+psychoacoustics&filter=type:journal-article&rows=1&sort=published&order=desc"
        ),
        "schedule_minutes": 30,
    },

    # Local docs / ingestion
    "research_ingest": {"type": "research_ingest", "max_total": 60, "schedule_minutes": 30},
}


# Optional Dependencies & Stubs
try:
    from itertools import combinations
except Exception:
    combinations = None
try:
    import ollama
except Exception:
    ollama = None
try:
    import google.generativeai as genai
except Exception:
    genai = None
try:
    import clifford
    CLIFFORD_AVAILABLE = True
except Exception:
    clifford = None
    CLIFFORD_AVAILABLE = False
try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except Exception:
    nx = None
    NETWORKX_AVAILABLE = False
try:
    from rich.console import Console
    from rich.panel import Panel
    from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn
    from rich.markup import escape
except Exception:
    class Progress:
        def __init__(self, *args, **kwargs): pass
        def __enter__(self): return self
        def __exit__(self, exc_type, exc_val, exc_tb): pass
        def add_task(self, *args, **kwargs): return 0
        def update(self, *args, **kwargs): pass
    class Console:
        def __init__(self, record: bool=False): pass
        def log(self, *a, **k): print(*a)
        def print(self, *a, **k): print(*a)
        def rule(self, *a, **k): print('-'*40)
        def export_text(self): return ""
        def print_exception(self, *a, **k):
            import traceback as _tb; _tb.print_exc()
        def __enter__(self): return self
        def __exit__(self, exc_type, exc_val, exc_tb): pass
    class Panel(str):
        def __new__(cls, content, **kwargs): return str(content)
    def escape(s): return s
try:
    import networkx as nx
    from networkx.algorithms import community as nx_comm
    from networkx.readwrite import json_graph
except Exception:
    nx = None; nx_comm = None
    class _JG:
        def node_link_data(self, g): return {"nodes": [], "links": []}
    json_graph = _JG()

try:
    from sklearn.decomposition import PCA
    from sklearn.cluster import DBSCAN
except Exception:
    PCA, DBSCAN = None, None

try:
    from sklearn.neighbors import KDTree as _SKKDTree
    from sklearn.metrics.pairwise import cosine_distances as _sk_cosine_distances, cosine_similarity as _sk_cosine_similarity
except Exception:
    _SKKDTree = None
    _sk_cosine_distances, _sk_cosine_similarity = None, None

try:
    from scipy.spatial import cKDTree as _SPKDTree
    from scipy.sparse import csr_matrix, diags
    from scipy.sparse.linalg import eigsh, expm_multiply
    import scipy as sp
except Exception:
    _SPKDTree, csr_matrix, diags, eigsh, expm_multiply, sp = None, None, None, None, None, None

# Optional FAISS support (used by KDTree wrapper when available)
try:
    import faiss  # type: ignore
    HAVE_FAISS = True
except Exception:
    faiss = None  # type: ignore
    HAVE_FAISS = False

# Latent Planner Stubs
class LatentDiffusionProposer:
    def __init__(self, action_dim:int, **kwargs):
        self.action_dim = int(action_dim)
    def propose(self, *args, **kwargs): return None

class MacroManager:
    def __init__(self, layout, action_dim:int, **kwargs):
        self.layout = layout
        self.action_dim = int(action_dim)
    # NOTE: Made synchronous to avoid un-awaited coroutine warnings in action loop.
    # If future implementations require async (e.g., calling an LLM), wrap that
    # logic internally and return a best-effort cached result instead of a coroutine.
    def select_action(self, state, mind):
        try:
            return np.zeros(self.action_dim, dtype=np.float32)
        except Exception:
            return np.zeros(self.action_dim, dtype=np.float32)

class LatentCEMPlanner:
    def __init__(self, action_dim:int, **kwargs):
        self.action_dim = int(action_dim)
    def plan(self, *args, **kwargs): return None

# Helper Functions and Classes
def get_run_id() -> str:
    try:
        return datetime.now(timezone.utc).strftime("run_%Y%m%d_%H%M%S")
    except Exception:
        return time.strftime("run_%Y%m%d_%H%M%S", time.gmtime())

def get_path(rel: str, run_id: str) -> str:
    base = os.path.join(RUNTIME_DIR, str(run_id)) if run_id else RUNTIME_DIR
    path = os.path.join(base, rel)
    os.makedirs(os.path.dirname(path), exist_ok=True)
    return path

def mood_get(mood_vector: dict, key: str, default: float = 0.5) -> float:
    return float(mood_vector.get(key, default))

def sanitize_line(text: str, max_chars: int = 80) -> str:
    if not isinstance(text, str): return ""
    return text.replace('\n', ' ').replace('\r', '').strip()[:max_chars]

def sanitize_block(text: str, max_sentences: int = 5, max_chars: int = 500) -> str:
    if not isinstance(text, str): return ""
    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
    return " ".join(sentences[:max_sentences])[:max_chars]

def safe_json_write(filepath: str, data: Any):
    try:
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with tempfile.NamedTemporaryFile('w', delete=False, dir=os.path.dirname(filepath), encoding='utf-8') as tf:
            json.dump(data, tf, ensure_ascii=False, indent=2, cls=NumpyEncoder)
            tempname = tf.name
        os.replace(tempname, filepath)
    except Exception as e:
        logging.warning(f"Failed to write JSON to {filepath}: {e}")

def safe_json_read(filepath: str, default: Any = None) -> Any:
    if not os.path.exists(filepath): return default
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError) as e:
        logging.warning(f"Failed to read JSON from {filepath}: {e}")
        return default

def _parse_json_object(text: str) -> Dict:
    if not text: return {}
    match = re.search(r'\{.*?\}', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(0))
        except json.JSONDecodeError:
            pass
    return {}

def export_graph(graph: Any) -> Dict[str, Any]:
    if nx is None: return {"nodes": [], "links": []}
    try:
        # Use edges="edges" for forward compatibility with NetworkX 3.6+
        return json_graph.node_link_data(graph, edges="edges")
    except TypeError:
        # Fallback for older NetworkX versions
        return json_graph.node_link_data(graph)

# --- Sentinel / Safety Utilities ---
def filter_sentinel_values(arr: Any, sentinel_threshold: float = SENTINEL_LOGPROB_THRESHOLD):
    """Return a mask or filtered list excluding sentinel logprob/weight values.
    Accepts numpy arrays or lists. Values <= sentinel_threshold are considered invalid.
    """
    try:
        import numpy as _np
        if isinstance(arr, _np.ndarray):
            mask = arr > sentinel_threshold
            # If all masked out, fall back to uniform mask to avoid downstream empty reductions
            if not mask.any():
                return _np.ones_like(arr, dtype=bool)
            return mask
        if isinstance(arr, (list, tuple)):
            mask = [x > sentinel_threshold for x in arr]
            if not any(mask):
                return [True]*len(arr)
            return mask
    except Exception:
        pass
    return arr

def classify_geometry_theme(delta_vector: np.ndarray) -> list[str]:
    themes = set()
    magnitude = np.linalg.norm(delta_vector)

    if magnitude < 0.05:
        themes.add("stasis")
        return list(themes)

    if magnitude > 1.5: themes.add("burst")
    elif magnitude > 0.8: themes.add("growth")
    else: themes.add("drift")

    non_zero_elements = np.count_nonzero(np.abs(delta_vector) > 1e-4)
    if non_zero_elements <= 2: themes.add("focus")
    elif non_zero_elements >= 7: themes.add("integration")
    else: themes.add("shift")

    if np.var(delta_vector) > 0.2: themes.add("disorder")
    else: themes.add("coherence")

    return list(themes)

def teacher_prompt_ok(prompt: str, graph_terms: set[str], *, total_graph_nodes: int | None = None, telemetry_cb=None) -> bool:
    """Stricter teacher prompt acceptance with cold-start leniency & telemetry.

    Differences from previous logic:
      - Requires >0 overlap with graph terms (was allowing >=0 via a buggy condition).
      - Cold-start leniency threshold configurable via E8_MIN_GRAPH_BOOTSTRAP_N (default 50).
      - Optional telemetry callback receives structured acceptance info.
    """
    tokens = {t.lower() for t in re.findall(r"[A-Za-z0-9]+", prompt or "")}
    MIN_GRAPH_BOOTSTRAP_N = int(os.getenv("E8_MIN_GRAPH_BOOTSTRAP_N", "50"))
    graph_size = total_graph_nodes if total_graph_nodes is not None else len(graph_terms)
    common_terms = {"ai", "artificial", "intelligence", "machine", "learning", "neural", "network", "model", "data", "algorithm", "system", "memory", "graph", "node", "edge"}

    if graph_size < MIN_GRAPH_BOOTSTRAP_N:  # cold start leniency (relaxed)
        has_common = len(tokens & common_terms) > 0
        has_any_content = len(tokens) >= 3
        accepted = has_common or has_any_content
        if telemetry_cb:
            telemetry_cb({"accepted": accepted, "reason": "cold_start", "tokens": len(tokens), "graph_size": graph_size})
        return accepted

    overlap = tokens & graph_terms
    has_memory_match = len(overlap) > 0
    has_common = len(tokens & common_terms) > 0
    # Require either a memory overlap or a domain-common term for acceptance
    accepted = has_memory_match or has_common
    if telemetry_cb:
        telemetry_cb({
            "accepted": accepted,
            "reason": "ok" if accepted else "no_graph_terms",
            "graph_overlap": len(overlap),
            "tokens": len(tokens),
            "graph_size": graph_size
        })
    return accepted

class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray): return obj.tolist()
        if isinstance(obj, np.integer): return int(obj)
        if isinstance(obj, np.floating): return float(obj)
        return super(NumpyEncoder, self).default(obj)

def update_node_potential(node, rating: float, global_modulator: float = 1.0):
    """Graded reinforcement toward 1.0, mild decay otherwise."""
    if rating is None:
        return
    r = float(rating)
    if r >= POTENTIAL_SUCCESS_THRESH:
        # scale 0..1 as rating moves from threshold to 1.0
        gain = max(0.0, (r - POTENTIAL_SUCCESS_THRESH) / (1.0 - POTENTIAL_SUCCESS_THRESH))
        node["connectivity_potential"] = min(1.0, node.get("connectivity_potential", 0.5) + POTENTIAL_REWARD * gain * float(global_modulator))
    else:
        node["connectivity_potential"] = max(0.0, node.get("connectivity_potential", 0.5) * (1.0 - POTENTIAL_DECAY))

@dataclass
class EmergenceSeed:
    remnant_id: str
    embedding_vector: np.ndarray
    projected_vector: np.ndarray
    mass: float
    absorbed_ids: List[str]
    step_created: int

class UniversalEmbeddingAdapter:
    def __init__(self, in_dim, out_dim):
        self.in_dim, self.out_dim = in_dim, out_dim
        if in_dim == out_dim:
            self.W = np.eye(in_dim, dtype=np.float32)
        else:
            rng = np.random.default_rng(GLOBAL_SEED)
            self.W = rng.standard_normal((in_dim, out_dim)).astype(np.float32)
            self.W /= np.linalg.norm(self.W, axis=0, keepdims=True)

    def __call__(self, vector: np.ndarray) -> np.ndarray:
        if vector.shape[0] != self.in_dim:
            padded_vec = np.zeros(self.in_dim, dtype=np.float32)
            size_to_copy = min(vector.shape[0], self.in_dim)
            padded_vec[:size_to_copy] = vector[:size_to_copy]
            vector = padded_vec
        return vector @ self.W

class EmbeddingBootstrap:
    """Deterministic math-only embeddings using VSA operations and E8 lattice structure."""

    def __init__(self, mind_instance: 'E8Mind', embedding_dim: int = 64):
        self.mind = mind_instance
        self.embedding_dim = embedding_dim
        self.console = mind_instance.console

        # Initialize VSA basis vectors using E8 roots
        self._initialize_vsa_basis()

        # Cache for computed embeddings
        self.embedding_cache = {}

        # Statistical tracking
        self.embedding_stats = {
            'total_computed': 0,
            'cache_hits': 0,
            'average_similarity': 0.0
        }

    def _initialize_vsa_basis(self):
        """Initialize VSA basis using E8 root system for deterministic embeddings."""
        try:
            # Use E8 roots as basis vectors for VSA
            if hasattr(self.mind.physics, 'roots_unit') and self.mind.physics.roots_unit is not None:
                e8_roots = self.mind.physics.roots_unit

                # Select subset of roots for basis
                n_basis = min(len(e8_roots), max(8, self.embedding_dim))
                base_roots = np.asarray(e8_roots[:n_basis]).copy()

                # If root vectors have smaller dimension (e.g., 8) project them to embedding_dim
                try:
                    if base_roots.ndim == 2 and base_roots.shape[1] != self.embedding_dim:
                        rng = np.random.default_rng(GLOBAL_SEED)
                        proj = rng.standard_normal((base_roots.shape[1], self.embedding_dim))
                        self.vsa_basis = (base_roots @ proj)
                    else:
                        self.vsa_basis = base_roots

                    # Normalize each basis vector
                    for i in range(len(self.vsa_basis)):
                        norm = float(np.linalg.norm(self.vsa_basis[i]))
                        if norm > 1e-9:
                            self.vsa_basis[i] = self.vsa_basis[i] / norm

                    # Ensure we have at least embedding_dim rows (tile or pad if necessary)
                    if self.vsa_basis.shape[0] < self.embedding_dim:
                        # Tile rows to reach desired count
                        times = int(np.ceil(self.embedding_dim / float(self.vsa_basis.shape[0])))
                        self.vsa_basis = np.tile(self.vsa_basis, (times, 1))[:self.embedding_dim]

                    self.console.log(f"[EmbeddingBootstrap] Initialized VSA basis from E8 roots (projected to dim={self.embedding_dim})")
                except Exception as e:
                    self.console.log(f"[EmbeddingBootstrap] Failed projecting E8 roots to embedding dim: {e}; falling back to random basis")
                    raise
            else:
                # Fallback: random orthonormal basis
                self.console.log("[yellow][EmbeddingBootstrap] E8 roots unavailable, using random basis[/yellow]")
                rng = np.random.default_rng(GLOBAL_SEED)
                self.vsa_basis = rng.standard_normal((self.embedding_dim, self.embedding_dim))
                # Orthogonalize using QR decomposition
                Q, _ = np.linalg.qr(self.vsa_basis.T)
                self.vsa_basis = Q.T

        except Exception as e:
            self.console.log(f"[yellow][EmbeddingBootstrap] Basis initialization failed: {e}[/yellow]")
            # Final fallback: identity matrix
            self.vsa_basis = np.eye(self.embedding_dim)

    def encode_text(self, text: str) -> np.ndarray:
        """Encode text into deterministic embedding using VSA operations."""
        try:
            # Check cache first
            cache_key = hash(text) % 10000  # Simple hash for caching
            if cache_key in self.embedding_cache:
                self.embedding_stats['cache_hits'] += 1
                return self.embedding_cache[cache_key]

            # Tokenize text into words/concepts
            words = self._tokenize_text(text)

            if not words:
                return np.zeros(self.embedding_dim)

            # Encode each word using VSA bundling
            word_vectors = []
            for word in words:
                word_vec = self._encode_word(word)
                word_vectors.append(word_vec)

            # Bundle all word vectors using VSA addition
            if len(word_vectors) == 1:
                embedding = word_vectors[0]
            else:
                embedding = np.sum(word_vectors, axis=0)
                # Normalize the bundled vector
                norm = np.linalg.norm(embedding)
                if norm > 1e-6:
                    embedding /= norm

            # Cache the result
            self.embedding_cache[cache_key] = embedding
            self.embedding_stats['total_computed'] += 1

            # Log metrics for successful encoding
            try:
                metrics_log("bootstrap.encode.success", {
                    "event": "bootstrap_encode",
                    "text_length": len(text),
                    "word_count": len(words),
                    "embedding_norm": float(norm),
                    "cache_size": len(self.embedding_cache),
                    "total_computed": self.embedding_stats['total_computed']
                })
            except Exception:
                pass  # Metrics logging should never fail

            return embedding

        except Exception as e:
            # Log encoding failure
            try:
                metrics_log("bootstrap.encode.error", {
                    "event": "bootstrap_encode_error",
                    "error": str(e)[:100],
                    "text_length": len(text) if text else 0
                })
            except Exception:
                pass
            
            self.console.log(f"[yellow][EmbeddingBootstrap] Text encoding failed: {e}[/yellow]")
            return np.zeros(self.embedding_dim)

    def _tokenize_text(self, text: str) -> List[str]:
        """Simple tokenization for text processing."""
        import re
        # Remove punctuation and split into words
        clean_text = re.sub(r'[^\w\s]', '', text.lower())
        words = clean_text.split()
        return [word for word in words if len(word) > 1]  # Filter out single characters

    def _encode_word(self, word: str) -> np.ndarray:
        """Encode individual word using VSA basis and hashing."""
        try:
            # Use word hash to deterministically select basis vectors
            word_hash = hash(word) % 1000000
            rng = np.random.default_rng(word_hash)  # Deterministic RNG from word

            # Select random subset of basis vectors
            n_active = min(8, len(self.vsa_basis))  # Use up to 8 basis vectors per word
            indices = rng.choice(len(self.vsa_basis), size=n_active, replace=False)

            # Combine selected basis vectors
            word_embedding = np.zeros(self.embedding_dim)
            for idx in indices:
                # Add basis vector with random sign for binding variety
                sign = 1 if rng.random() > 0.5 else -1
                word_embedding += sign * self.vsa_basis[idx]

            # Normalize
            norm = np.linalg.norm(word_embedding)
            if norm > 1e-6:
                word_embedding /= norm

            return word_embedding

        except Exception as e:
            self.console.log(f"[yellow][EmbeddingBootstrap] Word encoding failed: {e}[/yellow]")
            return np.zeros(self.embedding_dim)

    def bind_vectors(self, vec_a: np.ndarray, vec_b: np.ndarray) -> np.ndarray:
        """VSA binding operation (circular convolution or element-wise multiplication)."""
        try:
            # Ensure vectors are same size
            min_dim = min(len(vec_a), len(vec_b), self.embedding_dim)
            a = vec_a[:min_dim]
            b = vec_b[:min_dim]

            # Use element-wise multiplication for binding (simpler than circular convolution)
            bound = a * b

            # Normalize result
            norm = np.linalg.norm(bound)
            if norm > 1e-6:
                bound /= norm

            return bound

        except Exception as e:
            self.console.log(f"[yellow][EmbeddingBootstrap] Vector binding failed: {e}[/yellow]")
            return np.zeros(self.embedding_dim)

    def unbind_vectors(self, bound_vec: np.ndarray, vec_a: np.ndarray) -> np.ndarray:
        """VSA unbinding operation (inverse of binding)."""
        try:
            # For element-wise multiplication, unbinding is division
            min_dim = min(len(bound_vec), len(vec_a), self.embedding_dim)
            bound = bound_vec[:min_dim]
            a = vec_a[:min_dim]

            # Avoid division by zero
            a_safe = np.where(np.abs(a) < 1e-6, 1e-6, a)
            unbound = bound / a_safe

            # Normalize result
            norm = np.linalg.norm(unbound)
            if norm > 1e-6:
                unbound /= norm

            return unbound

        except Exception as e:
            self.console.log(f"[yellow][EmbeddingBootstrap] Vector unbinding failed: {e}[/yellow]")
            return np.zeros(self.embedding_dim)

    def bundle_vectors(self, vectors: List[np.ndarray]) -> np.ndarray:
        """VSA bundling operation (vector addition with normalization)."""
        try:
            if not vectors:
                return np.zeros(self.embedding_dim)

            # Sum all vectors
            bundled = np.sum(vectors, axis=0)

            # Normalize result
            norm = np.linalg.norm(bundled)
            if norm > 1e-6:
                bundled /= norm

            return bundled

        except Exception as e:
            self.console.log(f"[yellow][EmbeddingBootstrap] Vector bundling failed: {e}[/yellow]")
            return np.zeros(self.embedding_dim)

    def compute_similarity(self, vec_a: np.ndarray, vec_b: np.ndarray) -> float:
        """Compute cosine similarity between two vectors."""
        try:
            min_dim = min(len(vec_a), len(vec_b))
            a = vec_a[:min_dim]
            b = vec_b[:min_dim]

            norm_a = np.linalg.norm(a)
            norm_b = np.linalg.norm(b)

            if norm_a < 1e-6 or norm_b < 1e-6:
                return 0.0

            similarity = np.dot(a, b) / (norm_a * norm_b)
            return float(similarity)

        except Exception as e:
            self.console.log(f"[yellow][EmbeddingBootstrap] Similarity computation failed: {e}[/yellow]")
            return 0.0

    def cleanup_vector(self, noisy_vec: np.ndarray, threshold: float = 0.1) -> np.ndarray:
        """Clean up noisy vector by projecting onto VSA basis."""
        try:
            # Project onto VSA basis
            projections = []
            for basis_vec in self.vsa_basis:
                proj = np.dot(noisy_vec, basis_vec)
                if abs(proj) > threshold:
                    projections.append(proj * basis_vec)

            if projections:
                cleaned = np.sum(projections, axis=0)
                norm = np.linalg.norm(cleaned)
                if norm > 1e-6:
                    cleaned /= norm
                return cleaned
            else:
                return np.zeros(self.embedding_dim)

        except Exception as e:
            self.console.log(f"[yellow][EmbeddingBootstrap] Vector cleanup failed: {e}[/yellow]")
            return np.zeros(self.embedding_dim)

    def assign_shells(self, embedding: np.ndarray, text: str = "") -> List[int]:
        """Determine which dimensional shells to assign this embedding to based on its properties."""
        try:
            assigned_shells = []
            
            # Analyze embedding properties
            embedding_norm = np.linalg.norm(embedding)
            embedding_entropy = self._calculate_entropy(embedding)
            embedding_sparsity = self._calculate_sparsity(embedding)
            
            # Text-based analysis if available
            text_complexity = 0.0
            if text:
                text_complexity = self._analyze_text_complexity(text)
            
            # Shell assignment logic based on embedding characteristics
            # Inner shells (8, 16) for simple, focused concepts
            # Outer shells (32, 64) for complex, abstract concepts
            
            # Low entropy + low sparsity ? simple concepts ? inner shells
            if embedding_entropy < 0.3 and embedding_sparsity < 0.7:
                assigned_shells.extend([8, 16])
            
            # High entropy + high sparsity ? complex concepts ? outer shells  
            elif embedding_entropy > 0.7 and embedding_sparsity > 0.8:
                assigned_shells.extend([32, 64])
            
            # Medium complexity ? middle shells
            else:
                assigned_shells.extend([16, 32])
            
            # Text complexity bonus for outer shells
            if text_complexity > 0.6:
                if 32 not in assigned_shells:
                    assigned_shells.append(32)
                if 64 not in assigned_shells:
                    assigned_shells.append(64)
            
            # Ensure at least one shell is assigned
            if not assigned_shells:
                assigned_shells = [16]  # Default to 16D shell
            
            # Sort for consistency
            assigned_shells.sort()
            
            self.console.log(f"[BOOTSTRAP] Assigned embedding to shells {assigned_shells} (norm={embedding_norm:.3f}, entropy={embedding_entropy:.3f}, sparsity={embedding_sparsity:.3f})")
            
            # Log shell assignment metrics
            try:
                metrics_log("bootstrap.shell_assignment", {
                    "event": "bootstrap_shell_assignment",
                    "shells_assigned": assigned_shells,
                    "embedding_norm": float(embedding_norm),
                    "embedding_entropy": float(embedding_entropy),
                    "embedding_sparsity": float(embedding_sparsity),
                    "text_complexity": float(text_complexity) if text else 0.0
                })
            except Exception:
                pass
            
            return assigned_shells
            
        except Exception as e:
            self.console.log(f"[BOOTSTRAP] Shell assignment failed: {e}")
            return [16]  # Safe default

    def _calculate_entropy(self, vector: np.ndarray) -> float:
        """Calculate entropy of the embedding vector."""
        try:
            # Normalize to probability distribution
            abs_vec = np.abs(vector)
            if np.sum(abs_vec) > 1e-6:
                probs = abs_vec / np.sum(abs_vec)
                # Add small epsilon to avoid log(0)
                probs = np.clip(probs, 1e-10, 1.0)
                entropy = -np.sum(probs * np.log2(probs))
                # Normalize by maximum possible entropy
                max_entropy = np.log2(len(vector))
                return entropy / max_entropy if max_entropy > 0 else 0.0
            return 0.0
        except Exception:
            return 0.0

    def _calculate_sparsity(self, vector: np.ndarray, threshold: float = 0.1) -> float:
        """Calculate sparsity (fraction of near-zero elements)."""
        try:
            abs_vec = np.abs(vector)
            max_val = np.max(abs_vec)
            if max_val > 1e-6:
                threshold_val = threshold * max_val
                sparse_count = np.sum(abs_vec < threshold_val)
                return sparse_count / len(vector)
            return 1.0  # All zeros = completely sparse
        except Exception:
            return 1.0

    def _analyze_text_complexity(self, text: str) -> float:
        """Analyze text complexity for shell assignment."""
        try:
            words = text.split()
            if not words:
                return 0.0
            
            # Factors contributing to complexity
            avg_word_length = np.mean([len(word) for word in words])
            unique_words = len(set(word.lower() for word in words))
            uniqueness_ratio = unique_words / len(words)
            
            # Technical/conceptual indicators
            technical_terms = sum(1 for word in words if len(word) > 6)
            technical_ratio = technical_terms / len(words)
            
            # Combine factors (weighted average)
            complexity = (
                0.3 * min(avg_word_length / 8.0, 1.0) +      # Word length
                0.3 * uniqueness_ratio +                      # Lexical diversity
                0.4 * technical_ratio                         # Technical content
            )
            
            return float(np.clip(complexity, 0.0, 1.0))
            
        except Exception:
            return 0.0

    def deduplicate_embeddings(self, embeddings: Dict[str, np.ndarray], threshold: float = 0.95) -> Dict[str, np.ndarray]:
        """Remove duplicate embeddings based on similarity threshold."""
        try:
            if not embeddings:
                return embeddings
            
            kept_embeddings = {}
            processed_ids = set()
            
            for id1, vec1 in embeddings.items():
                if id1 in processed_ids:
                    continue
                
                kept_embeddings[id1] = vec1
                processed_ids.add(id1)
                
                # Check similarity with remaining embeddings
                for id2, vec2 in embeddings.items():
                    if id2 in processed_ids or id1 == id2:
                        continue
                    
                    similarity = self.compute_similarity(vec1, vec2)
                    if similarity >= threshold:
                        self.console.log(f"[BOOTSTRAP] Deduplicating similar embeddings: {id1} � {id2} (sim={similarity:.3f})")
                        processed_ids.add(id2)
            
            self.console.log(f"[BOOTSTRAP] Deduplication: {len(embeddings)} ? {len(kept_embeddings)} embeddings")
            
            # Log deduplication metrics
            try:
                metrics_log("bootstrap.deduplication", {
                    "event": "bootstrap_deduplication",
                    "original_count": len(embeddings),
                    "deduplicated_count": len(kept_embeddings),
                    "removed_count": len(embeddings) - len(kept_embeddings),
                    "threshold": threshold
                })
            except Exception:
                pass
            
            return kept_embeddings
            
        except Exception as e:
            self.console.log(f"[BOOTSTRAP] Deduplication failed: {e}")
            return embeddings

    def generate_knn_edges(self, embeddings: Dict[str, np.ndarray], k: int = 5) -> List[Tuple[str, str, float]]:
        """Generate k-NN edges between embeddings for graph connectivity."""
        try:
            if len(embeddings) < 2:
                return []
            
            edges = []
            ids = list(embeddings.keys())
            
            # Compute all pairwise similarities
            for i, id1 in enumerate(ids):
                similarities = []
                for j, id2 in enumerate(ids):
                    if i != j:
                        sim = self.compute_similarity(embeddings[id1], embeddings[id2])
                        similarities.append((id2, sim))
                
                # Keep top-k most similar
                similarities.sort(key=lambda x: x[1], reverse=True)
                for neighbor_id, sim in similarities[:k]:
                    if sim > 0.1:  # Only create edges for meaningful similarities
                        edges.append((id1, neighbor_id, sim))
            
            self.console.log(f"[BOOTSTRAP] Generated {len(edges)} k-NN edges (k={k})")
            
            # Log k-NN edge generation metrics
            try:
                metrics_log("bootstrap.knn_edges", {
                    "event": "bootstrap_knn_edges",
                    "edges_generated": len(edges),
                    "k_parameter": k,
                    "nodes_processed": len(embeddings),
                    "avg_similarity": float(np.mean([sim for _, _, sim in edges])) if edges else 0.0
                })
            except Exception:
                pass
            
            return edges
            
        except Exception as e:
            self.console.log(f"[BOOTSTRAP] k-NN edge generation failed: {e}")
            return []

    def apply_graph_hygiene(self, embeddings: Dict[str, np.ndarray], 
                           dedup_threshold: float = 0.95, knn_k: int = 5) -> Tuple[Dict[str, np.ndarray], List[Tuple[str, str, float]]]:
        """Apply complete graph hygiene pipeline: deduplication + k-NN edges."""
        try:
            # Step 1: Deduplicate embeddings
            cleaned_embeddings = self.deduplicate_embeddings(embeddings, dedup_threshold)
            
            # Step 2: Generate k-NN edges
            edges = self.generate_knn_edges(cleaned_embeddings, knn_k)
            
            self.console.log(f"[BOOTSTRAP] Graph hygiene complete: {len(cleaned_embeddings)} nodes, {len(edges)} edges")
            
            # Log complete graph hygiene metrics
            try:
                metrics_log("bootstrap.graph_hygiene", {
                    "event": "bootstrap_graph_hygiene",
                    "original_embeddings": len(embeddings),
                    "final_embeddings": len(cleaned_embeddings),
                    "edges_generated": len(edges),
                    "dedup_threshold": dedup_threshold,
                    "knn_k": knn_k,
                    "dedup_ratio": len(cleaned_embeddings) / len(embeddings) if embeddings else 0.0,
                    "edges_per_node": len(edges) / len(cleaned_embeddings) if cleaned_embeddings else 0.0
                })
            except Exception:
                pass
            
            return cleaned_embeddings, edges
            
        except Exception as e:
            self.console.log(f"[BOOTSTRAP] Graph hygiene failed: {e}")
            return embeddings, []

    def get_stats(self) -> Dict[str, Any]:
        """Get embedding statistics."""
        stats = self.embedding_stats.copy()
        if stats['total_computed'] > 0:
            stats['cache_hit_rate'] = stats['cache_hits'] / stats['total_computed']
        else:
            stats['cache_hit_rate'] = 0.0
        return stats

# Ensure stdout/stderr use UTF-8 to avoid mojibake when printing emoji/Unicode on Windows
try:
    import sys
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(encoding="utf-8")
    if hasattr(sys.stderr, "reconfigure"):
        sys.stderr.reconfigure(encoding="utf-8")
except Exception:
    # best-effort; continue if environment doesn't allow reconfigure
    pass

try:
    # Force UTF-8-aware Console and enable terminal rendering.
    console = Console(record=True, force_terminal=True, color_system="auto")
except Exception:
    # Fallback: basic Console if environment doesn't support advanced parameters
    console = Console(record=True)

# Lightweight sanitizer to recover common "mojibake" caused by double-encoding
import unicodedata

def _fix_mojibake(text: str) -> str:
    """
    Attempt a stronger, iterative repair for mojibake.

    This function tries multiple heuristics and multi-pass rescues to
    recover UTF-8 that was mis-decoded as latin1/cp1252 (the common
    cause of sequences like 'ðŸ', 'Ã', 'Â', 'â', and 'âš ï¸'). It builds
    several candidate repairs (including iterative latin1->utf-8 and
    cp1252->utf-8 passes), scores them by the amount of high-codepoint
    Unicode they expose (while penalizing replacement characters), and
    returns the best-scoring normalized result. It also strips control
    characters and U+FFFD replacement glyphs and falls back to
    transliteration when appropriate.
    """
    if not isinstance(text, str):
        return str(text)

    try:
        # Quick normalization pass: if the text already looks fine, return it.
        norm = unicodedata.normalize('NFKC', text)
        # If there are a reasonable number of non-ascii codepoints, assume it's ok
        if any(ord(c) > 127 for c in norm) and '\ufffd' not in norm:
            return norm
    except Exception:
        pass

    candidates = [text]

    # Helper: try a single rescue using an 8-bit encoding -> utf-8 decode
    def try_rescue(src: str, from_enc: str, to_enc: str = 'utf-8') -> Optional[str]:
        try:
            return src.encode(from_enc, errors='replace').decode(to_enc, errors='replace')
        except Exception:
            return None

    # Add single-pass rescues (latin1 and cp1252)
    for enc in ('latin1', 'cp1252'):
        r = try_rescue(text, enc)
        if r is not None:
            candidates.append(r)

    # Add iterative multi-pass rescues: sometimes text has been double- or triple-decoded
    for enc in ('latin1', 'cp1252'):
        cur = text
        for _ in range(3):
            nxt = try_rescue(cur, enc)
            if not nxt or nxt == cur:
                break
            candidates.append(nxt)
            cur = nxt

    # Also try utf-8 -> latin1 round-trips (the reverse direction)
    cur = text
    for _ in range(3):
        try:
            nxt = cur.encode('utf-8', errors='replace').decode('latin1', errors='replace')
        except Exception:
            break
        if not nxt or nxt == cur:
            break
        candidates.append(nxt)
        cur = nxt

    # Scoring: prefer candidates with more valid non-ASCII codepoints, but
    # penalize presence of U+FFFD replacement chars and control characters.
    def score_candidate(s: str) -> int:
        if not isinstance(s, str):
            return -999
        # count useful unicode points
        useful = sum(1 for ch in s if ord(ch) > 127 and ch != '\ufffd')
        # penalty for replacement glyphs
        repl = s.count('\ufffd')
        # penalty for control characters
        ctrl = len(re.findall(r'[\x00-\x1f\x7f]', s))
        return useful - (repl * 5) - (ctrl * 2)

    best = text
    best_score = score_candidate(best)
    for cand in candidates:
        try:
            cand_norm = unicodedata.normalize('NFKC', cand)
            sc = score_candidate(cand_norm)
            if sc > best_score:
                best, best_score = cand_norm, sc
        except Exception:
            continue

    # Clean up: remove replacement char and stray controls
    try:
        cleaned = best.replace('\ufffd', '')
        cleaned = re.sub(r'[\x00-\x1f\x7f]', '', cleaned)
        cleaned = unicodedata.normalize('NFKC', cleaned)
    except Exception:
        cleaned = best

    # If result still looks ASCII-only but original contained mojibake markers,
    # try transliteration as a last-ditch effort.
    if not any(ord(c) > 127 for c in cleaned) and re.search(r'(Ã|Â|ðŸ|â|�|âš|ï¸)', text):
        try:
            import unidecode
            alt = unidecode.unidecode(cleaned)
            if alt:
                cleaned = alt
        except Exception:
            pass

    return cleaned


class SafeConsole:
    """Wraps a Rich Console and sanitizes string arguments before printing/logging.

    This keeps existing code that calls console.log(...) working while
    reducing visible mojibake in Windows terminals.
    """
    def __init__(self, console):
        self._console = console

    def _sanitize_arg(self, a):
        if isinstance(a, str):
            return _fix_mojibake(a)
        return a

    def log(self, *args, **kwargs):
        safe_args = tuple(self._sanitize_arg(a) for a in args)
        return self._console.log(*safe_args, **kwargs)

    def print(self, *args, **kwargs):
        safe_args = tuple(self._sanitize_arg(a) for a in args)
        return self._console.print(*safe_args, **kwargs)

    def __getattr__(self, name):
        # Delegate any other attribute to the underlying console
        return getattr(self._console, name)

    # Provide context manager support so `with SafeConsole(...)` works like the real Console
    def __enter__(self):
        enter = getattr(self._console, "__enter__", None)
        if callable(enter):
            return enter()
        return self

    def __exit__(self, exc_type, exc, tb):
        exit_fn = getattr(self._console, "__exit__", None)
        if callable(exit_fn):
            return exit_fn(exc_type, exc, tb)
        return False

# Replace the module-level console with the safe wrapper so all existing
# assignments like `self.console = console` keep the sanitization behavior.
console = SafeConsole(console)

# Wrapper that dims any output from .log() while leaving .print() and others untouched.
class DimmedLoggerConsole:
    """A wrapper that dims any output from .log() while leaving .print() untouched."""
    def __init__(self, console_instance):
        self._original_console = console_instance

    def log(self, *args, **kwargs):
        """Intercept log messages and either hide them completely or dim them based on E8_HIDE_DIMMED_LOGS flag."""
        if E8_HIDE_DIMMED_LOGS:
            # Completely hide dimmed log messages - don't output anything
            return
        
        try:
            dimmed_args = tuple((f"[dim]{arg}[/dim]" if isinstance(arg, str) else arg) for arg in args)
        except Exception:
            # If anything goes wrong, fall back to original args
            dimmed_args = args
        return self._original_console.log(*dimmed_args, **kwargs)

    def __getattr__(self, name):
        # Delegate any other attribute to the underlying console (print, rule, etc.)
        return getattr(self._original_console, name)

    def __enter__(self):
        enter = getattr(self._original_console, "__enter__", None)
        if callable(enter):
            return enter()
        return self

    def __exit__(self, exc_type, exc, tb):
        exit_fn = getattr(self._original_console, "__exit__", None)
        if callable(exit_fn):
            return exit_fn(exc_type, exc, tb)
        return False

# Apply the dimming wrapper globally so all console.log() calls are dim by default.
console = DimmedLoggerConsole(console)

class JourneyLogger:
    """
    Unified JSON logging that ties together insight_id, teacher_id, score, 
    proximity_distance, alert_threshold, bandit_bonus for end-to-end tracking.
    """
    def __init__(self, log_file: str = None, buffer_size: int = 100):
        self.log_file = log_file or E8_JOURNEY_LOGGER_FILE
        self.buffer_size = buffer_size
        self.buffer = []
        self.enabled = E8_JOURNEY_LOGGER_ENABLE
        
        if self.enabled:
            try:
                # Ensure directory exists
                os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
            except Exception:
                pass
    
    def log_journey_event(self, event_type: str, **kwargs):
        """Log a journey event with unified fields."""
        if not self.enabled:
            return
        
        # Create unified event record
        event = {
            "timestamp": time.time(),
            "event_type": event_type,
            **kwargs
        }
        
        # Add to buffer
        self.buffer.append(event)
        
        # Flush if buffer is full
        if len(self.buffer) >= self.buffer_size:
            self.flush()
    
    def flush(self):
        """Flush buffered events to file."""
        if not self.enabled or not self.buffer:
            return
        
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                for event in self.buffer:
                    f.write(json.dumps(event) + '\n')
            self.buffer.clear()
        except Exception as e:
            try:
                console.log(f"[JourneyLogger] Failed to flush: {e}")
            except Exception:
                pass
    
    def log_insight_creation(self, insight_id: str, teacher_id: str = None, 
                           score: float = None, proximity_distance: float = None,
                           alert_threshold: float = None, bandit_bonus: float = None,
                           step: int = None, **extra):
        """Log insight creation with all relevant context."""
        self.log_journey_event(
            "insight_creation",
            insight_id=insight_id,
            teacher_id=teacher_id,
            score=score,
            proximity_distance=proximity_distance,
            alert_threshold=alert_threshold,
            bandit_bonus=bandit_bonus,
            step=step,
            **extra
        )
    
    def log_memory_operation(self, operation: str, node_id: str = None,
                           teacher_id: str = None, score: float = None,
                           step: int = None, **extra):
        """Log memory operations (add, update, consolidate)."""
        self.log_journey_event(
            "memory_operation",
            operation=operation,
            node_id=node_id,
            teacher_id=teacher_id,
            score=score,
            step=step,
            **extra
        )
    
    def log_proximity_event(self, event_subtype: str, proximity_distance: float = None,
                          alert_threshold: float = None, node_id: str = None,
                          step: int = None, **extra):
        """Log proximity system events."""
        self.log_journey_event(
            "proximity_event",
            event_subtype=event_subtype,
            proximity_distance=proximity_distance,
            alert_threshold=alert_threshold,
            node_id=node_id,
            step=step,
            **extra
        )
    
    def log_bandit_decision(self, arm_index: int = None, bandit_bonus: float = None,
                          context_summary: str = None, step: int = None, **extra):
        """Log bandit arm selection decisions."""
        self.log_journey_event(
            "bandit_decision",
            arm_index=arm_index,
            bandit_bonus=bandit_bonus,
            context_summary=context_summary,
            step=step,
            **extra
        )

    def periodic_flush(self, step: int, flush_interval: int = 500):
        """Flush buffered events periodically based on step count."""
        if step % flush_interval == 0:
            self.flush()

LAST_INTRINSIC = {}

# Feature flag to enable/disable rich Ray Alert UI globally
E8_UI_RAY_ALERTS = os.getenv("E8_UI_RAY_ALERTS", "1") != "0"

# Compact helpers for Ray Alert building/printing (graceful fallback when Rich unavailable)
def _short(s: Optional[str], n: int = 80) -> str:
    try:
        if not s:
            return ""
        s = str(s).strip()
        return s if len(s) <= n else s[: max(0, n - 1)] + "…"
    except Exception:
        return str(s)

def _fill(v, default=None):
    return v if v is not None else default

def build_alert_entry(
    *,
    source_label: str,
    target_label: str,
    source_dim: int,
    target_dim: int,
    distance: Optional[float] = None,
    tier: Optional[str] = None,
    score: Optional[float] = None,
    hypothesis: Optional[str] = None,
    bulk_src: Optional[Iterable[float]] = None,
    bulk_tgt: Optional[Iterable[float]] = None,
    extra: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    return {
        "source_label": _short(source_label, 96),
        "target_label": _short(target_label, 96),
        "source_dim": int(_fill(source_dim, 0) or 0),
        "target_dim": int(_fill(target_dim, 0) or 0),
        "distance": float(distance) if isinstance(distance, (int, float)) else None,
        "tier": (tier or "").lower() if isinstance(tier, str) else tier,
        "score": float(score) if isinstance(score, (int, float)) else None,
        "hypothesis": _short(hypothesis or "", 160),
        "bulk_src": list(bulk_src) if bulk_src is not None else None,
        "bulk_tgt": list(bulk_tgt) if bulk_tgt is not None else None,
        "extra": dict(extra) if isinstance(extra, dict) else {},
    }

def render_ray_alert(entry: Dict[str, Any], *, console: Optional[Any] = None) -> None:
    """Render a Ray Alert panel or a plain fallback line if Rich is unavailable/disabled.

    Expected entry keys: source_label,target_label,source_dim,target_dim,distance,tier,score,hypothesis,
    bulk_src,bulk_tgt,extra.
    """
    try:
        if not E8_UI_RAY_ALERTS:
            return

        c = console or globals().get("console")
        if not c:
            print("[RayAlert]", entry)
            return

        title = "[bold red]RAY ALERT[/]" if not entry.get("tier") else f"[bold red]RAY {str(entry['tier']).upper()}[/]"

        # Main lines
        lines = []
        lines.append(f"[bold white]Source:[/] [cyan]{entry.get('source_label','?')}[/] ({entry.get('source_dim','?')}D)")
        lines.append(f"[bold white]Target:[/] [green]{entry.get('target_label','?')}[/] ({entry.get('target_dim','?')}D)")
        if entry.get("distance") is not None:
            dist = float(entry["distance"])  # type: ignore
            lines.append(f"[bold white]Distance:[/] [yellow]{dist:.4f}[/]")
        # Projection and distance breakdown (if bulk vectors included)
        try:
            src_d = entry.get('source_dim')
            if src_d is not None:
                lines.append(f"[dim]Projection:[/] {int(src_d)}D → E8")
        except Exception:
            pass
        try:
            bs = entry.get('bulk_src')
            bt = entry.get('bulk_tgt')
            geo_len = None
            try:
                xtra = entry.get('extra') or {}
                if isinstance(xtra, dict) and xtra.get('geo_len') is not None:
                    geo_len = float(xtra.get('geo_len'))
            except Exception:
                geo_len = None
            if isinstance(bs, list) and isinstance(bt, list) and len(bs) == len(bt) and len(bs) > 0:
                import math as _math
                bulk_norm = _math.sqrt(sum((float(bt[i]) - float(bs[i]))**2 for i in range(len(bs))))
                if geo_len is not None:
                    lines.append(f"[dim]Distance:[/] ℓ_geo≈{geo_len:.3f} vs ‖Δx‖_bulk={bulk_norm:.4f}")
                else:
                    lines.append(f"[dim]Distance:[/] ℓ_geo≈? vs ‖Δx‖_bulk={bulk_norm:.4f}")
        except Exception:
            pass
        if entry.get("tier") or entry.get("score") is not None:
            tier = entry.get("tier")
            score = entry.get("score")
            try:
                if score is not None:
                    lines.append(f"[bold white]Tier:[/] {str(tier).upper() if tier else '?'}  [bold white]Score:[/] {float(score):.3f}")
                else:
                    lines.append(f"[bold white]Tier:[/] {str(tier).upper() if tier else '?'}")
            except Exception:
                pass
        # Projection and distance breakdown (provenance vs physics cues)
        try:
            src_d = entry.get("source_dim")
            if isinstance(src_d, (int, float)):
                lines.append(f"[dim]Projection:[/] {int(src_d)}D → E8")
        except Exception:
            pass
        try:
            bs = entry.get("bulk_src")
            bt = entry.get("bulk_tgt")
            if isinstance(bs, list) and isinstance(bt, list) and len(bs) == len(bt) and len(bs) > 0:
                try:
                    import numpy as _np
                    delta = _np.asarray(bt, dtype=float) - _np.asarray(bs, dtype=float)
                    bulk_norm = float(_np.linalg.norm(delta))
                except Exception:
                    bulk_norm = None
                # ℓ_geo not known here; show bulk if available
                if bulk_norm is not None:
                    lines.append(f"[dim]Distance:[/] ℓ_geo≈? vs ‖Δx‖_bulk={bulk_norm:.4f}")
        except Exception:
            pass

        if entry.get("hypothesis"):
            lines.append(f"[bold white]Coupling:[/] [italic]{entry['hypothesis']}[/]")

        try:
            # Use Rich Panel if available
            c.print(Panel("\n".join(lines), title=title, border_style="red"))
        except Exception:
            # Plain fallback
            try:
                print(f"RAY ALERT | {title} :: " + " | ".join(lines))
            except Exception:
                print("RAY ALERT", entry)
    except Exception as e:
        try:
            (console or globals().get("console")).log(f"[RayAlert][WARN] failed to render: {e}")
        except Exception:
            pass

try:
    from profiles.loader import load_profile as _real_load_profile
except ImportError:
    _real_load_profile = None

if TORCH_AVAILABLE:
    class VariationalAutoencoder(nn.Module):
        def __init__(self, layer_sizes, latent_dim=None, lr=1e-4, beta=1.0, console=None):
            super().__init__()
            self.console = console
            self.layer_sizes = list(layer_sizes)
            self.latent_dim = latent_dim or self.layer_sizes[-1]
            self.beta = beta
            self._trained = False
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

            # KL warmup scheduling
            self.step_count = 0
            self.kl_warmup_steps = E8_VAE_KL_WARMUP_STEPS
            self.kl_target_beta = E8_VAE_KL_TARGET_BETA
            self.free_bits = E8_VAE_FREE_BITS
            self.grad_clip = E8_VAE_GRAD_CLIP

            self.encoder_layers = nn.ModuleList()
            for i in range(len(self.layer_sizes) - 2):
                self.encoder_layers.append(nn.Linear(self.layer_sizes[i], self.layer_sizes[i+1]))
            self.fc_mu = nn.Linear(self.layer_sizes[-2], self.latent_dim)
            self.fc_logvar = nn.Linear(self.layer_sizes[-2], self.latent_dim)

            self.decoder_layers = nn.ModuleList()
            self.decoder_layers.append(nn.Linear(self.latent_dim, self.layer_sizes[-2]))
            for i in range(len(self.layer_sizes) - 2, 0, -1):
                self.decoder_layers.append(nn.Linear(self.layer_sizes[i], self.layer_sizes[i-1]))

            self.to(self.device)
            self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)

            if self.console:
                try:
                    self.console.log(f"🧠 [VAE] Enhanced VAE with KL-warmup (β: 0→{self.kl_target_beta} over {self.kl_warmup_steps} steps), free-bits={self.free_bits}, grad-clip={self.grad_clip}")
                except Exception:
                    pass


        @property
        def is_trained(self) -> bool:
            return bool(self._trained)

        def get_current_beta(self):
            """Get current beta value based on KL warmup schedule."""
            if self.step_count >= self.kl_warmup_steps:
                return self.kl_target_beta
            else:
                # Linear warmup from 0 to target_beta
                warmup_progress = self.step_count / self.kl_warmup_steps
                return warmup_progress * self.kl_target_beta

        def apply_free_bits_constraint(self, kld_per_dim):
            """Apply free-bits constraint to KL divergence per dimension."""
            # Free bits constraint: don't penalize KL below threshold (in nats)
            free_bits_nats = self.free_bits * 0.693147  # Convert bits to nats (ln(2))
            return torch.clamp(kld_per_dim - free_bits_nats, min=0.0)

        def _enc_forward_until(self, x, target_idx_exclusive=None):
            layers = self.encoder_layers if target_idx_exclusive is None else self.encoder_layers[:target_idx_exclusive]
            for layer in layers:
                x = F.relu(layer(x))
            return x

        def encode(self, x):
            h = self._enc_forward_until(x)
            return self.fc_mu(h), self.fc_logvar(h)

        def reparameterize(self, mu, logvar):
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            return mu + eps * std

        def decode(self, z):
            for i, layer in enumerate(self.decoder_layers):
                z = layer(z)
                if i < len(self.decoder_layers) - 1:
                    z = F.relu(z)
            return z

        def forward(self, x):
            mu, logvar = self.encode(x)
            z = self.reparameterize(mu, logvar)
            return self.decode(z), mu, logvar

        def train_on_batch(self, x_batch):
            self.train()
            x_batch = x_batch.to(self.device)
            self.optimizer.zero_grad()
            
            # Forward pass
            recon_batch, mu, logvar = self.forward(x_batch)
            
            # Reconstruction loss
            recon_loss = F.mse_loss(recon_batch, x_batch, reduction='mean') * x_batch.shape[1]
            
            # KL divergence per dimension
            kld_per_dim = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())  # [batch, latent_dim]
            
            # Apply free-bits constraint
            constrained_kld_per_dim = self.apply_free_bits_constraint(kld_per_dim)
            
            # Total KL loss with free-bits constraint
            kld_loss = torch.mean(torch.sum(constrained_kld_per_dim, dim=1)) * x_batch.shape[1]
            
            # Get current beta (with warmup)
            current_beta = self.get_current_beta()
            
            # Total loss
            loss = recon_loss + current_beta * kld_loss
            
            # Backward pass with gradient clipping
            loss.backward()
            if self.grad_clip > 0:
                torch.nn.utils.clip_grad_norm_(self.parameters(), self.grad_clip)
            self.optimizer.step()
            
            # Update step count and training flag
            self.step_count += 1
            if not self._trained:
                self._trained = True

            # Enhanced logging metrics
            metrics = {
                "total_loss": float(loss.item()),
                "recon_loss": float(recon_loss.item()),
                "kld_loss": float(kld_loss.item()),
                "current_beta": float(current_beta),
                "step_count": int(self.step_count),
                "mean_kl_per_dim": float(torch.mean(kld_per_dim).item()),
                "free_bits_active_pct": float((kld_per_dim < (self.free_bits * 0.693147)).float().mean().item() * 100)
            }
            
            # VAE Truthiness Logging: Structured JSON output with per-dimension metrics
            if E8_VAE_ENHANCED_LOGGING and self.console:
                try:
                    # Raw and constrained KL per-dimension (detach from computation graph)
                    kld_raw = torch.mean(kld_per_dim, dim=0).detach().cpu().numpy()  # [latent_dim]
                    kld_freebits = torch.mean(constrained_kld_per_dim, dim=0).detach().cpu().numpy()  # [latent_dim]
                    
                    vae_json_metrics = {
                        "kl_raw": kld_raw.tolist(),
                        "kl_freebits": kld_freebits.tolist(), 
                        "beta": float(current_beta),
                        "step": int(self.step_count),
                        "total_loss": float(loss.item()),
                        "recon_loss": float(recon_loss.item()),
                        "free_bits_floor": float(self.free_bits),
                        "kl_raw_mean": float(kld_raw.mean()),
                        "kl_freebits_mean": float(kld_freebits.mean()),
                        "free_bits_active_dims": int((kld_per_dim.mean(0) < (self.free_bits * 0.693147)).sum().detach().item())
                    }
                    
                    # Structured JSON logging
                    import json
                    self.console.log(f"🧠 [VAE-JSON] {json.dumps(vae_json_metrics, separators=(',', ':'))}")
                    
                    # Human-readable summary
                    self.console.log(
                        f"🧠 [VAE] β={current_beta:.3f}, "
                        f"KL_raw={kld_raw.mean():.3f}, "
                        f"KL_fb={kld_freebits.mean():.3f}, "
                        f"active_dims={vae_json_metrics['free_bits_active_dims']}/{len(kld_raw)}"
                    )
                except Exception as e:
                    self.console.log(f"[VAE] Logging error: {e}")
                    pass

            return metrics

        def project_to_dim(self, x, target_dim: int):
            self.eval()
            x = x.to(self.device)
            if target_dim == self.layer_sizes[-1]:
                with torch.no_grad():
                    mu, _ = self.encode(x)
                    return mu

            try:
                idx = self.layer_sizes.index(target_dim)
            except ValueError:
                return None
            if idx == 0:
                return x
            with torch.no_grad():
                return self._enc_forward_until(x, target_idx_exclusive=idx)

        def project_between_dim(self, x, source_dim: int, target_dim: int):
            self.eval()
            return self.project_to_dim(x, target_dim)
        
        def project(self, x):
            """Project input through encode-decode cycle for denoising."""
            self.eval()
            with torch.no_grad():
                x = x.to(self.device)
                mu, logvar = self.encode(x)
                # Use mean for deterministic projection
                z = mu
                return self.decode(z)

else:
    # NOTE:
    # Avoid declaring a second public class named 'VariationalAutoencoder' in the
    # fallback branch. Static analyzers (e.g., Pylance) flag this as a duplicate
    # declaration even though branches are mutually exclusive at runtime.
    # We define a private stub and alias it. Keep interface parity
    # (is_trained, train_on_batch, project_to_dim, project_between_dim).
    class _VariationalAutoencoderStub:  # lightweight runtime stub
        def __init__(self, layer_sizes=None, console=None):
            self._trained = False
            self.layer_sizes = layer_sizes or []
            self.console = console

        @property
        def is_trained(self) -> bool:
            return bool(self._trained)

        def train_on_batch(self, x):  # type: ignore[unused-argument]
            return {"total_loss": 0.0, "recon_loss": 0.0, "kld_loss": 0.0}

        def _to_np(self, x):
            try:
                if x is None:
                    return None
                if hasattr(x, "detach"):
                    x = x.detach().cpu().numpy()
                return np.asarray(x, dtype=np.float32)
            except Exception:
                return x

        def project_to_dim(self, x, target_dim: int):
            x_np = self._to_np(x)
            if x_np is None:
                return None
            x2 = np.atleast_2d(x_np).astype(np.float32)
            out = np.zeros((x2.shape[0], int(target_dim)), dtype=np.float32)
            n = min(x2.shape[1], int(target_dim))
            out[:, :n] = x2[:, :n]
            return out if x2.ndim == 2 else out[0]

        def project_between_dim(self, x, source_dim: int, target_dim: int):  # type: ignore[unused-argument]
            return self.project_to_dim(x, target_dim)
        
        def project(self, x):
            """Stub implementation of project method."""
            return self._to_np(x)

    VariationalAutoencoder = _VariationalAutoencoderStub  # type: ignore
    try:  # Attempt to make name appear consistent in repr / logs
        _VariationalAutoencoderStub.__name__ = "VariationalAutoencoder"  # type: ignore
    except Exception:  # pragma: no cover - best effort only
        pass

try:
    # Use unified index implementation (preferred)
    from memory.index import KDTree, cosine_distances, cosine_similarity  # type: ignore
except Exception:  # pragma: no cover - fallback only if modular import unavailable
    # Minimal fallback (keeps previous basic behavior) â€“ should rarely trigger.
    def cosine_distances(A, B):  # type: ignore
        if _sk_cosine_distances is not None:
            return _sk_cosine_distances(A, B)
        A = np.asarray(A, dtype=np.float32); B = np.asarray(B, dtype=np.float32)
        def _norm(x): return np.sqrt((x*x).sum(axis=1, keepdims=True)) + 1e-12
        A2, B2 = A / _norm(A), B / _norm(B)
        return 1.0 - (A2 @ B2.T)

    def cosine_similarity(A, B):  # type: ignore
        if _sk_cosine_similarity is not None:
            return _sk_cosine_similarity(A, B)
        A = np.asarray(A, dtype=np.float32); B = np.asarray(B, dtype=np.float32)
        def _norm(x): return np.sqrt((x*x).sum(axis=1, keepdims=True)) + 1e-12
        A2, B2 = A / _norm(A), B / _norm(B)
        return A2 @ B2.T

    class KDTree:  # type: ignore
        """Emergency fallback KDTree (NumPy only)."""
        def __init__(self, data):
            X = np.atleast_2d(np.asarray(data, dtype=np.float32))
            self._data = X
            self.n = X.shape[0]
        def query(self, q, k: int = 1):
            if self.n == 0:
                return np.zeros((0,), dtype=np.float32), np.zeros((0,), dtype=np.int64)
            qv = np.atleast_2d(np.asarray(q, dtype=np.float32))
            ds_list = []
            ix_list = []
            for row in qv:
                dv = np.sqrt(((self._data - row)**2).sum(axis=1))
                kk = min(int(k), self.n)
                part = np.argpartition(dv, kk-1)[:kk] if kk < self.n else np.arange(self.n)
                order = part[np.argsort(dv[part])]
                ds_list.append(dv[order].astype(np.float32))
                ix_list.append(order.astype(np.int64))
            d = ds_list[0] if (len(ds_list)==1 and qv.shape[0]==1) else np.stack(ds_list,0)
            i = ix_list[0] if (len(ix_list)==1 and qv.shape[0]==1) else np.stack(ix_list,0)
            return d, i

E8_MARKET_FEED_ENABLED = os.getenv("E8_MARKET_FEED_ENABLED", "0") == "1"

class Bar:
    def __init__(self, **kwargs):
        for k,v in kwargs.items(): setattr(self, k, v)

class MarketFeed:
    """Connects to a real-time financial data websocket to stream tick data."""
    def __init__(self, symbols: List[str], api_key: str, on_tick, on_bar):
        self.symbols = symbols
        self.on_tick = on_tick
        self.on_bar = on_bar
        self.running = False
        self._task = None
        self._bar_aggregator = {}
        if websockets is None:
            self.ws_url = None
            console.log("[yellow]websockets lib not available; MarketFeed disabled.[/yellow]")
        else:
            self.ws_url = f"wss://ws.finnhub.io?token={api_key}"

    async def start(self):
        if self.running or self.ws_url is None: return
        console.log(f"ðŸ“ˆ [MarketFeed] Starting connection for symbols: {self.symbols}")
        self.running = True
        self._task = asyncio.create_task(self._run())

    async def stop(self):
        self.running = False
        if self._task:
            self._task.cancel()
        console.log("ðŸ“‰ [MarketFeed] Connection stopped.")

    async def _run(self):
        while self.running:
            try:
                if websockets is None or self.ws_url is None: return
                async with websockets.connect(self.ws_url) as websocket:
                    for symbol in self.symbols:
                        await websocket.send(json.dumps({"type": "subscribe", "symbol": symbol}))
                        console.log(f"[MarketFeed] Subscribed to {symbol}")

                    async for message in websocket:
                        data = json.loads(message)
                        if data.get("type") == "trade":
                            for trade in data.get("data", []):
                                self.on_tick(trade["s"], trade)
                                self._aggregate_bar(trade)
                        elif data.get("type") == "ping":
                            await websocket.send(json.dumps({"type": "pong"}))
            except (asyncio.CancelledError, Exception):
                console.log("[MarketFeed] Connection closed or cancelled.")
            except Exception as e:
                console.log(f"[bold red][MarketFeed] Error: {e}. Reconnecting in 15 seconds.[/bold red]")
                await asyncio.sleep(15)

    def _aggregate_bar(self, trade: Dict):
        symbol = trade["s"]
        price = trade["p"]
        volume = trade["v"]
        timestamp_ms = trade["t"]
        current_minute = int(timestamp_ms / 60000)

        if symbol not in self._bar_aggregator:
            self._bar_aggregator[symbol] = {"minute": current_minute, "o": price, "h": price, "l": price, "c": price, "v": 0}

        bar = self._bar_aggregator[symbol]
        if current_minute > bar["minute"]:
            final_bar = Bar(o=bar["o"], h=bar["h"], l=bar["l"], c=bar["c"], v=bar["v"], ts=bar["minute"]*60)
            self.on_bar(symbol, "1m", final_bar)
            bar.update({"minute": current_minute, "o": price, "h": price, "l": price, "c": price, "v": volume})
        else:
            bar["h"] = max(bar["h"], price)
            bar["l"] = min(bar["l"], price)
            bar["c"] = price
            bar["v"] += volume

class OUNoise:
    """Ornstein-Uhlenbeck process for temporally correlated exploration noise."""
    def __init__(self, size: int, theta: float = 0.05, sigma: float = 0.06):
        self.size = int(size)
        self.theta = float(theta)
        self.sigma = float(sigma)
        self.state = np.zeros(self.size, dtype=np.float32)

    def reset(self):
        self.state = np.zeros(self.size, dtype=np.float32)

    def sample(self):
        dx = self.theta * (-self.state) + self.sigma * np.random.randn(self.size).astype(np.float32)
        self.state = self.state + dx
        return self.state

def clamp_action(vec, max_norm=0.04):
    n = float(np.linalg.norm(vec))
    if n == 0.0 or n <= max_norm:
        return vec
    return (vec * (max_norm / n)).astype(np.float32)

def shaped_reward_components(bh, bh_ma50, action, prev_action, extras):
    w_grad, w20, w40, w60, w_act, w_smooth = 0.8, 0.02, 0.05, 0.10, 0.5, 0.25
    grad_term = w_grad * max(0.0, bh - (bh_ma50 or 0.0))
    dwell = (w20 if bh > 0.20 else 0.0) + (w40 if bh > 0.40 else 0.0) + (w60 if bh > 0.60 else 0.0)
    force_pen = w_act * (float(np.linalg.norm(action)) ** 2)
    smooth_pen = w_smooth * float(np.sum((action - prev_action) ** 2))
    goal_term = 0.4 * float(extras.get('goal_resonance', 0.0))
    tension_term = 0.1 * float(extras.get('avg_tension', 0.0))
    valence_term = 0.1 * float(extras.get('valence', 0.0))
    surprise_term = 0.4 * float(extras.get('surprise', 0.0))
    try:
        fe = float(extras.get('free_energy', LAST_INTRINSIC.get('free_energy', 0.0)))
        epi = float(extras.get('epistemic', LAST_INTRINSIC.get('epistemic', 0.0)))
        topo = float(extras.get('topo', LAST_INTRINSIC.get('topo', 0.0)))
    except Exception:
        fe = epi = topo = 0.0
    fe_term = 0.2 * fe
    epi_term = 0.3 * epi
    topo_term = 0.3 * topo
    return {
        'grad': grad_term, 'dwell': dwell, 'force_pen': -force_pen, 'smooth_pen': -smooth_pen,
        'goal': goal_term, 'tension': tension_term, 'valence': valence_term, 'surprise': surprise_term,
        'free_energy': fe_term, 'epistemic': epi_term, 'topo': topo_term
    }
    def inflate_quasicrystal(self, base_lattice: np.ndarray, inflation_factor: float = 1.618033988749895) -> np.ndarray:
        """Inflate quasicrystal lattice using golden ratio scaling with local matching rules."""
        try:
            if base_lattice.size == 0:
                return base_lattice

            # Golden ratio inflation matrix
            phi = inflation_factor
            inflation_matrix = np.array([
                [phi, 1.0],
                [1.0, phi]
            ])

            # Apply inflation to 2D projections of E8 lattice
            inflated_points = []
            for point in base_lattice:
                if point.size >= 2:
                    # Inflate first 2 dimensions
                    inflated_2d = inflation_matrix @ point[:2]
                    # Keep higher dimensions unchanged
                    inflated_point = np.concatenate([inflated_2d, point[2:]])
                    inflated_points.append(inflated_point)
                else:
                    inflated_points.append(point)

            return np.array(inflated_points)

        except Exception as e:
            self.console.log(f"[yellow]Quasicrystal inflation failed: {e}[/yellow]")
            return base_lattice

    def enforce_matching_rules(self, lattice_points: np.ndarray, tolerance: float = 0.1) -> np.ndarray:
        """Enforce local matching rules for quasicrystal aperiodicity."""
        try:
            if lattice_points.size == 0 or len(lattice_points) < 2:
                return lattice_points

            # Find nearest neighbors and enforce matching rules
            from scipy.spatial import cKDTree
            tree = cKDTree(lattice_points)
            corrected_points = []

            for i, point in enumerate(lattice_points):
                # Find local neighborhood
                distances, indices = tree.query(point, k=7)  # Include self + 6 neighbors

                if len(indices) > 1:
                    neighbors = lattice_points[indices[1:]]  # Exclude self

                    # Compute local matching rule (golden ratio relationships)
                    phi = 1.618033988749895
                    matching_scores = []

                    for neighbor in neighbors:
                        diff = neighbor - point
                        if np.linalg.norm(diff) > 1e-6:
                            # Check if distances follow golden ratio relationships
                            dist = np.linalg.norm(diff)
                            phi_ratio = dist / phi
                            nearest_phi = round(phi_ratio)

                            if abs(phi_ratio - nearest_phi) < tolerance:
                                matching_scores.append(1.0)
                            else:
                                matching_scores.append(0.0)

                    # If matching rule is satisfied, keep point; otherwise adjust
                    if matching_scores and np.mean(matching_scores) > 0.5:
                        corrected_points.append(point)
                    else:
                        # Adjust point to satisfy matching rule
                        if neighbors.size > 0:
                            centroid = np.mean(neighbors, axis=0)
                            adjusted_point = point + 0.1 * (centroid - point)
                            corrected_points.append(adjusted_point)
                        else:
                            corrected_points.append(point)
                else:
                    corrected_points.append(point)

            return np.array(corrected_points)

        except Exception as e:
            self.console.log(f"[yellow]Matching rules enforcement failed: {e}[/yellow]")
            return lattice_points

    def validate_quasicrystal_properties(self, lattice_points: np.ndarray) -> Dict[str, float]:
        """Validate quasicrystal properties: aperiodicity, golden ratio scaling, local matching."""
        try:
            validation_results = {
                'aperiodicity_score': 0.0,
                'golden_ratio_compliance': 0.0,
                'local_matching_score': 0.0,
                'dimensional_consistency': 0.0
            }

            if lattice_points.size == 0:
                return validation_results

            # Check aperiodicity (no translational symmetry)
            aperiodicity_score = self._check_aperiodicity(lattice_points)
            validation_results['aperiodicity_score'] = aperiodicity_score

            # Check golden ratio scaling relationships
            golden_ratio_score = self._check_golden_ratio_scaling(lattice_points)
            validation_results['golden_ratio_compliance'] = golden_ratio_score

            # Check local matching rules
            matching_score = self._check_local_matching(lattice_points)
            validation_results['local_matching_score'] = matching_score

            # Check dimensional consistency
            dim_consistency = self._check_dimensional_consistency(lattice_points)
            validation_results['dimensional_consistency'] = dim_consistency

            return validation_results

        except Exception as e:
            self.console.log(f"[yellow]Quasicrystal validation failed: {e}[/yellow]")
            return validation_results

    def _check_aperiodicity(self, points: np.ndarray) -> float:
        """Check aperiodicity by testing for translational symmetry."""
        try:
            if len(points) < 10:
                return 0.5

            # Test for translational symmetry by checking if lattice can be generated by unit cell
            from scipy.spatial import cKDTree
            tree = cKDTree(points)

            # Sample test vectors
            test_vectors = []
            for i in range(min(5, len(points))):
                for j in range(i+1, min(i+6, len(points))):
                    test_vectors.append(points[j] - points[i])

            aperiodicity_score = 0.0
            symmetry_count = 0

            for vec in test_vectors:
                # Count how many points can be reached by translating by this vector
                translated_points = points + vec
                matches = 0

                for tp in translated_points:
                    dist, _ = tree.query(tp, k=1)
                    if dist < 0.1:  # Tolerance for lattice matching
                        matches += 1

                if matches > len(points) * 0.8:  # If most points match, indicates periodicity
                    symmetry_count += 1

            aperiodicity_score = 1.0 - (symmetry_count / len(test_vectors))
            return max(0.0, min(1.0, aperiodicity_score))

        except Exception:
            return 0.5

    def _check_golden_ratio_scaling(self, points: np.ndarray) -> float:
        """Check if distances follow golden ratio scaling relationships."""
        try:
            phi = 1.618033988749895
            golden_ratio_count = 0
            total_pairs = 0

            for i in range(len(points)):
                for j in range(i+1, len(points)):
                    diff = points[j] - points[i]
                    dist = np.linalg.norm(diff)

                    if dist > 1e-6:
                        # Check if distance is close to golden ratio multiples
                        phi_ratio = dist / phi
                        nearest_phi = round(phi_ratio)

                        if abs(phi_ratio - nearest_phi) < 0.1:
                            golden_ratio_count += 1

                        total_pairs += 1

            return golden_ratio_count / total_pairs if total_pairs > 0 else 0.0

        except Exception:
            return 0.0

    def _check_local_matching(self, points: np.ndarray) -> float:
        """Check local matching rules in neighborhood of each point."""
        try:
            from scipy.spatial import cKDTree
            tree = cKDTree(points)
            matching_scores = []

            for i, point in enumerate(points):
                # Get local neighborhood
                distances, indices = tree.query(point, k=7)

                if len(indices) > 1:
                    neighbors = points[indices[1:]]
                    local_score = 0.0

                    # Check matching rules in local neighborhood
                    for neighbor in neighbors:
                        diff = neighbor - point
                        dist = np.linalg.norm(diff)

                        if dist > 1e-6:
                            # Check if distance follows golden ratio
                            phi = 1.618033988749895
                            phi_ratio = dist / phi
                            nearest_phi = round(phi_ratio)

                            if abs(phi_ratio - nearest_phi) < 0.15:
                                local_score += 1.0

                    local_score /= len(neighbors)
                    matching_scores.append(local_score)

            return np.mean(matching_scores) if matching_scores else 0.0

        except Exception:
            return 0.0

    def _check_dimensional_consistency(self, points: np.ndarray) -> float:
        """Check dimensional consistency across the lattice."""
        try:
            if len(points) == 0:
                return 0.0

            # Check if all points have the same dimensionality
            dimensions = [p.size for p in points]
            consistent_count = sum(1 for d in dimensions if d == dimensions[0])

            return consistent_count / len(dimensions)

        except Exception:
            return 0.0

def normalize_vector(v):
    norm = np.linalg.norm(v)
    return v / norm if norm > 1e-9 else v

class CliffordRotorGenerator:
    def __init__(self, mind_instance: 'E8Mind', layout, blades):
        self.mind = mind_instance
        self.layout = layout
        self.blades = blades or {}
        try:
            dims = getattr(layout, 'dims', 0) if layout is not None else 0
        except Exception:
            dims = 0
        if CLIFFORD_AVAILABLE and dims:
            try:
                self.basis_vectors = [self.blades.get(f'e{i+1}') for i in range(dims)]
            except Exception:
                self.basis_vectors = []
        else:
            self.basis_vectors = []

    def _random_unit_bivector(self):
        if not CLIFFORD_AVAILABLE or len(self.basis_vectors) < 2:
            return 0
        try:
            n = len(self.basis_vectors)
            i, j = np.random.choice(np.arange(n), size=2, replace=False)
            B = self.basis_vectors[i] ^ self.basis_vectors[j]
            return B.normal()
        except Exception:
            return 0

    def _select_dynamic_pair(self, shell: 'DimensionalShell') -> Optional[tuple[np.ndarray, np.ndarray]]:
        nodes = list(shell.vectors.keys())
        if len(nodes) < 2:
            return None
        candidates = []
        for nid in nodes:
            node_data = self.mind.memory.graph_db.get_node(nid)
            if node_data:
                vec_np = shell.get_vector(nid)
                if vec_np is not None and np.linalg.norm(vec_np) > 1e-9:
                    candidates.append({'id': nid, 'temp': node_data.get('temperature', 0.1), 'vec': vec_np})
        if len(candidates) < 2:
            return None
        candidates.sort(key=lambda x: x['temp'], reverse=True)
        anchor_a = candidates[0]
        best_partner = None
        max_dist = -1.0
        for partner_candidate in candidates[1:min(len(candidates), 15)]:
            dist = 1.0 - abs(np.dot(normalize_vector(anchor_a['vec']), normalize_vector(partner_candidate['vec'])))
            if dist > max_dist:
                max_dist = dist
                best_partner = partner_candidate
        if best_partner is None:
            return None
        return anchor_a['vec'], best_partner['vec']

    def generate_rotor(self, shell: 'DimensionalShell', angle: float) -> Any:
        if not CLIFFORD_AVAILABLE or not self.basis_vectors:
            return 1
        pair = self._select_dynamic_pair(shell)
        if pair is None:
            return 1
        try:
            a_vec, b_vec = pair
            a = sum(val * bv for val, bv in zip(a_vec, self.basis_vectors))
            b = sum(val * bv for val, bv in zip(b_vec, self.basis_vectors))
            B = (a ^ b)
            if hasattr(B, 'normal'):
                try:
                    Bn = B.normal()
                    if abs(Bn) < 1e-9:
                        return 1
                    rotor = (-Bn * angle / 2.0).exp()
                    return rotor
                except Exception:
                    return 1
            return 1
        except Exception:
            return 1

    def create_rotor_from_bivector(self, B: Any, angle: float) -> Any:
        """Create rotor R = exp((?/2) B) from unit bivector B and angle ?."""
        if not CLIFFORD_AVAILABLE:
            return 1
        try:
            if hasattr(B, 'normal'):
                Bn = B.normal()
                if abs(Bn) < 1e-9:
                    return 1
                rotor = (Bn * angle / 2.0).exp()
                return rotor
            return 1
        except Exception:
            return 1

    def rotate_vector(self, rotor: Any, vector: Any) -> Any:
        """Rotate vector x' = R x ?R using rotor R."""
        if not CLIFFORD_AVAILABLE:
            return vector
        try:
            if hasattr(rotor, '__mul__') and hasattr(rotor, 'conjugate'):
                # x' = R x ?R
                rotor_rev = rotor.conjugate()
                return rotor * vector * rotor_rev
            return vector
        except Exception:
            return vector

    def compose_rotors(self, rotor1: Any, rotor2: Any) -> Any:
        """Compose two rotors: R_total = R2 * R1."""
        if not CLIFFORD_AVAILABLE:
            return 1
        try:
            if hasattr(rotor1, '__mul__') and hasattr(rotor2, '__mul__'):
                return rotor2 * rotor1
            return 1
        except Exception:
            return 1

    def rotor_inverse(self, rotor: Any) -> Any:
        """Compute rotor inverse R^{-1} = ?R (reverse/conjugate)."""
        if not CLIFFORD_AVAILABLE:
            return 1
        try:
            if hasattr(rotor, 'conjugate'):
                return rotor.conjugate()
            return 1
        except Exception:
            return 1

    def integrate_rotor_step(self, rotor: Any, omega: Any, dt: float) -> Any:
        """Time-stepped rotor integration: R_{t+?t} � exp((?t/2) O) R_t where O = ? B."""
        if not CLIFFORD_AVAILABLE:
            return rotor
        try:
            # Differential update: dR/dt = (1/2) O R
            # Approximate: R_{t+?t} � exp((?t/2) O) R_t
            if hasattr(omega, '__mul__') and hasattr(rotor, '__mul__'):
                delta_rotor = (omega * dt / 2.0).exp()
                return delta_rotor * rotor
            return rotor
        except Exception:
            return rotor

    def validate_rotor_properties(self, rotor: Any, test_vectors: list = None) -> dict:
        """Validate rotor properties: norm invariance, double-cover behavior."""
        if not CLIFFORD_AVAILABLE:
            return {'norm_invariant': False, 'double_cover': False}
        results = {'norm_invariant': False, 'double_cover': False}

        try:
            # Prepare simple basis test vectors if not provided
            if test_vectors is None:
                test_vectors = [np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
                               np.array([0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])]

            norm_invariant = False
            double_cover = False

            # Try a few perturbations to avoid false negatives from numerical edge cases
            for attempt in range(3):
                angle = 0.5 * (1.0 + 0.01 * attempt)
                try:
                    # If rotor not provided, try to generate one
                    if rotor is None:
                        rotor = self.generate_rotor(shell=None, angle=angle)

                    all_norm_ok = True
                    for vec_np in test_vectors:
                        if len(self.basis_vectors) >= len(vec_np):
                            vec = sum(val * bv for val, bv in zip(vec_np, self.basis_vectors))
                            rotated = self.rotate_vector(rotor, vec)
                            # Fall back: convert to numpy arrays if Clifford multivectors
                            try:
                                vnorm = float(np.linalg.norm(np.asarray([float(getattr(vec, 'value', vec)[i]) if hasattr(vec, 'value') else vec[i] for i in range(len(vec_np))])))
                                rnorm = float(np.linalg.norm(np.asarray([float(getattr(rotated, 'value', rotated)[i]) if hasattr(rotated, 'value') else rotated[i] for i in range(len(vec_np))])))
                            except Exception:
                                try:
                                    vnorm = float(abs(vec))
                                    rnorm = float(abs(rotated))
                                except Exception:
                                    vnorm = float(np.linalg.norm(np.asarray(vec)))
                                    rnorm = float(np.linalg.norm(np.asarray(rotated)))

                            if abs(vnorm - rnorm) > 1e-6:
                                all_norm_ok = False
                                break

                    if all_norm_ok:
                        norm_invariant = True

                    # Double-cover: rotation by ~2pi should return vector (identity)
                    try:
                        R2 = self.generate_rotor(shell=None, angle=2 * math.pi)
                        all_double_ok = True
                        for vec_np in test_vectors:
                            if len(self.basis_vectors) >= len(vec_np):
                                vec = sum(val * bv for val, bv in zip(vec_np, self.basis_vectors))
                                rotated2 = self.rotate_vector(R2, vec)
                                # compare via numpy fallback
                                try:
                                    v_arr = np.asarray([float(getattr(vec, 'value', vec)[i]) if hasattr(vec, 'value') else vec[i] for i in range(len(vec_np))])
                                    r2_arr = np.asarray([float(getattr(rotated2, 'value', rotated2)[i]) if hasattr(rotated2, 'value') else rotated2[i] for i in range(len(vec_np))])
                                    if not np.allclose(v_arr, r2_arr, atol=1e-5, rtol=1e-4):
                                        all_double_ok = False
                                        break
                                except Exception:
                                    try:
                                        if abs(vec - rotated2) > 1e-5:
                                            all_double_ok = False
                                            break
                                    except Exception:
                                        all_double_ok = False
                                        break

                        if all_double_ok:
                            double_cover = True
                    except Exception:
                        double_cover = False

                    # If both properties satisfied, stop early
                    if norm_invariant and double_cover:
                        break

                except Exception:
                    # try a different rotor construction on failure
                    try:
                        B = self._random_unit_bivector()
                        Rf = self.create_rotor_from_bivector(B, angle)
                        rotor = Rf
                    except Exception:
                        continue

            results['norm_invariant'] = norm_invariant
            results['double_cover'] = double_cover

            # Emit metrics where available
            try:
                if hasattr(self.mind, 'metrics'):
                    self.mind.metrics.gauge('rotor.norm_invariant', 1.0 if norm_invariant else 0.0)
                    self.mind.metrics.gauge('rotor.double_cover', 1.0 if double_cover else 0.0)
            except Exception:
                pass

        except Exception as e:
            try:
                if hasattr(self.mind, 'console'):
                    self.mind.console.log(f"[ROTOR] Validation error: {e}")
            except Exception:
                pass

        return results

def _finite_guard(M: np.ndarray):
    """Zero out non-finite entries; report if any existed."""
    bad = ~np.isfinite(M)
    if bad.any():
        M = M.copy()
        M[bad] = 0.0
        return M, True
    return M, False

def _symmetrize(M: np.ndarray):
    return 0.5 * (M + M.T)

def _scale_to_rms(M: np.ndarray, target_rms: float = 1.0):
    rms = float(np.sqrt(np.mean(M*M)) + 1e-18)
    return M * (target_rms / rms), rms

def _estimate_condition_from_svd(Ms: np.ndarray):
    try:
        s = np.linalg.svd(Ms, compute_uv=False)
        if s.size == 0: return np.inf, 0.0, 0.0
        return float(s[0] / max(s[-1], 1e-18)), float(s[0]), float(s[-1])
    except Exception:
        return np.inf, 0.0, 0.0

def _safe_metric_inverse(g: np.ndarray):
    """
    Bullet-proof inverse for symmetric metric-like matrices.
    Returns (g_inv, diag) with diag containing driver, ridge, scale, cond, had_nonfinite.
    """
    g = np.asarray(g, dtype=np.float64)
    ridge_min = float(os.getenv("E8_FM_RIDGE_MIN", "1e-10"))
    ridge_max = float(os.getenv("E8_FM_RIDGE_MAX", "1e-3"))
    target_rms = float(os.getenv("E8_FM_METRIC_TARGET_RMS", "1.0"))

    diag = {"driver": "?", "ridge": 0.0, "scale": 1.0, "cond": np.inf, "had_nonfinite": False}

    g, had_nonfinite = _finite_guard(g)
    g = _symmetrize(g)
    g_s, scale = _scale_to_rms(g, target_rms=target_rms)

    I = np.eye(g_s.shape[0], dtype=g_s.dtype)

    # Try Hermitian pinv first (best path for symmetric matrices)
    try:
        from scipy.linalg import pinvh
        lam = ridge_min
        g_inv_s = pinvh(g_s + lam * I)
        cond, smax, smin = _estimate_condition_from_svd(g_s)
        diag.update(driver="pinvh", ridge=lam, scale=scale, cond=cond, had_nonfinite=had_nonfinite)
        return g_inv_s / scale, diag
    except Exception:
        pass

    # SVD fallback with adaptive ridge
    try:
        U, s, Vt = np.linalg.svd(g_s, full_matrices=False)
        smax = float(s[0]) if s.size else 1.0
        smin = float(s[-1]) if s.size else 1e-18
        cond = smax / max(smin, 1e-18)
        # Adaptive ridge grows with ill-conditioning but capped
        lam = min(ridge_max, max(ridge_min, 1e-6 * cond))
        s_inv = 1.0 / (s + lam)
        g_inv_s = (Vt.T * s_inv) @ U.T
        diag.update(driver="svd", ridge=lam, scale=scale, cond=cond, had_nonfinite=had_nonfinite)
        return g_inv_s / scale, diag
    except Exception:
        # Last-ditch: regularized Gram solve
        GTG = g_s.T @ g_s + ridge_max * I
        try:
            g_inv_s = np.linalg.solve(GTG, g_s.T)  # (GᵀG + λI)^(-1) Gᵀ
            diag.update(driver="gram_solve", ridge=ridge_max, scale=scale, cond=np.inf, had_nonfinite=had_nonfinite)
            return g_inv_s / scale, diag
        except Exception:
            # Absolute fallback: NumPy pinv with heavy ridge on unscaled g
            g_fallback = g + ridge_max * np.eye(g.shape[0], dtype=g.dtype)
            diag.update(driver="np_pinv", ridge=ridge_max, scale=1.0, cond=np.inf, had_nonfinite=had_nonfinite)
            return np.linalg.pinv(g_fallback), diag

def _thermostat_fields(E: np.ndarray, B: np.ndarray):
    """
    Smoothly cap field magnitudes to prevent metric blow-ups.
    Uses a tanh-like soft cap with per-field saturation.
    """
    E = np.asarray(E, dtype=np.float64)
    B = np.asarray(B, dtype=np.float64)

    E_sat = float(os.getenv("E8_FM_E_SAT", "8.0"))
    B_sat = float(os.getenv("E8_FM_B_SAT", "8.0"))
    gain  = float(os.getenv("E8_FM_TANH_GAIN", "0.75"))

    # elementwise soft cap: x -> sat * tanh(gain * x / sat)
    E_safe = E_sat * np.tanh(gain * (E / (E_sat + 1e-18)))
    B_safe = B_sat * np.tanh(gain * (B / (B_sat + 1e-18)))

    # zero out non-finite just in case
    E_safe[~np.isfinite(E_safe)] = 0.0
    B_safe[~np.isfinite(B_safe)] = 0.0

    # norms for telemetry
    En = float(np.sqrt(np.mean(E_safe*E_safe)))
    Bn = float(np.sqrt(np.mean(B_safe*B_safe)))
    return E_safe, B_safe, En, Bn

def _apply_metric_update_with_backtracking(g_old: np.ndarray, delta_g: np.ndarray):
    cond_limit   = float(os.getenv("E8_FM_COND_LIMIT", "1e12"))
    max_bt       = int(os.getenv("E8_FM_BACKTRACKS", "5"))
    shrink       = float(os.getenv("E8_FM_STEP_SHRINK", "0.5"))
    target_rms   = float(os.getenv("E8_FM_METRIC_TARGET_RMS", "1.0"))

    alpha = 1.0
    for tries in range(max_bt + 1):
        g_new = _symmetrize(g_old + alpha * delta_g)
        g_new, had_nonfinite = _finite_guard(g_new)
        g_scaled, scale = _scale_to_rms(g_new, target_rms=target_rms)
        cond, smax, smin = _estimate_condition_from_svd(g_scaled)

        if (not had_nonfinite) and (cond < cond_limit):
            return g_new, alpha, {"cond": cond, "scale": scale, "tries": tries, "accepted": True}

        alpha *= shrink  # backtrack and retry

    # reject update; return old metric
    return g_old, 0.0, {"cond": np.inf, "scale": 1.0, "tries": max_bt, "accepted": False}


class CliffordRotorGenerator:
    def __init__(self, mind_instance: 'E8Mind', layout, blades):
        self.mind = mind_instance
        self.layout = layout
        self.blades = blades or {}
        try:
            dims = getattr(layout, 'dims', 0) if layout is not None else 0
        except Exception:
            dims = 0
        if CLIFFORD_AVAILABLE and dims:
            try:
                self.basis_vectors = [self.blades.get(f'e{i+1}') for i in range(dims)]
            except Exception:
                self.basis_vectors = []
        else:
            self.basis_vectors = []

    def _random_unit_bivector(self):
        if not CLIFFORD_AVAILABLE or len(self.basis_vectors) < 2:
            return 0
        try:
            n = len(self.basis_vectors)
            i, j = np.random.choice(np.arange(n), size=2, replace=False)
            B = self.basis_vectors[i] ^ self.basis_vectors[j]
            return B.normal()
        except Exception:
            return 0

    def _select_dynamic_pair(self, shell: 'DimensionalShell') -> Optional[tuple[np.ndarray, np.ndarray]]:
        nodes = list(shell.vectors.keys())
        if len(nodes) < 2:
            return None
        candidates = []
        for nid in nodes:
            node_data = self.mind.memory.graph_db.get_node(nid)
            if node_data:
                vec_np = shell.get_vector(nid)
                if vec_np is not None and np.linalg.norm(vec_np) > 1e-9:
                    candidates.append({'id': nid, 'temp': node_data.get('temperature', 0.1), 'vec': vec_np})
        if len(candidates) < 2:
            return None
        candidates.sort(key=lambda x: x['temp'], reverse=True)
        anchor_a = candidates[0]
        best_partner = None
        max_dist = -1.0
        for partner_candidate in candidates[1:min(len(candidates), 15)]:
            dist = 1.0 - abs(np.dot(normalize_vector(anchor_a['vec']), normalize_vector(partner_candidate['vec'])))
            if dist > max_dist:
                max_dist = dist
                best_partner = partner_candidate
        if best_partner is None:
            return None
        return anchor_a['vec'], best_partner['vec']

    def generate_rotor(self, shell: 'DimensionalShell', angle: float) -> Any:
        if not CLIFFORD_AVAILABLE or not self.basis_vectors:
            return 1
        pair = self._select_dynamic_pair(shell)
        if pair is None:
            return 1
        try:
            a_vec, b_vec = pair
            a = sum(val * bv for val, bv in zip(a_vec, self.basis_vectors))
            b = sum(val * bv for val, bv in zip(b_vec, self.basis_vectors))
            B = (a ^ b)
            if hasattr(B, 'normal'):
                try:
                    Bn = B.normal()
                    if abs(Bn) < 1e-9:
                        return 1
                    rotor = (-Bn * angle / 2.0).exp()
                    return rotor
                except Exception:
                    return 1
            return 1
        except Exception:
            return 1

    def create_rotor_from_bivector(self, B: Any, angle: float) -> Any:
        """Create rotor R = exp((?/2) B) from unit bivector B and angle ?."""
        if not CLIFFORD_AVAILABLE:
            return 1
        try:
            if hasattr(B, 'normal'):
                Bn = B.normal()
                if abs(Bn) < 1e-9:
                    return 1
                rotor = (Bn * angle / 2.0).exp()
                return rotor
            return 1
        except Exception:
            return 1

    def rotate_vector(self, rotor: Any, vector: Any) -> Any:
        """Rotate vector x' = R x ?R using rotor R."""
        if not CLIFFORD_AVAILABLE:
            return vector
        try:
            if hasattr(rotor, '__mul__') and hasattr(rotor, 'conjugate'):
                # x' = R x ?R
                rotor_rev = rotor.conjugate()
                return rotor * vector * rotor_rev
            return vector
        except Exception:
            return vector

    def compose_rotors(self, rotor1: Any, rotor2: Any) -> Any:
        """Compose two rotors: R_total = R2 * R1."""
        if not CLIFFORD_AVAILABLE:
            return 1
        try:
            if hasattr(rotor1, '__mul__') and hasattr(rotor2, '__mul__'):
                return rotor2 * rotor1
            return 1
        except Exception:
            return 1

    def rotor_inverse(self, rotor: Any) -> Any:
        """Compute rotor inverse R^{-1} = ?R (reverse/conjugate)."""
        if not CLIFFORD_AVAILABLE:
            return 1
        try:
            if hasattr(rotor, 'conjugate'):
                return rotor.conjugate()
            return 1
        except Exception:
            return 1

    def integrate_rotor_step(self, rotor: Any, omega: Any, dt: float) -> Any:
        """Time-stepped rotor integration: R_{t+?t}  exp((?t/2) O) R_t where O = ? B."""
        if not CLIFFORD_AVAILABLE:
            return rotor
        try:
            # Differential update: dR/dt = (1/2) O R
            # Approximate: R_{t+?t}  exp((?t/2) O) R_t
            if hasattr(omega, '__mul__') and hasattr(rotor, '__mul__'):
                delta_rotor = (omega * dt / 2.0).exp()
                return delta_rotor * rotor
            return rotor
        except Exception:
            return rotor

    def validate_rotor_properties(self, rotor: Any, test_vectors: list = None) -> dict:
        """Validate rotor properties: norm invariance, double-cover behavior."""
        if not CLIFFORD_AVAILABLE:
            return {'norm_invariant': False, 'double_cover': False}
        results = {'norm_invariant': False, 'double_cover': False}

        try:
            # Prepare simple basis test vectors if not provided
            if test_vectors is None:
                test_vectors = [np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),
                               np.array([0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])]

            norm_invariant = False
            double_cover = False

            # Try a few perturbations to avoid false negatives from numerical edge cases
            for attempt in range(3):
                angle = 0.5 * (1.0 + 0.01 * attempt)
                try:
                    # If rotor not provided, try to generate one
                    if rotor is None:
                        rotor = self.generate_rotor(shell=None, angle=angle)

                    all_norm_ok = True
                    for vec_np in test_vectors:
                        if len(self.basis_vectors) >= len(vec_np):
                            vec = sum(val * bv for val, bv in zip(vec_np, self.basis_vectors))
                            rotated = self.rotate_vector(rotor, vec)
                            # Fall back: convert to numpy arrays if Clifford multivectors
                            try:
                                vnorm = float(np.linalg.norm(np.asarray([float(getattr(vec, 'value', vec)[i]) if hasattr(vec, 'value') else vec[i] for i in range(len(vec_np))])))
                                rnorm = float(np.linalg.norm(np.asarray([float(getattr(rotated, 'value', rotated)[i]) if hasattr(rotated, 'value') else rotated[i] for i in range(len(vec_np))])))
                            except Exception:
                                try:
                                    vnorm = float(abs(vec))
                                    rnorm = float(abs(rotated))
                                except Exception:
                                    vnorm = float(np.linalg.norm(np.asarray(vec)))
                                    rnorm = float(np.linalg.norm(np.asarray(rotated)))

                            if abs(vnorm - rnorm) > 1e-6:
                                all_norm_ok = False
                                break

                    if all_norm_ok:
                        norm_invariant = True

                    # Double-cover: rotation by ~2pi should return vector (identity)
                    try:
                        R2 = self.generate_rotor(shell=None, angle=2 * math.pi)
                        all_double_ok = True
                        for vec_np in test_vectors:
                            if len(self.basis_vectors) >= len(vec_np):
                                vec = sum(val * bv for val, bv in zip(vec_np, self.basis_vectors))
                                rotated2 = self.rotate_vector(R2, vec)
                                # compare via numpy fallback
                                try:
                                    v_arr = np.asarray([float(getattr(vec, 'value', vec)[i]) if hasattr(vec, 'value') else vec[i] for i in range(len(vec_np))])
                                    r2_arr = np.asarray([float(getattr(rotated2, 'value', rotated2)[i]) if hasattr(rotated2, 'value') else rotated2[i] for i in range(len(vec_np))])
                                    if not np.allclose(v_arr, r2_arr, atol=1e-5, rtol=1e-4):
                                        all_double_ok = False
                                        break
                                except Exception:
                                    try:
                                        if abs(vec - rotated2) > 1e-5:
                                            all_double_ok = False
                                            break
                                    except Exception:
                                        all_double_ok = False
                                        break

                        if all_double_ok:
                            double_cover = True
                    except Exception:
                        double_cover = False

                    # If both properties satisfied, stop early
                    if norm_invariant and double_cover:
                        break

                except Exception:
                    # try a different rotor construction on failure
                    try:
                        B = self._random_unit_bivector()
                        Rf = self.create_rotor_from_bivector(B, angle)
                        rotor = Rf
                    except Exception:
                        continue

            results['norm_invariant'] = norm_invariant
            results['double_cover'] = double_cover

            # Emit metrics where available
            try:
                if hasattr(self.mind, 'metrics'):
                    self.mind.metrics.gauge('rotor.norm_invariant', 1.0 if norm_invariant else 0.0)
                    self.mind.metrics.gauge('rotor.double_cover', 1.0 if double_cover else 0.0)
            except Exception:
                pass

        except Exception as e:
            try:
                if hasattr(self.mind, 'console'):
                    self.mind.console.log(f"[ROTOR] Validation error: {e}")
            except Exception:
                pass

        return results

    def validate_rotor_algebra(self, rotor1: Any, rotor2: Any, rotor3: Any = None) -> dict:
        """Validate rotor algebra: associativity, inverse properties."""
        if not CLIFFORD_AVAILABLE:
            return {'associativity': False, 'inverse': False}

        results = {'associativity': False, 'inverse': False}

        try:
            # Test associativity: (R1 R2) R3 = R1 (R2 R3)
            if rotor3 is not None and hasattr(rotor1, '__mul__'):
                left_assoc = (rotor1 * rotor2) * rotor3
                right_assoc = rotor1 * (rotor2 * rotor3)

                if hasattr(left_assoc, '__sub__') and hasattr(right_assoc, '__sub__'):
                    diff = left_assoc - right_assoc
                    results['associativity'] = abs(diff) < 1e-10

            # Test inverse: R * R^{-1} = 1
            if hasattr(rotor1, 'conjugate'):
                rotor1_inv = rotor1.conjugate()
                product = rotor1 * rotor1_inv

                if hasattr(product, '__sub__'):
                    # Check if product is close to identity
                    identity = 1
                    if hasattr(identity, '__sub__'):
                        diff = product - identity
                        results['inverse'] = abs(diff) < 1e-10

        except Exception as e:
            if hasattr(self.mind, 'console'):
                self.mind.console.log(f"[ROTOR] Algebra validation error: {e}")

        return results




class HyperdimensionalFluidMantle:
    """Continuous fluid spacetime consciousness substrate replacing discrete shells"""

    def __init__(self, core_dimensions=8, max_dimensions=248, lattice_points=None):
        self.core_dim = core_dimensions
        self.max_dim = max_dimensions
        if lattice_points is None:
            self.lattice_points = []
        elif isinstance(lattice_points, np.ndarray):
            self.lattice_points = lattice_points.tolist()
        else:
            self.lattice_points = list(lattice_points)

        # Spacetime geometry
        self.metric_tensor = np.eye(8)  # Start with flat spacetime
        self.christoffel_symbols = {}
        self.ricci_tensor = np.zeros((8, 8))

        # Fluid dynamics fields
        self.velocity_field = {}  # 8D velocity vectors at each point
        self.pressure_field = {}  # Scalar pressure at each point
        self.density_field = {}   # Memory density at each point
        self.potential_field = {} # Consciousness potential phi

        # Physical constants for consciousness fluid
        self.consciousness_viscosity = 0.1
        self.consciousness_diffusivity = 0.05
        self.planck_consciousness = 1e-6  # Minimum consciousness unit
        self.reynolds_number = 1000

        # Dimensional gradient parameters
        self.gradient_sigma = 2.0
        self.gradient_steepness = 1.0

        self.initialize_fields()

    def initialize_fields(self):
        """Initialize consciousness fluid fields"""
        if not self.lattice_points:
            return

        for i, point in enumerate(self.lattice_points):
            pos_key = tuple(point[:8] if len(point) >= 8 else np.pad(point, (0, 8-len(point))))

            # Initialize fluid state
            self.velocity_field[pos_key] = np.random.normal(0, 0.01, 8)
            self.pressure_field[pos_key] = 0.0
            self.density_field[pos_key] = 0.0
            self.potential_field[pos_key] = 0.0

    def dimensional_gradient(self, position):
        """Smooth dimensional gradient: d(r) = 8 + 240 * tanh(r/sigma)"""
        if isinstance(position, (list, tuple)):
            position = np.array(position[:8])
        elif len(position) > 8:
            position = position[:8]

        radius = np.linalg.norm(position)
        return (self.core_dim + 
                (self.max_dim - self.core_dim) * 
                np.tanh(radius / self.gradient_sigma))

    def compute_velocity_field(self, consciousness_distribution):
        """Calculate fluid velocity from consciousness potential"""
        for pos_key in self.potential_field:
            position = np.array(pos_key)

            # Compute potential gradient (grad phi)
            gradient = self.compute_potential_gradient(position, consciousness_distribution)

            # Add geometric corrections for curved spacetime
            metric_correction = self.compute_christoffel_correction(position)

            # Velocity field v = -grad phi + corrections
            self.velocity_field[pos_key] = -gradient + metric_correction

    def compute_potential_gradient(self, position, consciousness_data):
        """Compute grad phi numerically"""
        epsilon = 1e-6
        gradient = np.zeros(8)

        for i in range(8):
            pos_plus = position.copy()
            pos_minus = position.copy()
            pos_plus[i] += epsilon
            pos_minus[i] -= epsilon

            phi_plus = self.evaluate_potential_at(pos_plus, consciousness_data)
            phi_minus = self.evaluate_potential_at(pos_minus, consciousness_data)

            gradient[i] = (phi_plus - phi_minus) / (2 * epsilon)

        return gradient

    def evaluate_potential_at(self, position, consciousness_data):
        """Evaluate consciousness potential at position"""
        potential = 0.0
        pos_key = tuple(position)

        # Distance-weighted contribution from all consciousness sources
        for data_point in consciousness_data:
            if hasattr(data_point, 'position') and hasattr(data_point, 'strength'):
                distance = np.linalg.norm(position - data_point.position[:8])
                potential += data_point.strength / (1 + distance)

        return potential

    def update_spacetime_curvature(self, consciousness_stress_tensor):
        """Update metric tensor using Einstein field equations"""
        # Simplified Einstein equations: G_mu_nu = 8*pi*G T_mu_nu
        G = 1.0  # Consciousness-spacetime coupling constant

        # Compute Einstein tensor from current metric
        einstein_tensor = self.compute_einstein_tensor()

        # Update metric: delta g_mu_nu = 8*pi*G delta T_mu_nu
        metric_update = 8 * np.pi * G * consciousness_stress_tensor
        self.metric_tensor += 0.01 * metric_update  # Small time step

        # Recompute Christoffel symbols
        self.update_christoffel_symbols()

    def compute_einstein_tensor(self):
        """Simplified Einstein tensor computation"""
        # For demonstration - full implementation would use proper tensor calculus
        return self.ricci_tensor - 0.5 * np.trace(self.ricci_tensor) * self.metric_tensor

    def update_christoffel_symbols(self):
        """Update connection coefficients for parallel transport"""
        # Simplified computation - proper implementation uses metric derivatives
        for i in range(8):
            for j in range(8):
                for k in range(8):
                    symbol_key = (i, j, k)
                    self.christoffel_symbols[symbol_key] = 0.01 * np.random.normal()

    def compute_christoffel_correction(self, position):
        """Geometric correction term for curved spacetime"""
        correction = np.zeros(8)

        # Simplified correction based on position curvature
        for i in range(8):
            curvature_effect = 0.0
            for j in range(8):
                for k in range(8):
                    symbol = self.christoffel_symbols.get((i, j, k), 0.0)
                    curvature_effect += symbol * position[j] * position[k]
            correction[i] = curvature_effect

        return 0.01 * correction  # Scale factor

    def transport_memories_along_geodesics(self, memories, dt=0.1):
        """Transport memories along geodesic flows in curved spacetime"""
        transported_memories = []

        for memory in memories:
            if not hasattr(memory, 'position') or not hasattr(memory, 'velocity'):
                continue

            # Current position and velocity in 8D
            pos = memory.position[:8] if len(memory.position) >= 8 else np.pad(memory.position, (0, 8-len(memory.position)))
            vel = memory.velocity[:8] if hasattr(memory, 'velocity') and len(memory.velocity) >= 8 else np.zeros(8)

            # Geodesic equation: d?x/dt? = -?(dx/dt)(dx/dt)
            acceleration = self.compute_geodesic_acceleration(pos, vel)

            # Update position and velocity
            new_velocity = vel + acceleration * dt
            new_position = pos + new_velocity * dt

            # Update memory object
            memory.position = new_position
            memory.velocity = new_velocity
            memory.effective_dimension = self.dimensional_gradient(new_position)

            transported_memories.append(memory)

        return transported_memories

    def compute_geodesic_acceleration(self, position, velocity):
        """Compute acceleration along geodesic using Christoffel symbols"""
        acceleration = np.zeros(8)

        for i in range(8):
            acc_component = 0.0
            for j in range(8):
                for k in range(8):
                    symbol = self.christoffel_symbols.get((i, j, k), 0.0)
                    acc_component -= symbol * velocity[j] * velocity[k]
            acceleration[i] = acc_component

        return acceleration

    def flow_step(self, consciousness_data, dt=0.1):
        """Single time step of fluid dynamics evolution"""
        # 1. Update consciousness potential field
        self.update_potential_field(consciousness_data)

        # 2. Compute new velocity field
        self.compute_velocity_field(consciousness_data)

        # 3. Update pressure field using continuity equation
        self.update_pressure_field(dt)

        # 4. Update density field using advection-diffusion
        self.update_density_field(dt)

        # 5. Update spacetime curvature
        stress_tensor = self.compute_consciousness_stress_tensor()
        self.update_spacetime_curvature(stress_tensor)

    def update_potential_field(self, consciousness_data):
        """Update consciousness potential phi"""
        for pos_key in self.potential_field:
            position = np.array(pos_key)
            self.potential_field[pos_key] = self.evaluate_potential_at(position, consciousness_data)

    def update_pressure_field(self, dt):
        """Update pressure using simplified Navier-Stokes"""
        for pos_key in self.pressure_field:
            # Simplified pressure update based on velocity divergence
            velocity = self.velocity_field.get(pos_key, np.zeros(8))
            divergence = self.compute_velocity_divergence(pos_key, velocity)

            # Pressure correction
            self.pressure_field[pos_key] -= 0.1 * divergence * dt

    def update_density_field(self, dt):
        """Update density using advection-diffusion equation"""
        new_density_field = {}

        for pos_key in self.density_field:
            position = np.array(pos_key)
            density = self.density_field[pos_key]
            velocity = self.velocity_field.get(pos_key, np.zeros(8))

            # Advection term: -v dot grad rho
            density_gradient = self.compute_density_gradient(pos_key)
            advection = -np.dot(velocity, density_gradient)

            # Diffusion term: D laplacian rho
            laplacian = self.compute_density_laplacian(pos_key)
            diffusion = self.consciousness_diffusivity * laplacian

            # Source term (memory creation/destruction)
            source = self.compute_density_source(position, density)

            # Update equation: d rho/dt = -v dot grad rho + D laplacian rho + S
            new_density = density + dt * (advection + diffusion + source)
            new_density_field[pos_key] = max(0, new_density)  # Non-negative density

        self.density_field = new_density_field

    def compute_velocity_divergence(self, pos_key, velocity):
        """Compute div v at position"""
        # Simplified finite difference
        epsilon = 1e-6
        divergence = 0.0

        position = np.array(pos_key)
        for i in range(8):
            pos_plus = position.copy()
            pos_minus = position.copy()
            pos_plus[i] += epsilon
            pos_minus[i] -= epsilon

            v_plus = self.velocity_field.get(tuple(pos_plus), np.zeros(8))[i]
            v_minus = self.velocity_field.get(tuple(pos_minus), np.zeros(8))[i]

            divergence += (v_plus - v_minus) / (2 * epsilon)

        return divergence

    def compute_density_gradient(self, pos_key):
        """Compute grad rho at position"""
        epsilon = 1e-6
        gradient = np.zeros(8)
        position = np.array(pos_key)

        for i in range(8):
            pos_plus = position.copy()
            pos_minus = position.copy()
            pos_plus[i] += epsilon
            pos_minus[i] -= epsilon

            rho_plus = self.density_field.get(tuple(pos_plus), 0.0)
            rho_minus = self.density_field.get(tuple(pos_minus), 0.0)

            gradient[i] = (rho_plus - rho_minus) / (2 * epsilon)

        return gradient

    def compute_density_laplacian(self, pos_key):
        """Compute laplacian rho at position"""
        epsilon = 1e-6
        laplacian = 0.0
        position = np.array(pos_key)
        center_density = self.density_field[pos_key]

        for i in range(8):
            pos_plus = position.copy()
            pos_minus = position.copy()
            pos_plus[i] += epsilon
            pos_minus[i] -= epsilon

            rho_plus = self.density_field.get(tuple(pos_plus), 0.0)
            rho_minus = self.density_field.get(tuple(pos_minus), 0.0)

            second_derivative = (rho_plus - 2*center_density + rho_minus) / (epsilon**2)
            laplacian += second_derivative

        return laplacian

    def compute_density_source(self, position, current_density):
        """Memory creation/destruction source term"""
        # Memories tend to be created in high-potential regions
        potential = self.potential_field.get(tuple(position), 0.0)

        # Creation rate proportional to potential, destruction rate proportional to density
        creation_rate = 0.1 * max(0, potential)
        destruction_rate = 0.05 * current_density

        return creation_rate - destruction_rate

    def compute_consciousness_stress_tensor(self):
        """Compute stress-energy tensor for consciousness field"""
        # T_mu_nu = energy-momentum tensor for consciousness
        stress_tensor = np.zeros((8, 8))

        # T_00 = energy density (sum of consciousness energy)
        total_energy = sum(self.density_field.values()) + sum(self.potential_field.values())
        stress_tensor[0, 0] = total_energy

        # T_ij = momentum flux (simplified)
        for i in range(1, 8):
            for j in range(1, 8):
                momentum_flux = 0.0
                for pos_key in self.velocity_field:
                    velocity = self.velocity_field[pos_key]
                    density = self.density_field.get(pos_key, 0.0)
                    momentum_flux += density * velocity[i-1] * velocity[j-1]

                stress_tensor[i, j] = momentum_flux / len(self.velocity_field) if self.velocity_field else 0.0

        return stress_tensor

class HyperdimensionalFieldMantle:
    """Maxwell-Lorentz field substrate evolving on the lifted shell lattice."""

    def __init__(self, mind: Optional['E8Mind'] = None, core_dimensions: int = 8, max_dimensions: int = 248, lattice_points: Optional[Iterable[Any]] = None):
        # --- PATCH A: Mantle.curvature ---
        self._vec8d = self._vec8d.__get__(self)
        self.find_high_curvature_regions = self.find_high_curvature_regions.__get__(self)
        self.mind = mind
        self.core_dim = int(core_dimensions)
        self.max_dim = int(max_dimensions)
        if lattice_points is None:
            self.lattice_points: list[Any] = []
        elif isinstance(lattice_points, np.ndarray):
            self.lattice_points = lattice_points.tolist()
        else:
            self.lattice_points = list(lattice_points)

        self.metric_tensor = np.eye(8, dtype=float)
        self.base_metric = np.eye(4, dtype=float)
        self.metric_perturbation_global = np.zeros((4, 4), dtype=float)
        self.christoffel_symbols: dict[tuple[int, int, int], float] = {}

        self.vector_potential: dict[tuple[float, ...], np.ndarray] = {}
        self.scalar_potential: dict[tuple[float, ...], float] = {}
        self.field_tensor: dict[tuple[float, ...], np.ndarray] = {}
        self.metric_perturbations: dict[tuple[float, ...], np.ndarray] = {}
        self.current_density: dict[tuple[float, ...], np.ndarray] = {}
        self.energy_density_field: dict[tuple[float, ...], float] = {}
        self.pressure_field: dict[tuple[float, ...], float] = {}
        self.velocity_field: dict[tuple[float, ...], np.ndarray] = {}
        self.potential_field: dict[tuple[float, ...], float] = {}
        self.shell_energy: dict[int, float] = {}
        self.shell_poynting: dict[int, float] = {}
        self.stress_energy: dict[tuple[float, ...], np.ndarray] = {}

        self._neighbor_map: dict[tuple[float, ...], list[tuple[float, ...]]] = {}
        self._prev_vector_potential: dict[tuple[float, ...], np.ndarray] = {}
        self._prev_scalar_potential: dict[tuple[float, ...], float] = {}
        self._prev_shell_matrices: dict[int, np.ndarray] = {}
        self._field_summary: dict[str, float] = {}
        self._lattice_array4 = np.zeros((0, 4), dtype=float)

        self.c_light = 299_792_458.0
        self.mu_0 = 4.0e-7 * math.pi
        self.epsilon_0 = 1.0 / (self.mu_0 * self.c_light ** 2)
        self.alpha_src = float(os.getenv("E8_FIELD_ALPHA_SRC", "1.0"))
        self.beta_src = float(os.getenv("E8_FIELD_BETA_SRC", "0.8"))
        self.source_sigma = float(os.getenv("E8_FIELD_SOURCE_SIGMA", "1.5"))
        self.gauge_damping = float(os.getenv("E8_FIELD_GAUGE_DAMPING", "0.1"))
        self.gravitational_coupling = float(os.getenv("E8_FIELD_GRAV_COUPLING", "1e-3"))

        # Telemetry
        self._svd_failures = 0
        self._metric_backtracks = 0
        self._inv_uses = {"pinvh":0, "svd":0, "gram_solve":0, "np_pinv":0}
        self.Enorm = 0.0
        self.Bnorm = 0.0
        
        # Previous values for change detection
        self._prev_Enorm = None
        self._prev_Bnorm = None

        self.initialize_fields()

    def _point_key(self, point: Any) -> tuple[float, ...]:
        arr = np.asarray(point, dtype=float).reshape(-1)
        if arr.size < 8:
            arr = np.pad(arr, (0, 8 - arr.size), mode='constant')
        else:
            arr = arr[:8]
        return tuple(np.round(arr, 6))

    def _build_neighbor_map(self, k: int = 6) -> None:
        if not self.lattice_points:
            self._neighbor_map = {}
            return
        try:
            pts = np.asarray(self.lattice_points, dtype=float)
            if pts.ndim != 2:
                pts = pts.reshape(len(self.lattice_points), -1)
            truncated = pts[:, :4] if pts.shape[1] >= 4 else np.pad(pts, ((0, 0), (0, max(0, 4 - pts.shape[1]))))
            count = truncated.shape[0]
            if count <= 1:
                self._neighbor_map = {}
                return
            neighbor_count = min(max(1, k), count - 1)
            idx_matrix = None
            try:
                from sklearn.neighbors import KDTree as _SKKDTree  # type: ignore
                tree = _SKKDTree(truncated)
                idx_matrix = tree.query(truncated, k=neighbor_count + 1, return_distance=False)
            except Exception:
                try:
                    from scipy.spatial import cKDTree  # type: ignore
                    tree = cKDTree(truncated)
                    idx_matrix = tree.query(truncated, k=neighbor_count + 1, workers=1)[1]
                except Exception:
                    idx_matrix = None
            if idx_matrix is None:
                distances = cdist(truncated, truncated)
                idx_matrix = np.argpartition(distances, range(1, neighbor_count + 1), axis=1)[:, 1:neighbor_count + 1]
            neighbor_map: dict[tuple[float, ...], list[tuple[float, ...]]] = {}
            for idx, indices in enumerate(np.atleast_2d(idx_matrix)):
                key = self._point_key(self.lattice_points[idx])
                neighbors = []
                for j in indices:
                    if int(j) == idx:
                        continue
                    neighbor_key = self._point_key(self.lattice_points[int(j)])
                    neighbors.append(neighbor_key)
                neighbor_map[key] = neighbors[:neighbor_count]
            self._neighbor_map = neighbor_map
        except Exception:
            self._neighbor_map = {}

    def initialize_fields(self) -> None:
        if not self.lattice_points:
            return
        for point in self.lattice_points:
            key = self._point_key(point)
            self.vector_potential[key] = np.zeros(4, dtype=float)
            self.scalar_potential[key] = 0.0
            self.potential_field[key] = 0.0
            self.field_tensor[key] = np.zeros((4, 4), dtype=float)
            self.metric_perturbations[key] = np.zeros((4, 4), dtype=float)
            self.current_density[key] = np.zeros(4, dtype=float)
            self.energy_density_field[key] = 0.0
            self.pressure_field[key] = 0.0
            self.velocity_field[key] = np.zeros(8, dtype=float)
            self.stress_energy[key] = np.zeros((4, 4), dtype=float)
        self._build_neighbor_map()
        try:
            self._lattice_array4 = np.asarray([np.asarray(self._point_key(p)[:4], dtype=float) for p in self.lattice_points], dtype=float)
        except Exception:
            self._lattice_array4 = np.zeros((0, 4), dtype=float)


    def sample_field_value(self, field: dict[tuple[float, ...], Any], position: Any, default: float = 0.0) -> float:
        if not field:
            return float(default)
        key = self._point_key(position)
        if key in field:
            try:
                return float(field[key])
            except Exception:
                return float(default)
        if self._lattice_array4.size == 0:
            return float(default)
        try:
            pos = np.asarray(position, dtype=float)
            if pos.size < 4:
                pos = np.pad(pos, (0, 4 - pos.size))
            else:
                pos = pos[:4]
            diff = self._lattice_array4 - pos[:4]
            dist2 = np.sum(diff * diff, axis=1)
            if dist2.size == 0:
                return float(default)
            idx = int(np.argmin(dist2))
            nearest_key = self._point_key(self.lattice_points[idx])
            return float(field.get(nearest_key, default))
        except Exception:
            return float(default)

    def sample_vector_field(self, field: dict[tuple[float, ...], np.ndarray], position: Any, length: int) -> np.ndarray:
        key = self._point_key(position)
        vec = field.get(key)
        if vec is not None:
            try:
                arr = np.asarray(vec, dtype=float)
                if arr.size >= length:
                    return arr[:length]
                padded = np.zeros(length, dtype=float)
                padded[:arr.size] = arr
                return padded
            except Exception:
                pass
        if self._lattice_array4.size == 0:
            return np.zeros(length, dtype=float)
        try:
            pos = np.asarray(position, dtype=float)
            if pos.size < 4:
                pos = np.pad(pos, (0, 4 - pos.size))
            else:
                pos = pos[:4]
            diff = self._lattice_array4 - pos[:4]
            dist2 = np.sum(diff * diff, axis=1)
            if dist2.size == 0:
                return np.zeros(length, dtype=float)
            idx = int(np.argmin(dist2))
            nearest_key = self._point_key(self.lattice_points[idx])
            vec = field.get(nearest_key)
            if vec is None:
                return np.zeros(length, dtype=float)
            arr = np.asarray(vec, dtype=float)
            if arr.size >= length:
                return arr[:length]
            padded = np.zeros(length, dtype=float)
            padded[:arr.size] = arr
            return padded
        except Exception:
            return np.zeros(length, dtype=float)
    def flow_step(self, consciousness_data: Optional[Iterable[Any]], dt: float = 0.1) -> None:
        if not self.vector_potential:
            return
        dt = float(max(dt, 1e-5))
        try:
            self.compute_em_sources(consciousness_data, dt)
            self._prev_vector_potential = {k: v.copy() for k, v in self.vector_potential.items()}
            self._prev_scalar_potential = dict(self.scalar_potential)
            self.update_vector_potential(dt)
            self.update_field_tensor(dt)
            self.update_em_stress_energy()
            aggregate_T = None
            if self.stress_energy:
                aggregate_T = sum(self.stress_energy.values()) / max(len(self.stress_energy), 1)
            if aggregate_T is not None:
                self.update_spacetime_curvature(aggregate_T)
            self._update_field_summaries()
        except Exception as exc:
            if self.mind is not None and hasattr(self.mind, 'console'):
                try:
                    self.mind.console.log(f"[FieldMantle] flow_step error: {exc}")
                except Exception:
                    pass

    def compute_em_sources(self, consciousness_data: Optional[Iterable[Any]], dt: float) -> None:
        for key in self.current_density:
            self.current_density[key].fill(0.0)
        sources = self._compute_rotor_sources(dt)
        for data_point in consciousness_data or []:
            try:
                position = np.asarray(getattr(data_point, 'position', np.zeros(4)), dtype=float)[:4]
                strength = float(getattr(data_point, 'strength', 0.0))
            except Exception:
                continue
            sources.append({'position': position, 'rho': strength, 'current': np.zeros(3, dtype=float)})
        if not sources:
            self._project_current_to_divergence_free()
            return
        sigma2 = max(self.source_sigma ** 2, 1e-6)
        for key in self.vector_potential:
            pos = np.asarray(key[:4], dtype=float)
            accumulator = np.zeros(4, dtype=float)
            for src in sources:
                src_pos = np.asarray(src['position'], dtype=float)[:4]
                delta = pos - src_pos
                dist2 = float(np.dot(delta, delta))
                weight = math.exp(-dist2 / (2.0 * sigma2))
                accumulator[0] += weight * float(src['rho'])
                current_vec = np.asarray(src['current'], dtype=float)
                k = min(3, current_vec.size)
                if k:
                    accumulator[1:1 + k] += weight * current_vec[:k]
            self.current_density[key] = accumulator
        total_charge = sum(val[0] for val in self.current_density.values())
        if self.current_density and abs(total_charge) > 1e-8:
            correction = total_charge / len(self.current_density)
            for key in self.current_density:
                self.current_density[key][0] -= correction
        self._project_current_to_divergence_free()

    def _compute_rotor_sources(self, dt: float) -> list[dict[str, Any]]:
        sources: list[dict[str, Any]] = []
        if self.mind is None:
            return sources
        dimensional_shells = getattr(self.mind, 'dimensional_shells', {}) or {}
        if not dimensional_shells:
            return sources
        dt = float(max(dt, 1e-5))
        for dim, shell in dimensional_shells.items():
            try:
                matrix, _ids = shell.get_all_vectors_as_matrix()
            except Exception:
                matrix = None
            if matrix is None or matrix.size == 0:
                continue
            matrix = np.asarray(matrix, dtype=float)
            centroid = np.mean(matrix, axis=0)
            spatial_axis = centroid[:3]
            axis_norm = np.linalg.norm(spatial_axis)
            if axis_norm < 1e-9:
                continue
            direction = spatial_axis / axis_norm
            prev_matrix = self._prev_shell_matrices.get(dim)
            if prev_matrix is not None and prev_matrix.shape == matrix.shape:
                delta = matrix - prev_matrix
                baseline = np.linalg.norm(prev_matrix) + 1e-6
                omega = float(np.linalg.norm(delta)) / (baseline * dt)
            else:
                omega = float(np.linalg.norm(matrix)) * 1e-3
            self._prev_shell_matrices[dim] = matrix.copy()
            kappa = float(np.sign(np.sum(centroid)) or 1.0)
            rho = self.beta_src * omega * kappa
            current = self.alpha_src * omega * direction
            sources.append({'position': centroid[:4], 'rho': rho, 'current': current})
        return sources

    def _project_current_to_divergence_free(self) -> None:
        if not self._neighbor_map:
            return
        for key, neighbors in self._neighbor_map.items():
            if not neighbors:
                continue
            local_currents = []
            for neighbor in neighbors:
                vec = self.current_density.get(neighbor)
                if vec is not None and vec.size >= 4:
                    local_currents.append(vec[1:4])
            if not local_currents:
                continue
            mean_current = np.mean(local_currents, axis=0)
            current_vec = self.current_density.get(key)
            if current_vec is not None and current_vec.size >= 4:
                current_vec[1:4] -= mean_current
                self.current_density[key] = current_vec

    def _lorenz_divergence(self, key: tuple[float, ...]) -> float:
        A_mu = self.vector_potential.get(key)
        if A_mu is None:
            return 0.0
        neighbors = self._neighbor_map.get(key, [])
        if not neighbors:
            return float(np.sum(A_mu))
        pos = np.asarray(key[:4], dtype=float)
        divergence = 0.0
        for neighbor in neighbors:
            neighbor_A = self.vector_potential.get(neighbor)
            if neighbor_A is None:
                continue
            delta = np.asarray(neighbor[:4], dtype=float) - pos
            dist2 = float(np.dot(delta, delta)) + 1e-9
            divergence += float(np.dot(neighbor_A - A_mu, delta) / dist2)
        return divergence / max(len(neighbors), 1)

    def update_vector_potential(self, dt: float) -> None:
        mu0 = self.mu_0
        for key, A_mu in self.vector_potential.items():
            source = self.current_density.get(key)
            if source is None:
                continue
            divergence = self._lorenz_divergence(key)
            update = mu0 * source - self.gauge_damping * divergence
            A_mu = A_mu + dt * update
            self.vector_potential[key] = A_mu
            self.scalar_potential[key] = float(A_mu[0])
            self.potential_field[key] = float(A_mu[0])

    def _approx_scalar_gradient(self, key: tuple[float, ...]) -> np.ndarray:
        phi = self.scalar_potential.get(key, 0.0)
        neighbors = self._neighbor_map.get(key, [])
        if not neighbors:
            return np.zeros(3, dtype=float)
        pos = np.asarray(key[:4], dtype=float)
        grad = np.zeros(3, dtype=float)
        weight_sum = 0.0
        for neighbor in neighbors:
            neighbor_phi = self.scalar_potential.get(neighbor, 0.0)
            delta_phi = neighbor_phi - phi
            delta_pos = np.asarray(neighbor[:4], dtype=float) - pos
            spatial_delta = delta_pos[:3]
            dist2 = float(np.dot(spatial_delta, spatial_delta))
            if dist2 <= 1e-9:
                continue
            weight = 1.0 / dist2
            grad += weight * spatial_delta * delta_phi
            weight_sum += weight
        if weight_sum <= 0.0:
            return np.zeros(3, dtype=float)
        return grad / weight_sum

    def _approx_vector_curl(self, key: tuple[float, ...]) -> np.ndarray:
        neighbors = self._neighbor_map.get(key, [])
        if len(neighbors) < 2:
            return np.zeros(3, dtype=float)
        A_mu = self.vector_potential.get(key)
        if A_mu is None:
            return np.zeros(3, dtype=float)
        pos = np.asarray(key[:4], dtype=float)
        curl = np.zeros(3, dtype=float)
        for neighbor in neighbors[:3]:
            neighbor_A = self.vector_potential.get(neighbor)
            if neighbor_A is None:
                continue
            edge = np.asarray(neighbor[:4], dtype=float) - pos
            spatial_edge = edge[:3]
            curl += np.cross(spatial_edge, neighbor_A[1:4] - A_mu[1:4])
        return curl / max(len(neighbors[:3]), 1)

    def update_field_tensor(self, dt: float) -> None:
        for key, A_mu in self.vector_potential.items():
            prev_A = self._prev_vector_potential.get(key, np.zeros_like(A_mu))
            dA_dt = (A_mu - prev_A) / dt
            grad_phi = self._approx_scalar_gradient(key)
            E = -(dA_dt[1:4] + grad_phi)
            curl_A = self._approx_vector_curl(key)
            B = curl_A
            F = np.zeros((4, 4), dtype=float)
            for i in range(3):
                F[0, i + 1] = E[i]
                F[i + 1, 0] = -E[i]
            F[1, 2] = -B[2]
            F[2, 1] = B[2]
            F[1, 3] = B[1]
            F[3, 1] = -B[1]
            F[2, 3] = -B[0]
            F[3, 2] = B[0]
            self.field_tensor[key] = F
            velocity = np.zeros(8, dtype=float)
            S = (1.0 / self.mu_0) * np.cross(E, B)
            velocity[:3] = S
            self.velocity_field[key] = velocity

    def update_em_stress_energy(self) -> None:
        c = self.c_light
        mu0 = self.mu_0
        epsilon0 = self.epsilon_0
        shell_energy: dict[int, float] = {}
        shell_poynting: dict[int, float] = {}
        for key, F in self.field_tensor.items():
            E = np.array([F[0, 1], F[0, 2], F[0, 3]], dtype=float)
            B = np.array([F[2, 3], -F[1, 3], F[1, 2]], dtype=float)
            
            E, B, self.Enorm, self.Bnorm = _thermostat_fields(E, B)
            
            # Only log when values change and logging is enabled
            if (hasattr(self, "mind") and hasattr(self.mind, "console") and E8_LOG_FIELDMANTLE_THERMOSTAT):
                if (self._prev_Enorm is None or 
                    self._prev_Bnorm is None or 
                    abs(self.Enorm - self._prev_Enorm) > 1e-6 or 
                    abs(self.Bnorm - self._prev_Bnorm) > 1e-6):
                    
                    self.mind.console.log(f"[FieldMantle][Thermostat] ||E||={self.Enorm:.3f} ||B||={self.Bnorm:.3f}")
                    self._prev_Enorm = self.Enorm
                    self._prev_Bnorm = self.Bnorm

            energy_density = 0.5 * (epsilon0 * np.dot(E, E) + (1.0 / mu0) * np.dot(B, B))
            S = (1.0 / mu0) * np.cross(E, B)
            stress = np.zeros((4, 4), dtype=float)
            stress[0, 0] = energy_density
            for i in range(3):
                stress[0, i + 1] = S[i] / c
                stress[i + 1, 0] = S[i] / c
            sigma = np.zeros((3, 3), dtype=float)
            for i in range(3):
                for j in range(3):
                    sigma[i, j] = epsilon0 * E[i] * E[j] + (1.0 / mu0) * B[i] * B[j]
            trace_term = np.trace(sigma)
            for i in range(3):
                sigma[i, i] -= 0.5 * trace_term
            stress[1:4, 1:4] = sigma
            self.energy_density_field[key] = float(energy_density)
            self.pressure_field[key] = float(np.trace(sigma) / 3.0)
            self.stress_energy[key] = stress
            radius = int(np.floor(np.linalg.norm(np.asarray(key[:4], dtype=float))))
            shell_energy[radius] = shell_energy.get(radius, 0.0) + float(energy_density)
            shell_poynting[radius] = shell_poynting.get(radius, 0.0) + float(np.linalg.norm(S))
        self.shell_energy = shell_energy
        self.shell_poynting = shell_poynting

    def _update_field_summaries(self) -> None:
        total_energy = float(sum(self.energy_density_field.values()))
        total_flux = float(sum(self.shell_poynting.values()))
        max_energy = float(max(self.energy_density_field.values())) if self.energy_density_field else 0.0
        self._field_summary = {
            'total_energy': total_energy,
            'total_flux': total_flux,
            'max_energy_density': max_energy
        }

    # --- PATCH A: Mantle.curvature ---
    def _vec8d(self, emb):
        """Return normalized 8D projection from an embedding or None if unavailable.
        Uses the canonical MemoryManager.project_to_dim8 when available.
        """
        try:
            mem = getattr(self.mind, 'memory', None)
            if mem and hasattr(mem, 'project_to_dim8'):
                return mem.project_to_dim8(emb)
            # Fallback to simple slice+norm
            import numpy as _np
            v = _np.asarray(emb, dtype=_np.float32).reshape(-1)
            if v.size < 8:
                return None
            v8 = v[:8]
            n = float(_np.linalg.norm(v8))
            if n < 1e-9:
                return None
            return (v8 / n).astype(_np.float32)
        except Exception:
            return None

    def find_high_curvature_regions(self, top_n=3):
        """
        Identify gravity wells via curvature proxies (mass, Laplacian, diffusion centrality, E8 alignment),
        then apply Q(t)-based compression from the decoding-axis projection, a robust z-gate, and
        a short cooldown to prevent one hotspot from dominating across steps.

        Env knobs:
          E8_CURV_W_MASS (default 0.40), E8_CURV_W_LAP (0.30), E8_CURV_W_DIFF (0.20), E8_CURV_W_E8 (0.10)
          E8_Q_TAU (0.12)      -> timescale in Q(t)
          E8_Q_ALPHA (0.65)    -> compression strength
          E8_CURV_ZMAX (2.5)   -> z-gate clamp (mu + z*std)
          E8_WELL_STREAK (3)   -> how many consecutive wins before cooldown
          E8_WELL_COOLDOWN (25)-> steps to cool a repeated winner
          E8_COOLDOWN_FACTOR (0.35) -> downweight while cooled
          E8_DECODING_AXIS_INDEX (optional int) -> fallback to a particular E8 root as decoding axis
        """
        import numpy as _np
        mind = getattr(self, "mind", None)
        if mind is None or not hasattr(mind, "memory"):
            return []

        # ---------- lazy init of well-state ----------
        if not hasattr(self, "_well_state"):
            self._well_state = {
                "last_top_id": None,
                "streak": 0,
                "cool_until": {},   # node_id -> step_until
                "born_at": {},      # node_id -> step first seen or reset on cooldown
            }
        # ---------- lazy init of curvature AGC stats ----------
        if not hasattr(self, "_curv_mu"):
            self._curv_mu = 0.0
        if not hasattr(self, "_curv_var"):
            self._curv_var = 1.0

        try:
            _mem = getattr(mind, "memory", None)
            _gdb = getattr(_mem, "graph_db", None)
            G = getattr(_gdb, "graph", None)
            if G is None or (hasattr(G, "number_of_nodes") and G.number_of_nodes() == 0):
                return []
            # weights
            w_mass = float(os.getenv("E8_CURV_W_MASS", "0.40"))
            w_lap  = float(os.getenv("E8_CURV_W_LAP",  "0.30"))
            w_diff = float(os.getenv("E8_CURV_W_DIFF", "0.20"))
            w_e8   = float(os.getenv("E8_CURV_W_E8",   "0.10"))

            # decoding axis v for s_Q = |r_Q · v|
            # prefer physics.decoding_axis_unit if present; else mean of first few roots; else first root
            _phys = getattr(mind, "physics", None)
            roots = getattr(_phys, "roots_unit", None)
            v = getattr(_phys, "decoding_axis_unit", None)
            if v is None:
                if roots is not None and len(roots) >= 8:
                    v = _np.mean(_np.asarray(roots[:8], dtype=float), axis=0)
                    v = v / (_np.linalg.norm(v) + 1e-9)
                elif roots is not None and len(roots) > 0:
                    v = _np.asarray(roots[0], dtype=float)
                    v = v / (_np.linalg.norm(v) + 1e-9)
                else:
                    v = _np.zeros(8, dtype=float); v[0] = 1.0  # ultra-safe fallback

            # knobs for compression/cooldown
            tau      = float(os.getenv("E8_Q_TAU", "0.08"))
            alpha    = float(os.getenv("E8_Q_ALPHA", "0.40"))
            zmax     = float(os.getenv("E8_CURV_ZMAX", "2.0"))
            # Hard global caps for curvature/flux (fail-safe)
            # curv_cap = float(os.getenv("E8_CURV_GLOBAL_CAP", "6.0"))  # COMMENTED OUT - removing 6.0 curvature limiter
            # Hard global caps for curvature/flux (fail-safe)
            # curv_cap = float(os.getenv("E8_CURV_GLOBAL_CAP", "6.0"))  # COMMENTED OUT - removing 6.0 curvature limiter
            streak_L = int(os.getenv("E8_WELL_STREAK", "3"))
            cool_S   = int(os.getenv("E8_WELL_COOLDOWN", "25"))
            cool_F   = float(os.getenv("E8_COOLDOWN_FACTOR", "0.35"))

            # Precompute degrees and diffusion proxy
            deg = {n: max(1, G.degree(n)) for n in G.nodes}
            diff = {n: sum(1.0 / max(1, deg.get(nb, 1)) for nb in G.neighbors(n)) for n in G.nodes}

            # ---- collect component features first for robust per-component normalization ----
            node_ids, v8s, masses, laps, diffs, aligns = [], [], [], [], [], []
            for n, data in G.nodes(data=True):
                mass = float(data.get("connectivity_potential", data.get("rating", 0.5)))
                emb = mind.memory.main_vectors.get(n)
                v8 = self._vec8d(emb)
                if v8 is None:
                    continue
                nbs = list(G.neighbors(n))
                nb_mass_mean = (float(sum(float(G.nodes[m].get("connectivity_potential",
                                                              G.nodes[m].get("rating", 0.5)))
                                          for m in nbs)) / len(nbs)) if nbs else 0.0
                lap = mass - nb_mass_mean
                dc  = float(diff.get(n, 0.0))
                align = float(_np.dot(v8, v) / ((_np.linalg.norm(v8) * _np.linalg.norm(v)) + 1e-9))

                node_ids.append(n)
                v8s.append(v8)
                masses.append(mass)
                laps.append(lap)
                diffs.append(dc)
                aligns.append(align)

            if not node_ids:
                return []

            masses = _np.asarray(masses, dtype=float)
            laps   = _np.asarray(laps, dtype=float)
            diffs  = _np.asarray(diffs, dtype=float)
            aligns = _np.asarray(aligns, dtype=float)

            def _zrob(x: _np.ndarray) -> _np.ndarray:
                med = _np.median(x)
                q75, q25 = _np.percentile(x, [75, 25])
                iqr = float(q75 - q25)
                return (x - med) / (iqr + 1e-6)

            m_z = _zrob(masses)
            l_z = _zrob(laps)
            d_z = _zrob(diffs)
            a_z = _zrob(aligns)

            # ---- raw scores (normalized components) ----
            raw = []   # (node_id, v8, curv_raw)
            for i, n in enumerate(node_ids):
                curv_raw = (w_mass * float(m_z[i])) + (w_lap * float(l_z[i])) + (w_diff * float(d_z[i])) + (w_e8 * float(a_z[i]))
                # no hard clamp here; allow AGC+tanh to squash
                raw.append((n, v8s[i], float(curv_raw)))

            if not raw:
                return []

            # ---- AGC: population-wise EMA mean/var for curvature mix ----
            curvs = _np.array([c for _, _, c in raw], dtype=_np.float32)
            beta = float(os.getenv("E8_CURV_AGC_BETA", "0.01"))
            batch_mu  = float(_np.mean(curvs))
            batch_var = float(_np.var(curvs) + 1e-9)
            self._curv_mu  = (1.0 - beta) * float(self._curv_mu)  + beta * batch_mu
            self._curv_var = (1.0 - beta) * float(self._curv_var) + beta * batch_var
            sigma = float(max(1e-6, _np.sqrt(self._curv_var)))
            # telemetry: occasionally log AGC stats for monitoring
            try:
                agc_log_mod = int(os.getenv("E8_CURV_LOG_MOD", "50"))
                step_num = int(getattr(mind, "step_num", 0) or 0)
                if agc_log_mod > 0 and (step_num % agc_log_mod == 0):
                    metrics_log("curvature.agc", {
                        "mu": float(self._curv_mu),
                        "sigma": sigma,
                        "batch_mu": batch_mu,
                        "batch_var": batch_var,
                        "n": int(len(curvs)),
                        "event": "curvature.agc"
                    })
            except Exception:
                pass

            # ---- Q(t) compression from paper: Q(t)=1-exp(-s_Q t), s_Q=|r_Q·v| ----
            # Use node's v8 as r_Q proxy and step_num as time proxy (scaled by tau).
            step = int(getattr(mind, "step_num", 0) or 0)
            scores = []  # (node_id, v8, curv_final)
            for n, v8, c in raw:
                # s_Q = |v8 · v|
                sQ = float(abs(_np.dot(v8, v) / ((_np.linalg.norm(v8) * _np.linalg.norm(v)) + 1e-9)))
                # Q(t) made local to node age (since birth or last cooldown)
                born_at = int(self._well_state["born_at"].get(n, step))
                if n not in self._well_state["born_at"]:
                    self._well_state["born_at"][n] = step
                    born_at = step
                age = max(1, step - born_at)
                # Q(t): 0..1, faster rise for larger sQ (alignment) and with node age
                q = float(1.0 - _np.exp(-sQ * tau * age))
                # compression with denom floor: divide by max(eps, 1 + alpha*q*|c|)
                denom = float(max(1e-6, 1.0 + alpha * q * abs(c)))
                c_comp = c / denom
                # AGC standardization and soft symmetric squashing
                z = (c_comp - self._curv_mu) / sigma
                gamma = float(os.getenv("E8_CURV_SQUASH_GAIN", "0.8"))
                c_final = float(_np.tanh(gamma * z))  # in (-1, 1)
                # hard global cap and finite guard
                if not _np.isfinite(c_final):
                    c_final = 0.0
                # c_final = float(_np.clip(c_final, -curv_cap, curv_cap))  # COMMENTED OUT - removing 6.0 curvature limiter
                scores.append((n, v8, c_final))

            # ---- cooldown: downweight nodes currently cooled ----
            cool_until = self._well_state["cool_until"]
            cooled = []
            now = step
            for i, (n, v8, c) in enumerate(scores):
                until = int(cool_until.get(n, -10**9))
                if now < until:
                    scores[i] = (n, v8, float(c * cool_F))
                    cooled.append(n)

            # initial rank
            scores.sort(key=lambda x: x[2], reverse=True)

            # update streak and possibly schedule cooldown for the winner
            if scores:
                top_id = scores[0][0]
                if top_id == self._well_state["last_top_id"]:
                    self._well_state["streak"] += 1
                else:
                    self._well_state["last_top_id"] = top_id
                    self._well_state["streak"] = 1

                if self._well_state["streak"] >= streak_L:
                    # schedule cooldown
                    cool_until[top_id] = now + cool_S
                    # reset node age so Q(t) adapts again after cooldown
                    self._well_state["born_at"][top_id] = now
                    # immediately apply damping this call, then re-rank
                    scores[0] = (scores[0][0], scores[0][1], float(scores[0][2] * cool_F))
                    scores.sort(key=lambda x: x[2], reverse=True)

            # return top hotspots as (vec8d, curvature_score)
            top = [(v8, curv) for (_, v8, curv) in scores[:max(1, int(top_n))]]

            # telemetry (best effort)
            try:
                mind.console.jsonl(evt="curvature.hotspots",
                                   count=len(top),
                                   top=(top[0][1] if top else 0.0),
                                   cooled=len(cooled),
                                   streak=self._well_state.get("streak", 0))
                # Also emit to metrics file occasionally
                agc_log_mod = int(os.getenv("E8_CURV_LOG_MOD", "50"))
                step_num = int(getattr(mind, "step_num", 0) or 0)
                if agc_log_mod > 0 and (step_num % agc_log_mod == 0):
                    metrics_log("curvature.hotspots", {
                        "count": int(len(top)),
                        "top": float(top[0][1]) if top else 0.0,
                        "cooled": int(len(cooled)),
                        "streak": int(self._well_state.get("streak", 0)),
                        "event": "curvature.hotspots"
                    })
            except Exception:
                pass
            return top

        except Exception as e:
            try:
                self.mind.console.log(f"[GravityWells] Error (PATCH B): {e}")
            except Exception:
                pass
            return []
    # --- END PATCH A ---

    def update_spacetime_curvature(self, em_stress_tensor: np.ndarray) -> None:
        if em_stress_tensor.shape != (4, 4):
            return

        # Backtracking metric update
        g_old = self.metric_tensor[:4, :4]
        delta_g = self.gravitational_coupling * em_stress_tensor

        g_new, alpha_used, bt = _apply_metric_update_with_backtracking(g_old, delta_g)

        if hasattr(self, "mind") and hasattr(self.mind, "console") and E8_LOG_FIELDMANTLE_METRIC:
            self.mind.console.log(
                "[FieldMantle][Metric] backtracks={} accepted={} alpha={:.3f} cond≈{:.2e} scale={:.3f}".format(
                    bt["tries"], bt["accepted"], alpha_used, bt["cond"], bt["scale"]
                )
            )

        self._metric_backtracks += bt["tries"]

        if not bt["accepted"]:
            # Update rejected, keep old metric
            return

        self.metric_tensor[:4, :4] = g_new
        # This is an approximation, as H is not directly g.
        # But we need to update it to reflect the change in the metric.
        self.metric_perturbation_global = g_new - self.base_metric
        self._update_christoffel_symbols(g_new)

    def _update_christoffel_symbols(self, metric: np.ndarray) -> None:
        # Hygiene before inversion
        metric = _symmetrize(metric)
        metric, had_nonfinite = _finite_guard(metric)

        g_inv, inv_diag = _safe_metric_inverse(metric)

        if hasattr(self, "mind") and hasattr(self.mind, "console") and E8_LOG_FIELDMANTLE_INV:
            self.mind.console.log(
                "[FieldMantle][Inv] driver={} ridge={:.2e} scale={:.3f} cond≈{} nonfinite={}".format(
                    inv_diag.get("driver","?"), inv_diag.get("ridge",0.0),
                    inv_diag.get("scale",1.0),
                    f"{inv_diag.get('cond',np.inf):.2e}" if np.isfinite(inv_diag.get("cond",np.inf)) else "inf",
                    had_nonfinite
                )
            )
        
        self._inv_uses[inv_diag["driver"]] = self._inv_uses.get(inv_diag["driver"], 0) + 1

        inverse_metric = g_inv
        keys = list(self._neighbor_map.keys())[:1]
        metric_gradient = np.zeros((4, 4, 4), dtype=float)
        for key in keys:
            neighbors = self._neighbor_map.get(key, [])
            pos = np.asarray(key[:4], dtype=float)
            for neighbor in neighbors:
                delta = np.asarray(neighbor[:4], dtype=float) - pos
                dist2 = float(np.dot(delta, delta)) + 1e-9
                # Build a (4,4,4) tensor where metric_gradient[j,k,l] accumulates
                # contributions proportional to delta[l] * (metric[j,k] - base_metric[j,k]).
                # Use explicit broadcasting instead of einsum for better reliability.
                diff = (metric - self.base_metric)
                metric_gradient += (diff[:, :, np.newaxis] * delta[np.newaxis, np.newaxis, :]) / dist2
        for i in range(4):
            for j in range(4):
                for k in range(4):
                    value = 0.0
                    for l in range(4):
                        value += 0.5 * inverse_metric[i, l] * (metric_gradient[j, k, l] + metric_gradient[k, j, l] - metric_gradient[l, j, k])
                    self.christoffel_symbols[(i, j, k)] = float(value)

    def transport_memories_along_geodesics(self, memories: Iterable[Any], dt: float = 0.1) -> list[Any]:
        transported = []
        dt = float(max(dt, 1e-5))
        for memory in memories:
            if not hasattr(memory, 'position'):
                continue
            try:
                pos = np.asarray(memory.position, dtype=float)
            except Exception:
                continue
            if pos.size < 4:
                pos = np.pad(pos, (0, 4 - pos.size))
            else:
                pos = pos[:4]
            vel = np.zeros(4, dtype=float)
            if hasattr(memory, 'velocity') and memory.velocity is not None:
                vel = np.asarray(memory.velocity, dtype=float)
                if vel.size < 4:
                    vel = np.pad(vel, (0, 4 - vel.size))
                else:
                    vel = vel[:4]
            accel = self.compute_geodesic_acceleration(pos, vel)
            new_velocity = vel + accel * dt
            new_position = pos + new_velocity * dt
            padded_position = np.zeros(8, dtype=float)
            padded_position[:new_position.size] = new_position
            padded_velocity = np.zeros(8, dtype=float)
            padded_velocity[:new_velocity.size] = new_velocity
            memory.position = padded_position
            memory.velocity = padded_velocity
            try:
                memory.effective_dimension = float(self.core_dim)
            except Exception:
                try:
                    setattr(memory, 'effective_dimension', float(self.core_dim))
                except Exception:
                    pass
            transported.append(memory)
        return transported

    def compute_geodesic_acceleration(self, position: np.ndarray, velocity: np.ndarray) -> np.ndarray:
        accel = np.zeros(4, dtype=float)
        for i in range(4):
            total = 0.0
            for j in range(4):
                for k in range(4):
                    gamma = self.christoffel_symbols.get((i, j, k), 0.0)
                    total -= gamma * velocity[j] * velocity[k]
            accel[i] = total
        return accel

    def compute_pressure_proxy(self) -> float:
        if not self.stress_energy:
            return 0.0
        anisotropy = 0.0
        for tensor in self.stress_energy.values():
            spatial = tensor[1:4, 1:4]
            mean = np.trace(spatial) / 3.0
            anisotropy = max(anisotropy, float(np.linalg.norm(spatial - mean * np.eye(3))))
        max_energy = max(self.energy_density_field.values()) if self.energy_density_field else 0.0
        return float(max_energy * (1.0 + anisotropy))

    def get_shell_energy_summary(self) -> dict[str, float]:
        summary = dict(self._field_summary)
        summary['num_sites'] = float(len(self.energy_density_field))
        return summary

    def get_shell_field_state(self, dim: int) -> Optional[Dict[str, Any]]:
        """
        Get the field state for a specific dimensional shell.
        
        Args:
            dim: The shell dimension
            
        Returns:
            Dictionary with E field, B field, rotor sources, and energy density
        """
        try:
            # For now, return default field state since shell-specific E/B fields may not be initialized
            # In the future, this could be populated from E_shell and B_shell if they exist
            
            # Default E and B fields (zero vectors of the appropriate dimension)
            E_field = np.zeros(dim, dtype=float)
            B_field = np.zeros(dim, dtype=float)
            
            # Try to get shell-specific fields if they exist
            if hasattr(self, 'E_shell') and self.E_shell and dim in self.E_shell:
                E_shell_data = self.E_shell[dim]
                if E_shell_data is not None:
                    E_arr = np.asarray(E_shell_data, dtype=float)
                    if E_arr.size >= dim:
                        E_field = E_arr[:dim]
                    elif E_arr.size > 0:
                        E_field = np.pad(E_arr, (0, dim - E_arr.size), mode='constant')
            
            if hasattr(self, 'B_shell') and self.B_shell and dim in self.B_shell:
                B_shell_data = self.B_shell[dim]
                if B_shell_data is not None:
                    B_arr = np.asarray(B_shell_data, dtype=float)
                    if B_arr.size >= dim:
                        B_field = B_arr[:dim]
                    elif B_arr.size > 0:
                        B_field = np.pad(B_arr, (0, dim - B_arr.size), mode='constant')
            
            # Get energy density for this shell
            energy_density = self.shell_energy.get(dim, 0.0)
            
            # Rotor sources (empty for now - could be populated from rotor data)
            rotor_sources = {}
            
            return {
                'E': E_field,
                'B': B_field,
                'rotor_sources': rotor_sources,
                'energy_density': energy_density
            }
            
        except Exception as e:
            if self.mind and hasattr(self.mind, 'console'):
                self.mind.console.log(f"[FieldMantle] Error getting shell field state for dim {dim}: {e}")
            return None

    def compute_maxwell_evolution_factor(self, dim: int, distance: float) -> float:
        """
        Compute Maxwell field evolution factor for proximity calculations.
        
        Args:
            dim: Shell dimension
            distance: Proximity distance
            
        Returns:
            Evolution factor based on field dynamics
        """
        try:
            # Base factor from shell energy
            shell_energy = self.shell_energy.get(dim, 0.0)
            energy_factor = 1.0 + np.log(1.0 + shell_energy)
            
            # Distance attenuation
            distance_factor = 1.0 / (1.0 + distance)
            
            # Field flux influence
            shell_flux = self.shell_poynting.get(dim, 0.0)
            flux_factor = 1.0 + 0.1 * np.log(1.0 + shell_flux)
            
            return energy_factor * distance_factor * flux_factor
            
        except Exception:
            return 1.0

    def compute_rotor_field_sources(self, rotor_data: Dict[str, Any]) -> Dict[str, np.ndarray]:
        """Compute electromagnetic sources from Clifford rotor fields.

        Args:
            rotor_data: Dictionary containing rotor field information

        Returns:
            Dictionary with charge and current density sources
        """
        try:
            sources = {
                'charge_density': {},
                'current_density': {},
                'magnetic_sources': {}
            }

            if not rotor_data or not self.lattice_points:
                return sources

            # Extract rotor field information
            rotor_field = rotor_data.get('rotor_field', {})
            rotor_velocities = rotor_data.get('rotor_velocities', {})

            for point in self.lattice_points:
                key = self._point_key(point)

                # Get local rotor and its properties
                local_rotor = rotor_field.get(key)
                local_velocity = rotor_velocities.get(key, np.zeros(8))

                if local_rotor is not None:
                    # Compute charge density from rotor scalar part
                    charge_density = self._compute_rotor_charge_density(local_rotor)
                    sources['charge_density'][key] = charge_density

                    # Compute current density from rotor bivector part
                    current_density = self._compute_rotor_current_density(local_rotor, local_velocity)
                    sources['current_density'][key] = current_density

                    # Compute magnetic sources from rotor rotation
                    magnetic_sources = self._compute_rotor_magnetic_sources(local_rotor)
                    sources['magnetic_sources'][key] = magnetic_sources

            return sources

        except Exception as e:
            if self.mind and hasattr(self.mind, 'console'):
                self.mind.console.log(f"[FieldMantle] Rotor field sources computation failed: {e}")
            return sources

    def _compute_rotor_charge_density(self, rotor: Any) -> float:
        """Compute charge density from rotor scalar component."""
        try:
            if not CLIFFORD_AVAILABLE:
                return 0.0

            # Extract scalar part of rotor (represents charge/mass density)
            if hasattr(rotor, 'scalar'):
                scalar_part = float(rotor.scalar)
            elif hasattr(rotor, '__float__'):
                scalar_part = float(rotor)
            else:
                scalar_part = 0.0

            # Normalize and scale
            charge_density = scalar_part * self.epsilon_0
            return float(charge_density)

        except Exception:
            return 0.0

    def _compute_rotor_current_density(self, rotor: Any, velocity: np.ndarray) -> np.ndarray:
        """Compute current density from rotor bivector component and velocity."""
        try:
            if not CLIFFORD_AVAILABLE:
                return np.zeros(4)

            current_density = np.zeros(4)

            # Extract bivector part (represents electromagnetic field)
            if hasattr(rotor, 'grade'):
                bivector_part = rotor.grade(2)
            else:
                bivector_part = rotor

            # Convert bivector to current density using velocity
            if velocity.size >= 4:
                # Current density J = ? v (simplified)
                velocity_4d = velocity[:4]
                speed = np.linalg.norm(velocity_4d)

                if speed > 1e-6:
                    # Direction of current flow
                    current_direction = velocity_4d / speed

                    # Magnitude from bivector norm
                    if hasattr(bivector_part, '__abs__'):
                        current_magnitude = abs(bivector_part) * self.alpha_src
                    else:
                        current_magnitude = 0.0

                    current_density[1:4] = current_magnitude * current_direction[:3]
                    current_density[0] = current_magnitude * speed  # Time component

            return current_density

        except Exception:
            return np.zeros(4)

    def _compute_rotor_magnetic_sources(self, rotor: Any) -> np.ndarray:
        """Compute magnetic sources from rotor rotation properties."""
        try:
            if not CLIFFORD_AVAILABLE:
                return np.zeros(3)

            magnetic_sources = np.zeros(3)

            # Extract rotation axis and angle from rotor
            if hasattr(rotor, 'log'):
                # Get the bivector generator
                bivector_gen = rotor.log()

                if hasattr(bivector_gen, 'grade'):
                    bivector = bivector_gen.grade(2)

                    # Convert bivector to magnetic field components
                    # B = (1/2) e_{ijk} F_{jk} for magnetic field
                    if hasattr(bivector, 'coefficients'):
                        coeffs = bivector.coefficients()
                        if len(coeffs) >= 3:
                            magnetic_sources[0] = coeffs[0] / self.mu_0  # B_x
                            magnetic_sources[1] = coeffs[1] / self.mu_0  # B_y
                            magnetic_sources[2] = coeffs[2] / self.mu_0  # B_z

            return magnetic_sources

        except Exception:
            return np.zeros(3)

    def evolve_maxwell_fields_with_rotors(self, rotor_sources: Dict[str, np.ndarray], dt: float) -> None:
        """Evolve Maxwell fields using rotor-generated sources.

        ?E/?t = ?B - 0J  (Faraday's law with current)
        ?B/?t = -?E        (Ampere's law without displacement current)

        Args:
            rotor_sources: Sources computed from rotor fields
            dt: Time step for evolution
        """
        try:
            if not self.lattice_points:
                return

            dt = float(max(dt, 1e-6))

            for point in self.lattice_points:
                key = self._point_key(point)

                # Get current field values
                current_E = np.zeros(3)  # Electric field
                current_B = np.zeros(3)  # Magnetic field

                F = self.field_tensor.get(key)
                if F is not None:
                    current_E = np.array([F[0, 1], F[0, 2], F[0, 3]])
                    current_B = np.array([F[2, 3], -F[1, 3], F[1, 2]])

                # Get rotor sources
                charge_density = rotor_sources.get('charge_density', {}).get(key, 0.0)
                current_density = rotor_sources.get('current_density', {}).get(key, np.zeros(4))
                magnetic_sources = rotor_sources.get('magnetic_sources', {}).get(key, np.zeros(3))

                # Evolve electric field (Faraday's law)
                # ?E/?t = ?B - 0J
                curl_B = self._compute_curl_B(key)
                current_term = self.mu_0 * current_density[1:4] if current_density.size >= 4 else np.zeros(3)

                dE_dt = curl_B - current_term
                new_E = current_E + dE_dt * dt

                # Evolve magnetic field (Ampere's law)
                # ?B/?t = -?E + 0e0 ?E/?t (simplified)
                curl_E = self._compute_curl_E(key)
                displacement_current = self.mu_0 * self.epsilon_0 * dE_dt

                dB_dt = -curl_E + displacement_current + magnetic_sources
                new_B = current_B + dB_dt * dt

                # Update field tensor
                new_F = np.zeros((4, 4))
                for i in range(3):
                    new_F[0, i+1] = new_E[i]      # E field
                    new_F[i+1, 0] = -new_E[i]

                new_F[1, 2] = -new_B[2]  # B field
                new_F[2, 1] = new_B[2]
                new_F[1, 3] = new_B[1]
                new_F[3, 1] = -new_B[1]
                new_F[2, 3] = -new_B[0]
                new_F[3, 2] = new_B[0]

                self.field_tensor[key] = new_F

                # Update vector potential using rotor sources
                self._update_vector_potential_with_rotors(key, charge_density, current_density, dt)

        except Exception as e:
            if self.mind and hasattr(self.mind, 'console'):
                self.mind.console.log(f"[FieldMantle] Maxwell evolution with rotors failed: {e}")

    def _compute_curl_B(self, key: tuple[float, ...]) -> np.ndarray:
        """Compute curl of magnetic field at given point."""
        try:
            neighbors = self._neighbor_map.get(key, [])
            if len(neighbors) < 3:
                return np.zeros(3)

            pos = np.asarray(key[:4])
            curl_B = np.zeros(3)

            for i, neighbor in enumerate(neighbors[:3]):
                neighbor_F = self.field_tensor.get(neighbor)
                if neighbor_F is None:
                    continue

                neighbor_B = np.array([neighbor_F[2, 3], -neighbor_F[1, 3], neighbor_F[1, 2]])
                current_F = self.field_tensor.get(key, np.zeros((4, 4)))
                current_B = np.array([current_F[2, 3], -current_F[1, 3], current_F[1, 2]])

                delta_B = neighbor_B - current_B
                delta_pos = np.asarray(neighbor[:4]) - pos

                # Compute cross product for curl
                if i == 0:  # x-direction
                    curl_B += np.array([0, -delta_B[2], delta_B[1]]) / np.linalg.norm(delta_pos)
                elif i == 1:  # y-direction
                    curl_B += np.array([delta_B[2], 0, -delta_B[0]]) / np.linalg.norm(delta_pos)
                elif i == 2:  # z-direction
                    curl_B += np.array([-delta_B[1], delta_B[0], 0]) / np.linalg.norm(delta_pos)

            return curl_B / max(len(neighbors[:3]), 1)

        except Exception:
            return np.zeros(3)

    def _compute_curl_E(self, key: tuple[float, ...]) -> np.ndarray:
        """Compute curl of electric field at given point."""
        try:
            neighbors = self._neighbor_map.get(key, [])
            if len(neighbors) < 3:
                return np.zeros(3)

            pos = np.asarray(key[:4])
            curl_E = np.zeros(3)

            for i, neighbor in enumerate(neighbors[:3]):
                neighbor_F = self.field_tensor.get(neighbor)
                if neighbor_F is None:
                    continue

                neighbor_E = np.array([neighbor_F[0, 1], neighbor_F[0, 2], neighbor_F[0, 3]])
                current_F = self.field_tensor.get(key, np.zeros((4, 4)))
                current_E = np.array([current_F[0, 1], current_F[0, 2], current_F[0, 3]])

                delta_E = neighbor_E - current_E
                delta_pos = np.asarray(neighbor[:4]) - pos

                # Compute cross product for curl
                if i == 0:  # x-direction
                    curl_E += np.array([0, -delta_E[2], delta_E[1]]) / np.linalg.norm(delta_pos)
                elif i == 1:  # y-direction
                    curl_E += np.array([delta_E[2], 0, -delta_E[0]]) / np.linalg.norm(delta_pos)
                elif i == 2:  # z-direction
                    curl_E += np.array([-delta_E[1], delta_E[0], 0]) / np.linalg.norm(delta_pos)

            return curl_E / max(len(neighbors[:3]), 1)

        except Exception:
            return np.zeros(3)

    def _update_vector_potential_with_rotors(self, key: tuple[float, ...], charge_density: float,
                                           current_density: np.ndarray, dt: float) -> None:
        """Update vector potential using rotor-generated sources."""
        try:
            current_A = self.vector_potential.get(key, np.zeros(4))

            # Update using Lorentz gauge: ?A^ = -0 J^
            # Simplified evolution: ?A/?t = -0 J + gauge terms

            gauge_term = self._compute_gauge_term(key)
            source_term = -self.mu_0 * current_density

            # Include charge density in time component
            source_term[0] -= self.mu_0 * charge_density

            dA_dt = source_term + gauge_term
            new_A = current_A + dA_dt * dt

            self.vector_potential[key] = new_A

        except Exception as e:
            if self.mind and hasattr(self.mind, 'console'):
                self.mind.console.log(f"[FieldMantle] Vector potential update failed: {e}")

    def _compute_gauge_term(self, key: tuple[float, ...]) -> np.ndarray:
        """Compute gauge-fixing term for vector potential evolution."""
        # Coulomb gauge: ?A = 0
        divergence = self._lorenz_divergence(key)
        gauge_damping = self.gauge_damping

        # Gauge term: -?(?A)
        gauge_term = -gauge_damping * divergence * np.ones(4)
        return gauge_term

    # --- PATCH A: Mantle.curvature ---
    def _vec8d(self, emb):
        """Return normalized 8D projection from an embedding or None if unavailable.
        Uses the canonical MemoryManager.project_to_dim8 when available.
        """
        try:
            mem = getattr(self.mind, 'memory', None)
            if mem and hasattr(mem, 'project_to_dim8'):
                return mem.project_to_dim8(emb)
            # Fallback to simple slice+norm
            import numpy as _np
            v = _np.asarray(emb, dtype=_np.float32).reshape(-1)
            if v.size < 8:
                return None
            v8 = v[:8]
            n = float(_np.linalg.norm(v8))
            if n < 1e-9:
                return None
            return (v8 / n).astype(_np.float32)
        except Exception:
            return None

    def find_high_curvature_regions(self, top_n=3):
        """
        Identify gravity wells via curvature proxies (mass, Laplacian, diffusion centrality, E8 alignment),
        then apply Q(t)-based compression from the decoding-axis projection, a robust z-gate, and
        a short cooldown to prevent one hotspot from dominating across steps.

        Env knobs:
          E8_CURV_W_MASS (default 0.40), E8_CURV_W_LAP (0.30), E8_CURV_W_DIFF (0.20), E8_CURV_W_E8 (0.10)
          E8_Q_TAU (0.12)      -> timescale in Q(t)
          E8_Q_ALPHA (0.65)    -> compression strength
          E8_CURV_ZMAX (2.5)   -> z-gate clamp (mu + z*std)
          E8_WELL_STREAK (3)   -> how many consecutive wins before cooldown
          E8_WELL_COOLDOWN (25)-> steps to cool a repeated winner
          E8_COOLDOWN_FACTOR (0.35) -> downweight while cooled
          E8_DECODING_AXIS_INDEX (optional int) -> fallback to a particular E8 root as decoding axis
        """
        import numpy as _np
        mind = getattr(self, "mind", None)
        if mind is None or not hasattr(mind, "memory"):
            return []

        # ---------- lazy init of well-state ----------
        if not hasattr(self, "_well_state"):
            self._well_state = {
                "last_top_id": None,
                "streak": 0,
                "cool_until": {},   # node_id -> step_until
                "born_at": {},      # node_id -> step first seen or reset on cooldown
            }
        # ---------- lazy init of curvature AGC stats ----------
        if not hasattr(self, "_curv_mu"):
            self._curv_mu = 0.0
        if not hasattr(self, "_curv_var"):
            self._curv_var = 1.0

        try:
            _mem = getattr(mind, "memory", None)
            _gdb = getattr(_mem, "graph_db", None)
            G = getattr(_gdb, "graph", None)
            if G is None or (hasattr(G, "number_of_nodes") and G.number_of_nodes() == 0):
                return []
            # weights
            w_mass = float(os.getenv("E8_CURV_W_MASS", "0.40"))
            w_lap  = float(os.getenv("E8_CURV_W_LAP",  "0.30"))
            w_diff = float(os.getenv("E8_CURV_W_DIFF", "0.20"))
            w_e8   = float(os.getenv("E8_CURV_W_E8",   "0.10"))

            # decoding axis v for s_Q = |r_Q · v|
            # prefer physics.decoding_axis_unit if present; else mean of first few roots; else first root
            _phys = getattr(mind, "physics", None)
            roots = getattr(_phys, "roots_unit", None)
            v = getattr(_phys, "decoding_axis_unit", None)
            if v is None:
                if roots is not None and len(roots) >= 8:
                    v = _np.mean(_np.asarray(roots[:8], dtype=float), axis=0)
                    v = v / (_np.linalg.norm(v) + 1e-9)
                elif roots is not None and len(roots) > 0:
                    v = _np.asarray(roots[0], dtype=float)
                    v = v / (_np.linalg.norm(v) + 1e-9)
                else:
                    v = _np.zeros(8, dtype=float); v[0] = 1.0  # ultra-safe fallback

            # knobs for compression/cooldown
            tau      = float(os.getenv("E8_Q_TAU", "0.08"))
            alpha    = float(os.getenv("E8_Q_ALPHA", "0.40"))
            zmax     = float(os.getenv("E8_CURV_ZMAX", "2.0"))
            streak_L = int(os.getenv("E8_WELL_STREAK", "3"))
            cool_S   = int(os.getenv("E8_WELL_COOLDOWN", "25"))
            cool_F   = float(os.getenv("E8_COOLDOWN_FACTOR", "0.35"))

            # Hard global caps for curvature/flux (fail-safe)
            # curv_cap = float(os.getenv("E8_CURV_GLOBAL_CAP", "6.0"))  # COMMENTED OUT - removing 6.0 curvature limiter

            # Precompute degrees and diffusion proxy
            deg = {n: max(1, G.degree(n)) for n in G.nodes}
            diff = {n: sum(1.0 / max(1, deg.get(nb, 1)) for nb in G.neighbors(n)) for n in G.nodes}

            # ---- collect component features first for robust per-component normalization ----
            node_ids, v8s, masses, laps, diffs, aligns = [], [], [], [], [], []
            for n, data in G.nodes(data=True):
                mass = float(data.get("connectivity_potential", data.get("rating", 0.5)))
                emb = mind.memory.main_vectors.get(n)
                v8 = self._vec8d(emb)
                if v8 is None:
                    continue
                nbs = list(G.neighbors(n))
                nb_mass_mean = (float(sum(float(G.nodes[m].get("connectivity_potential",
                                                              G.nodes[m].get("rating", 0.5)))
                                          for m in nbs)) / len(nbs)) if nbs else 0.0
                lap = mass - nb_mass_mean
                dc  = float(diff.get(n, 0.0))
                align = float(_np.dot(v8, v) / ((_np.linalg.norm(v8) * _np.linalg.norm(v)) + 1e-9))

                node_ids.append(n)
                v8s.append(v8)
                masses.append(mass)
                laps.append(lap)
                diffs.append(dc)
                aligns.append(align)

            if not node_ids:
                return []

            masses = _np.asarray(masses, dtype=float)
            laps   = _np.asarray(laps, dtype=float)
            diffs  = _np.asarray(diffs, dtype=float)
            aligns = _np.asarray(aligns, dtype=float)

            def _zrob(x: _np.ndarray) -> _np.ndarray:
                med = _np.median(x)
                q75, q25 = _np.percentile(x, [75, 25])
                iqr = float(q75 - q25)
                return (x - med) / (iqr + 1e-6)

            m_z = _zrob(masses)
            l_z = _zrob(laps)
            d_z = _zrob(diffs)
            a_z = _zrob(aligns)

            # ---- raw scores (normalized components) ----
            raw = []   # (node_id, v8, curv_raw)
            for i, n in enumerate(node_ids):
                curv_raw = (w_mass * float(m_z[i])) + (w_lap * float(l_z[i])) + (w_diff * float(d_z[i])) + (w_e8 * float(a_z[i]))
                raw.append((n, v8s[i], float(curv_raw)))

            if not raw:
                return []

            # ---- AGC: population-wise EMA mean/var for curvature mix ----
            curvs = _np.array([c for _, _, c in raw], dtype=_np.float32)
            beta = float(os.getenv("E8_CURV_AGC_BETA", "0.01"))
            batch_mu  = float(_np.mean(curvs))
            batch_var = float(_np.var(curvs) + 1e-9)
            self._curv_mu  = (1.0 - beta) * float(self._curv_mu)  + beta * batch_mu
            self._curv_var = (1.0 - beta) * float(self._curv_var) + beta * batch_var
            sigma = float(max(1e-6, _np.sqrt(self._curv_var)))

            # ---- Q(t) compression from paper: Q(t)=1-exp(-s_Q t), s_Q=|r_Q·v| ----
            # Use node's v8 as r_Q proxy and step_num as time proxy (scaled by tau).
            step = int(getattr(mind, "step_num", 0) or 0)
            scores = []  # (node_id, v8, curv_final)
            for n, v8, c in raw:
                # s_Q = |v8 · v|
                sQ = float(abs(_np.dot(v8, v) / ((_np.linalg.norm(v8) * _np.linalg.norm(v)) + 1e-9)))
                # Q(t) made local to node age (since birth or last cooldown)
                born_at = int(self._well_state["born_at"].get(n, step))
                if n not in self._well_state["born_at"]:
                    self._well_state["born_at"][n] = step
                    born_at = step
                age = max(1, step - born_at)
                # Q(t): 0..1, faster rise for larger sQ and with node age
                q = float(1.0 - _np.exp(-sQ * tau * age))
                # compression with denom floor
                denom = float(max(1e-6, 1.0 + alpha * q * abs(c)))
                c_comp = c / denom
                # AGC standardization and soft symmetric squashing
                z = (c_comp - self._curv_mu) / sigma
                gamma = float(os.getenv("E8_CURV_SQUASH_GAIN", "0.8"))
                c_final = float(_np.tanh(gamma * z))
                # hard global cap and finite guard
                if not _np.isfinite(c_final):
                    c_final = 0.0
                # c_final = float(_np.clip(c_final, -curv_cap, curv_cap))  # COMMENTED OUT - removing 6.0 curvature limiter
                scores.append((n, v8, c_final))

            # ---- cooldown: downweight nodes currently cooled ----
            cool_until = self._well_state["cool_until"]
            cooled = []
            now = step
            for i, (n, v8, c) in enumerate(scores):
                until = int(cool_until.get(n, -10**9))
                if now < until:
                    scores[i] = (n, v8, float(c * cool_F))
                    cooled.append(n)

            # initial rank
            scores.sort(key=lambda x: x[2], reverse=True)

            # update streak and possibly schedule cooldown for the winner
            if scores:
                top_id = scores[0][0]
                if top_id == self._well_state["last_top_id"]:
                    self._well_state["streak"] += 1
                else:
                    self._well_state["last_top_id"] = top_id
                    self._well_state["streak"] = 1

                if self._well_state["streak"] >= streak_L:
                    # schedule cooldown
                    cool_until[top_id] = now + cool_S
                    # reset node age so Q(t) adapts again after cooldown
                    self._well_state["born_at"][top_id] = now
                    # immediately apply damping this call, then re-rank
                    scores[0] = (scores[0][0], scores[0][1], float(scores[0][2] * cool_F))
                    scores.sort(key=lambda x: x[2], reverse=True)

            # return top hotspots as (vec8d, curvature_score)
            top = [(v8, curv) for (_, v8, curv) in scores[:max(1, int(top_n))]]

            # telemetry (best effort)
            try:
                mind.console.jsonl(evt="curvature.hotspots",
                                   count=len(top),
                                   top=(top[0][1] if top else 0.0),
                                   cooled=len(cooled),
                                   streak=self._well_state.get("streak", 0))
            except Exception:
                pass
            return top

        except Exception as e:
            try:
                self.mind.console.log(f"[GravityWells] Error (PATCH B): {e}")
            except Exception:
                pass
            return []
    # --- END PATCH A ---
    def enable_horizons(self, physics, shells, console=None):
        """Build H_E8 and H_shell[d], plus C_d kernels. No side effects to fields."""
        self.horizons = HorizonManager()
        # blueprint 3D positions and tetra edges come from physics blueprint
        P3 = np.asarray(getattr(physics, "blueprint_points_3d", np.zeros((0,3))), dtype=float)
        edges = list(getattr(physics, "tetra_edges", []))
        self.horizons.H_E8 = build_e8_horizon(physics, P3, edges, console=console)

        for d, sh in self.shells.items():
            self.horizons.H_shell[d] = sh.extract_shell_horizon()
            self.horizons.C_d[d] = build_cross_horizon_kernel(self.horizons.H_E8, self.horizons.H_shell[d])

        if console:
            tot_shell = sum((h.pos.shape[0] for h in self.horizons.H_shell.values()), 0)
            console.log(f"[Horizon] Enabled: H_E8={self.horizons.H_E8.pos.shape[0]} sites; H_shell total={tot_shell}.")

    def _kernel_K(self, X, Xk, sigma):
        import numpy as np
        r2 = np.sum((X - Xk)**2, axis=-1)
        return np.exp(-0.5 * r2 / (sigma*sigma + 1e-12))

    def rotor_boundary_sources(self, rotor_states, sigma=None, alpha_src=1.0, beta_src=0.5):
        """
        Build J^mu_hor on H_E8 from rotor states, then map to shells with C_d.
        rotor_states: iterable of dicts {pos3, n (axis 3d), omega, kappa}
        """
        if not getattr(self, "horizons", None) or self.horizons.H_E8 is None:
            return
        import numpy as np, scipy.sparse
        sigma = float(sigma or float(os.getenv("E8_HORIZON_SIGMA", "0.12")))
        H = self.horizons.H_E8
        X = H.pos  # (N,3)
        N = X.shape[0]
        rho = np.zeros((N,), dtype=np.float64)
        j = np.zeros((N,3), dtype=np.float64)

        for rs in (rotor_states or []):
            Xk = np.asarray(rs.get("pos3", np.zeros(3)), dtype=float)
            nk = np.asarray(rs.get("n", np.array([0,0,1.0])), dtype=float)
            nk /= (np.linalg.norm(nk)+1e-12)
            omega = float(rs.get("omega", 0.0))
            kappa = float(rs.get("kappa", 1.0))
            K = self._kernel_K(X, Xk, sigma)  # (N,)
            j += (alpha_src * omega) * nk.reshape(1,3) * K.reshape(-1,1)
            rho += (beta_src * omega * kappa) * K

        # discrete charge conservation: project to divergence-free boundary subspace
        # (lightweight Laplacian smoothing on graph of edge-nearby sites)
        # NOTE: we keep this minimal to avoid new deps/heavy solvers.
        rho -= rho.mean()
        self.boundary_J = {"rho": rho, "j": j}

        # map to shells with C_d
        self.boundary_J_shell = {}
        for d, C in self.horizons.C_d.items():
            if C.shape[0] == 0:
                continue
            rho_d = (C @ rho.reshape(-1,1)).A[:,0]
            jx = (C @ j[:,0].reshape(-1,1)).A[:,0]
            jy = (C @ j[:,1].reshape(-1,1)).A[:,0]
            jz = (C @ j[:,2].reshape(-1,1)).A[:,0]
            self.boundary_J_shell[d] = {"rho": rho_d, "j": np.stack([jx, jy, jz], axis=1)}

    def apply_boundary_conditions(self, bc_mode=None, dt=0.01):
        """
        Mixed (Robin) or absorbing or conductive BCs at H_E8 and each H_shell[d].
        Only touches boundary nodes' field values; solver/bulk unchanged.
        """
        bc_mode = (bc_mode or os.getenv("E8_HORIZON_BC", "absorbing")).lower()
        c0 = float(os.getenv("E8_RADIATIVE_C", "1.0"))

        # E8 blueprint boundary
        if getattr(self, "E", None) is not None and self.horizons.H_E8 is not None:
            # tangential drive from boundary current j
            J = getattr(self, "boundary_J", None)
            if J:
                # inject tangential E parallel to horizon using j
                n = self.horizons.H_E8.normals
                Et = self._project_tangential(self.E_e8, n) if hasattr(self, "E_e8") else None
                if Et is not None:
                    Et += dt * J["j"]  # simple drive
                    self._apply_tangential(self.E_e8, Et, n)

            if bc_mode == "conductive":
                # n x E = 0, n  B = 0
                self._enforce_conductive(self.horizons.H_E8, layer="e8")
            elif bc_mode == "absorbing":
                # first-order absorbing update on tangential E
                self._enforce_absorbing(self.horizons.H_E8, c0, layer="e8")
            elif bc_mode == "robin":
                # a*A_parallel + b*(n?)A_parallel = s_parallel
                self._enforce_robin(self.horizons.H_E8, layer="e8")

        # Per-shell boundaries
        for d, H in self.horizons.H_shell.items():
            Jd = self.boundary_J_shell.get(d) if hasattr(self, "boundary_J_shell") else None
            if Jd:
                n = H.normals
                Et = self._project_tangential(self.E_shell[d], n) if d in self.E_shell else None
                if Et is not None:
                    Et += dt * Jd["j"]
                    self._apply_tangential(self.E_shell[d], Et, n)
            if bc_mode == "conductive":
                self._enforce_conductive(H, layer=("shell", d))
            elif bc_mode == "absorbing":
                self._enforce_absorbing(H, layer=("shell", d))
            elif bc_mode == "robin":
                self._enforce_robin(H, layer=("shell", d))

    # --- helpers used above (minimal, numerically safe) ---
    def _project_tangential(self, E, n):
        # E: (N,3), n: (N,3) -> tangential component at boundary
        import numpy as np
        if E is None or n is None or len(E) != len(n): return None
        En = (E * n).sum(1, keepdims=True) * n
        return E - En

    def _apply_tangential(self, E, Et, n):
        if E is None or Et is None or n is None: return
        # keep normal component, update tangential
        En = (E * n).sum(1, keepdims=True) * n
        E[:] = En + Et

    def _enforce_conductive(self, H, layer):
        # n x E = 0 ? zero tangential E; nB = 0 ? remove normal B
        n = H.normals
        if layer == "e8":
            if hasattr(self, "E_e8"): self._apply_tangential(self.E_e8, 0*self.E_e8, n)
            if hasattr(self, "B_e8"): self.B_e8 -= ((self.B_e8 * n).sum(1, keepdims=True)) * n
        else:
            _, d = layer
            if d in self.E_shell: self._apply_tangential(self.E_shell[d], 0*self.E_shell[d], n)
            if d in self.B_shell: self.B_shell[d] -= ((self.B_shell[d] * n).sum(1, keepdims=True)) * n

    def _enforce_absorbing(self, H, c0, layer):
        # ?t(nE) + c n(n?E)=0  discretized as light damping of tangential E
        n = H.normals
        gamma = 0.1 * c0
        if layer == "e8" and hasattr(self, "E_e8"):
            Et = self._project_tangential(self.E_e8, n)
            self._apply_tangential(self.E_e8, (1.0 - gamma) * Et, n)
        elif layer != "e8":
            _, d = layer
            if d in self.E_shell:
                Et = self._project_tangential(self.E_shell[d], n)
                self._apply_tangential(self.E_shell[d], (1.0 - gamma) * Et, n)

    def _enforce_robin(self, H, layer, a=1.0, b=0.1, s_parallel=None):
        # simple Robin: a*E_parallel + b*(E_parallel - E_parallel_prev)/dt  s
        n = H.normals
        if layer == "e8" and hasattr(self, "E_e8"):
            Et = self._project_tangential(self.E_e8, n)
            s = 0.0 if s_parallel is None else s_parallel
            self._apply_tangential(self.E_e8, (s - a*Et)/(b + 1e-6), n)
        elif layer != "e8":
            _, d = layer
            if d in self.E_shell:
                Et = self._project_tangential(self.E_shell[d], n)
                s = 0.0 if s_parallel is None else s_parallel
                self._apply_tangential(self.E_shell[d], (s - a*Et)/(b + 1e-6), n)

    def horizon_stress_energy(self):
        """
        Compute EM T^{?} on horizons from near-field values.
        Returns dict with per-layer energy density T00.
        """
        import numpy as np
        out = {}
        mu0 = 1.0
        if self.horizons and self.horizons.H_E8 is not None and hasattr(self, "E_e8") and hasattr(self, "B_e8"):
            Ee = self.E_e8; Be = self.B_e8
            T00 = 0.5/mu0 * (np.sum(Ee**2, axis=1) + np.sum(Be**2, axis=1))
            out["H_E8_T00"] = float(np.mean(T00)) if T00.size else 0.0
        for d, H in (self.horizons.H_shell.items() if self.horizons else []):
            if d in self.E_shell and d in self.B_shell:
                Es, Bs = self.E_shell[d], self.B_shell[d]
                T00 = 0.5/mu0 * (np.sum(Es**2, axis=1) + np.sum(Bs**2, axis=1))
                out[f"H_shell[{d}]_T00"] = float(np.mean(T00)) if T00.size else 0.0
        return out
    # --- [END ADD] ---

    def step(self, dt: float, rotor_states: dict):
        """
        Step the field mantle with horizon-aware boundary conditions.
        """
        if getattr(self, "horizons", None) and os.getenv("E8_USE_HORIZONS", "0") == "1":
            # 1) boundary sources from rotors on H_E8 -> map to shells
            self.rotor_boundary_sources(rotor_states)
            # 2) apply boundary conditions (Robin/absorbing/conductive)
            self.apply_boundary_conditions()
        # continue with your existing Maxwell updates...
        # after fields:
        if getattr(self, "horizons", None) and os.getenv("E8_USE_HORIZONS", "0") == "1":
            Hstress = self.horizon_stress_energy()
            self.metrics["horizon_energy"] = Hstress  # expose to summaries
            # weak-gravity boundary update via existing curvature hook
            try:
                self.update_metric_perturbations_from_fields(Hstress)
            except Exception:
                pass

class DimensionalShell:
    def __init__(self, dim: int, mind_instance: 'E8Mind'):
        self.dim = dim
        self.mind = mind_instance
        self.vectors: Dict[str, Any] = {}
        self.vector_mode = "clifford"
        self.rotor_generator = None
        self.orientation = 1

        if CLIFFORD_AVAILABLE:
            try:
                self.layout, self.blades = clifford.Cl(dim)
                self.basis_vectors = [self.blades.get(f"e{i+1}") for i in range(dim)]
                self.rotor_generator = CliffordRotorGenerator(mind_instance, self.layout, self.blades)
                self.orientation = getattr(self.layout, "scalar", 1)
            except Exception:
                # degrade to numpy
                self.layout, self.blades = None, {}
                self.basis_vectors = [np.eye(dim, dtype=float)[i] for i in range(dim)]
                self.vector_mode = "numpy"
        else:
            self.layout, self.blades = None, {}
            self.basis_vectors = [np.eye(dim, dtype=float)[i] for i in range(dim)]
            self.vector_mode = "numpy"

        # Optional bivector basis (only meaningful in Clifford mode)
        try:
            if CLIFFORD_AVAILABLE and self.vector_mode == "clifford":
                self._build_bivector_basis()
                # Validate rotor generator on boot to ensure invariants hold
                try:
                    if getattr(self, 'rotor_generator', None) is not None:
                        # generate a small test rotor and validate
                        test_rotor = self.rotor_generator.generate_rotor(self, angle=0.1)
                        val = self.rotor_generator.validate_rotor_properties(test_rotor)
                        try:
                            self.mind.console.log(f"[ROTOR] Boot validation: {val}")
                        except Exception:
                            pass
                except Exception:
                    pass
            else:
                self.bivector_basis = []
        except Exception:
            self.bivector_basis = []

    def _ensure_numpy_mode(self):
        if self.vector_mode != "numpy":
            self.vector_mode = "numpy"
            # Convert any existing stored Clifford multivectors to numpy coefficients if possible
            new_vectors = {}
            for nid, mv in self.vectors.items():
                try:
                    coeffs = [float(mv[bv]) for bv in self.basis_vectors]
                    new_vectors[nid] = np.array(coeffs, dtype=np.float32)
                except Exception:
                    pass
            if new_vectors:
                self.vectors.update(new_vectors)

    def add_vector(self, node_id: str, vector: np.ndarray):
        if vector is None:
            return
        if hasattr(vector, "shape") and vector.shape[0] != self.dim:
            padded_vector = np.zeros(self.dim, dtype=np.float32)
            size_to_copy = min(vector.shape[0], self.dim)
            padded_vector[:size_to_copy] = vector[:size_to_copy]
            vector = padded_vector

        snapped_vector = self.mind._snap_to_lattice(vector, self.dim)

        if self.vector_mode == "clifford":
            try:
                mv = 0
                for val, bv in zip(snapped_vector, self.basis_vectors):
                    mv = mv + float(val) * bv
                self.vectors[node_id] = mv
                return
            except Exception:
                # degrade safely
                self._ensure_numpy_mode()

        # numpy mode
        self.vectors[node_id] = np.asarray(snapped_vector, dtype=np.float32)

    def get_vector(self, node_id: str) -> Optional[np.ndarray]:
        v = self.vectors.get(node_id)
        if v is None:
            return None
        if self.vector_mode == "clifford":
            try:
                return np.array([float(v[bv]) for bv in self.basis_vectors], dtype=np.float32)
            except Exception:
                self._ensure_numpy_mode()
        # numpy mode
        return np.asarray(v, dtype=np.float32)

    def get_all_vectors_as_matrix(self) -> tuple[Optional[np.ndarray], list[str]]:
        if not self.vectors:
            # Test suite expects (None, None) when empty
            return None, None  # type: ignore
        node_ids = list(self.vectors.keys())
        if self.vector_mode == "clifford":
            try:
                matrix_list = [[float(self.vectors[nid][bv]) for bv in self.basis_vectors] for nid in node_ids]
                return np.array(matrix_list, dtype=np.float32), node_ids
            except Exception:
                self._ensure_numpy_mode()
        # numpy mode
        matrix = np.stack([np.asarray(self.vectors[nid], dtype=np.float32) for nid in node_ids], axis=0)
        return matrix, node_ids

    def _build_bivector_basis(self):
        if not CLIFFORD_AVAILABLE or self.vector_mode != "clifford":
            self.bivector_basis = []
            return
        try:
            self.bivector_basis = []
            for i in range(self.dim):
                for j in range(i+1, self.dim):
                    self.bivector_basis.append(self.basis_vectors[i] ^ self.basis_vectors[j])
        except Exception:
            self.bivector_basis = []

    def spin_with_bivector(self, bivector_coeffs, angle):
        # No-op in numpy mode or when no vectors
        if self.vector_mode != "clifford" or not CLIFFORD_AVAILABLE or not self.vectors:
            return
        try:
            if not hasattr(self, "bivector_basis") or not self.bivector_basis:
                self._build_bivector_basis()
            B = 0
            k = min(len(self.bivector_basis), len(bivector_coeffs))
            for idx in range(k):
                try:
                    B = B + float(bivector_coeffs[idx]) * self.bivector_basis[idx]
                except Exception:
                    pass
            Bn = B.normal() if hasattr(B, "normal") else None
            if Bn is None or not hasattr(self.layout, "multi_vector"):
                return
            R = np.cos(angle/2.0) - np.sin(angle/2.0) * Bn
            for nid in list(self.vectors.keys()):
                try:
                    mv = self.vectors[nid]
                    self.vectors[nid] = R * mv * (~R) if hasattr(R, "__invert__") else mv
                except Exception:
                    pass
        except Exception:
            # degrade rather than crash
            self._ensure_numpy_mode()

    def compute_brown_york_stress_tensor(self, boundary_geometry: np.ndarray, extrinsic_curvature: np.ndarray) -> np.ndarray:
        """Compute Brown-York boundary stress tensor for holographic stress-energy.

        t_ij = (1/(8pG)) * (K_ij - K ?_ij)

        Args:
            boundary_geometry: ?_ij boundary metric tensor
            extrinsic_curvature: K_ij extrinsic curvature tensor

        Returns:
            t_ij: Brown-York stress tensor
        """
        try:
            # Constants
            G = 6.67430e-11  # Gravitational constant (m^3 kg^-1 s^-2)
            factor = 1.0 / (8.0 * np.pi * G)

            # Ensure tensors have compatible shapes
            if boundary_geometry.shape != extrinsic_curvature.shape:
                self.console.log(f"[yellow]Tensor shape mismatch: ?_ij {boundary_geometry.shape} vs K_ij {extrinsic_curvature.shape}[/yellow]")
                return np.zeros_like(boundary_geometry)

            # Compute trace of extrinsic curvature
            K = np.trace(extrinsic_curvature)  # K = ?^ij K_ij

            # Brown-York stress tensor: t_ij = (1/(8pG)) * (K_ij - K ?_ij)
            stress_tensor = factor * (extrinsic_curvature - K * boundary_geometry)

            return stress_tensor

        except Exception as e:
            self.console.log(f"[yellow]Brown-York stress tensor computation failed: {e}[/yellow]")
            return np.zeros_like(boundary_geometry)

    def compute_holographic_boundary_stress(self, boundary_points: List[np.ndarray], bulk_geometry: Dict[str, Any]) -> Dict[str, np.ndarray]:
        """Compute holographic boundary stress from bulk geometry using E8 structure.

        Args:
            boundary_points: Points on the holographic boundary
            bulk_geometry: Dictionary containing bulk spacetime geometry information

        Returns:
            Dictionary with stress tensor components and derived quantities
        """
        try:
            results = {
                'stress_tensor': None,
                'energy_density': 0.0,
                'pressure': 0.0,
                'stress_components': {},
                'holographic_entropy': 0.0
            }

            if not boundary_points:
                return results

            # Extract boundary geometry from E8 lattice structure
            boundary_metric = self._extract_boundary_metric(boundary_points)
            extrinsic_curvature = self._compute_extrinsic_curvature(boundary_points, bulk_geometry)

            # Compute Brown-York stress tensor
            stress_tensor = self.compute_brown_york_stress_tensor(boundary_metric, extrinsic_curvature)
            results['stress_tensor'] = stress_tensor

            # Extract energy density (T^00 component)
            if stress_tensor.size >= 1:
                results['energy_density'] = float(stress_tensor[0, 0])

            # Compute isotropic pressure (trace of spatial stress tensor)
            if stress_tensor.shape[0] > 1:
                spatial_trace = np.trace(stress_tensor[1:, 1:])
                results['pressure'] = float(spatial_trace / (stress_tensor.shape[0] - 1))

            # Store individual stress components
            results['stress_components'] = {
                'T_00': float(stress_tensor[0, 0]) if stress_tensor.size >= 1 else 0.0,
                'T_11': float(stress_tensor[1, 1]) if stress_tensor.shape[0] > 1 else 0.0,
                'T_22': float(stress_tensor[2, 2]) if stress_tensor.shape[0] > 2 else 0.0,
                'trace': float(np.trace(stress_tensor))
            }

            # Compute holographic entropy using Bekenstein-Hawking formula
            results['holographic_entropy'] = self._compute_holographic_entropy(stress_tensor, boundary_points)

            return results

        except Exception as e:
            self.console.log(f"[yellow]Holographic boundary stress computation failed: {e}[/yellow]")
            return results

    def _extract_boundary_metric(self, boundary_points: List[np.ndarray]) -> np.ndarray:
        """Extract boundary metric tensor from E8 lattice structure."""
        try:
            if len(boundary_points) < 2:
                return np.eye(4)  # Minkowski metric as fallback

            # Use E8 roots to define coordinate system
            if hasattr(self.mind.physics, 'roots_unit') and self.mind.physics.roots_unit is not None:
                e8_roots = self.mind.physics.roots_unit

                # Select 4 roots to define 4D boundary coordinates
                basis_roots = e8_roots[:4] if len(e8_roots) >= 4 else e8_roots[:len(e8_roots)]

                # Compute metric from dot products of basis vectors
                n_dim = len(basis_roots)
                metric = np.zeros((n_dim, n_dim))

                for i in range(n_dim):
                    for j in range(n_dim):
                        metric[i, j] = np.dot(basis_roots[i], basis_roots[j])

                return metric
            else:
                # Fallback to flat metric
                return np.eye(4)

        except Exception as e:
            self.console.log(f"[yellow]Boundary metric extraction failed: {e}[/yellow]")
            return np.eye(4)

    def _compute_extrinsic_curvature(self, boundary_points: List[np.ndarray], bulk_geometry: Dict[str, Any]) -> np.ndarray:
        """Compute extrinsic curvature tensor for the boundary."""
        try:
            n_points = len(boundary_points)
            if n_points < 2:
                return np.zeros((4, 4))

            # Estimate extrinsic curvature from boundary point distribution
            # This is a simplified approximation - full computation would require bulk metric

            # Compute mean curvature approximation
            if n_points >= 3:
                # Use local curvature estimation
                curvatures = []
                for i, point in enumerate(boundary_points):
                    # Find nearest neighbors
                    distances = []
                    for j, other_point in enumerate(boundary_points):
                        if i != j:
                            dist = np.linalg.norm(point - other_point)
                            distances.append((dist, j))

                    # Sort by distance
                    distances.sort(key=lambda x: x[0])

                    # Estimate local curvature from nearest neighbors
                    if len(distances) >= 2:
                        r1 = distances[0][0]
                        r2 = distances[1][0]
                        # Simple curvature estimate: 1/r for spherical approximation
                        local_curvature = 2.0 / (r1 + r2) if (r1 + r2) > 1e-6 else 0.0
                        curvatures.append(local_curvature)

                mean_curvature = np.mean(curvatures) if curvatures else 0.0
            else:
                mean_curvature = 0.0

            # Construct extrinsic curvature tensor (simplified diagonal form)
            n_dim = 4  # 4D boundary
            K_ij = np.eye(n_dim) * mean_curvature

            return K_ij

        except Exception as e:
            self.console.log(f"[yellow]Extrinsic curvature computation failed: {e}[/yellow]")
            return np.zeros((4, 4))

    # --- [ADD INSIDE DimensionalShell] ---
    def extract_shell_horizon(self, degree_cap=6, voronoi_quantile=0.25):
        """
        Identify boundary subset by low-degree & low-Voronoi-area cues.
        Stores indices and lightweight outward normals (from density gradient).
        """
        import numpy as np
        H = HorizonLayer(f"H_shell[{self.dim}]")
        # indices of nodes considered "boundary"
        boundary_idx = []
        areas = []
        for nid, data in self.shell_nodes.items():
            deg = len(data.get("neighbors", []))
            areas.append(data.get("voronoi_area", 1.0))
            if deg <= degree_cap:
                boundary_idx.append(nid)

        # numeric gradient for normals in embedding coords
        pos = []
        normals = []
        area_arr = np.asarray(areas, dtype=float) if areas else None
        for nid in boundary_idx:
            d = self.shell_nodes[nid]
            p = np.asarray(d.get("pos"), dtype=float)
            neigh = d.get("neighbors", [])
            if not neigh:
                continue
            N = np.stack([np.asarray(self.shell_nodes[j]["pos"], dtype=float) for j in neigh], axis=0)
            center = N.mean(0)
            n = (p - center)
            n /= (np.linalg.norm(n) + 1e-12)
            pos.append(p)
            normals.append(n[:3] if n.shape[0] >= 3 else np.pad(n, (0, 3-n.shape[0])))

        H.indices = boundary_idx
        H.pos = np.stack(pos, axis=0) if pos else np.zeros((0, self.dim))
        H.normals = np.stack(normals, axis=0) if normals else np.zeros((0, 3))
        H.meta["degree_cap"] = degree_cap
        H.meta["voronoi_q"] = voronoi_quantile
        if getattr(self, "console", None):
            self.console.log(f"[Horizon] Shell d={self.dim}: {len(boundary_idx)} boundary sites.")
        return H
    # --- [END ADD] ---

    def _compute_holographic_entropy(self, stress_tensor: np.ndarray, boundary_points: List[np.ndarray]) -> float:
        """Compute holographic entropy using stress tensor and boundary area."""
        try:
            # Bekenstein-Hawking entropy: S = A/(4G) where A is horizon area
            # For holographic principle: S = (c^3 A)/(4 G h) but we'll use simplified version

            G = 6.67430e-11  # Gravitational constant

            # Estimate boundary area from point distribution
            if len(boundary_points) < 3:
                boundary_area = 1.0  # Unit area fallback
            else:
                # Simple area estimation using convex hull
                try:
                    from scipy.spatial import ConvexHull
                    hull = ConvexHull(boundary_points)
                    boundary_area = hull.area
                except Exception:
                    # Fallback: approximate as sphere
                    radii = [np.linalg.norm(point) for point in boundary_points]
                    mean_radius = np.mean(radii) if radii else 1.0
                    boundary_area = 4.0 * np.pi * mean_radius**2

            # Holographic entropy (simplified)
            entropy = boundary_area / (4.0 * G)

            # Modulate by stress tensor trace (energy content)
            trace_T = np.trace(stress_tensor)
            entropy *= (1.0 + 0.1 * abs(trace_T))  # Small modulation by energy

            return float(entropy)

        except Exception as e:
            self.console.log(f"[yellow]Holographic entropy computation failed: {e}[/yellow]")
            return 0.0

    def apply_holographic_stress_feedback(self, stress_results: Dict[str, Any], target_nodes: List[str]) -> None:
        """Apply holographic stress feedback to memory nodes."""
        try:
            if not stress_results or not target_nodes:
                return

            energy_density = stress_results.get('energy_density', 0.0)
            pressure = stress_results.get('pressure', 0.0)

            # Convert holographic stress to temperature/activation changes
            stress_factor = energy_density + pressure  # Simplified stress measure

            # Apply feedback to target nodes
            for node_id in target_nodes:
                node = self.graph_db.get_node(node_id)
                if node:
                    current_temp = node.get('temperature', 1.0)

                    # Stress feedback modulates temperature
                    # Positive stress (tension) increases temperature
                    # Negative stress (compression) decreases temperature
                    stress_feedback = 0.1 * stress_factor  # Small modulation factor

                    new_temp = current_temp * (1.0 + stress_feedback)
                    new_temp = max(0.1, min(5.0, new_temp))  # Clamp to reasonable range

                    node['temperature'] = float(new_temp)

                    # Log holographic influence
                    node['last_holographic_stress'] = {
                        'step': self.mind.step_num,
                        'energy_density': float(energy_density),
                        'pressure': float(pressure),
                        'stress_factor': float(stress_factor),
                        'temperature_change': float(new_temp - current_temp)
                    }

        except Exception as e:
            self.console.log(f"[yellow]Holographic stress feedback application failed: {e}[/yellow]")

@dataclass
class PathAsset:
    """
    Represents a ray-traced path between two nodes in the E8 manifold.

    This class encapsulates the geometric path found by solving the eikonal equation
    or integrating null-like geodesics through the curved field manifold.
    """
    # Core path geometry
    waypoints: List[str]  # Node IDs along the path
    length: float  # Total Euclidean path length L
    action: float  # Optical path action S = ? n(x) ds
    attenuation: float  # Cumulative attenuation factor A (0-1)

    # Horizon crossing information
    shell_crossings: List[Dict[str, Any]]  # [{'shell_from': int, 'shell_to': int, 'gate': str, 'refraction_factor': float}]
    horizon_gates: List[str]  # Gate identifiers used for crossings

    # Scoring and metadata
    score: float  # Final path score s = exp(-? S) * (1-A)
    computation_time: float  # Time taken to compute this path
    success: bool  # Whether path computation succeeded

    # Optional geometric data
    positions: Optional[List[np.ndarray]] = None  # 3D positions for visualization
    directions: Optional[List[np.ndarray]] = None  # Direction vectors at waypoints

    def __init__(self, waypoints: List[str], length: float, action: float, attenuation: float,
                 shell_crossings: List[Dict[str, Any]] = None, horizon_gates: List[str] = None,
                 positions: List[np.ndarray] = None, directions: List[np.ndarray] = None,
                 computation_time: float = 0.0, success: bool = True):
        self.waypoints = waypoints
        self.length = float(length)
        self.action = float(action)
        self.attenuation = float(attenuation)
        self.shell_crossings = shell_crossings or []
        self.horizon_gates = horizon_gates or []
        self.positions = positions
        self.directions = directions
        # Allow callers to set these via kwargs for compatibility with existing callsites
        self.computation_time = float(computation_time) if computation_time is not None else 0.0
        self.success = bool(success)
        self.score = self._compute_score()

    def _compute_score(self, lambda_param: float = 0.1) -> float:
        """Compute path score based on action and attenuation."""
        if not self.success:
            return 0.0
        # Score = exp(-? S) * (1-A) where ? controls action sensitivity
        action_penalty = np.exp(-lambda_param * self.action)
        attenuation_factor = 1.0 - self.attenuation
        return float(action_penalty * attenuation_factor)

    def update_score(self, lambda_param: float = 0.1):
        """Recalculate score with new lambda parameter."""
        self.score = self._compute_score(lambda_param)

    def get_summary(self) -> Dict[str, Any]:
        """Get a summary of the path asset for logging/debugging."""
        return {
            'waypoints_count': len(self.waypoints),
            'length': self.length,
            'action': self.action,
            'attenuation': self.attenuation,
            'score': self.score,
            'crossings': len(self.shell_crossings),
            'gates': self.horizon_gates,
            'computation_time_ms': self.computation_time * 1000,
            'success': self.success
        }

class GeodesicRayTracer:
    """
    Thin wrapper that centralizes geodesic ray/path computation strategies.

    It delegates to ProximityEngine's existing implementations:
    - _global_geodesic_path (graph Dijkstra/A*)
    - solve_eikonal_equation (fast marching on graph)
    - trace_geodesic_ray (physics-like integrator)
    and applies optional horizon refraction post-processing.

    Usage: tracer.trace(src, dst, dim) -> PathAsset | None
    """
    def __init__(self, engine: 'ProximityEngine'):
        self.engine = engine

    def trace(self, source_node: str, target_node: str, dim: int) -> Optional[PathAsset]:
        """Compute the best path according to configured strategy."""
        # Respect engine flags for ordering
        try:
            if getattr(self.engine, 'ray_global_first', True) and hasattr(self.engine, '_global_geodesic_path'):
                asset = self.engine._global_geodesic_path(source_node, target_node, dim)
                if asset and getattr(asset, 'success', False):
                    return self.engine.apply_horizon_refraction_to_path(asset, dim)

            # Eikonal next
            asset = self.engine.solve_eikonal_equation(source_node, target_node, dim)
            if asset and getattr(asset, 'success', False):
                return self.engine.apply_horizon_refraction_to_path(asset, dim)

            # Physics geodesic last
            asset = self.engine.trace_geodesic_ray(source_node, target_node, dim)
            if asset and getattr(asset, 'success', False):
                return self.engine.apply_horizon_refraction_to_path(asset, dim)

            return None
        except Exception:
            return None

class ProximityEngine:
    def __init__(self, shell_dims: List[int], mind_instance: 'E8Mind', console: Console):
        self.console = console
        self.shell_dims = shell_dims
        self.mind = mind_instance
        self.indices: Dict[int, Optional[KDTree]] = {dim: None for dim in shell_dims}
        self.id_maps: Dict[int, List[str]] = {dim: [] for dim in shell_dims}
        # Adaptive proximity stats (fluid mode)
        self.local_stats: Dict[int, Dict[str, float]] = {}
        self._last_stats_step: Dict[int, int] = {dim: -10**9 for dim in shell_dims}
        self.adaptive_enabled: bool = os.getenv("E8_PROXIMITY_ADAPTIVE", "1") != "0"
        # Temperature cache (for mood/temperature biased selection)
        self._temp_cache: Dict[int, Tuple[float, int]] = {}  # dim -> (avg_temp, step_cached)
        try:
            self.temp_cache_interval = int(os.getenv("E8_PROXIMITY_TEMP_CACHE_INTERVAL", "50"))
        except Exception:
            self.temp_cache_interval = 50
        try:
            self.stats_sample_size = int(os.getenv("E8_PROXIMITY_STATS_SAMPLE", "128"))
        except Exception:
            self.stats_sample_size = 128
        try:
            self.stats_k = int(os.getenv("E8_PROXIMITY_STATS_K", "8"))
        except Exception:
            self.stats_k = 8
        try:
            self.recalc_interval = int(os.getenv("E8_PROXIMITY_RECALC_INTERVAL", "200"))
            self.recalc_interval = _scale_steps(self.recalc_interval)
        except Exception:
            self.recalc_interval = 200
        # Percentile thresholds (can be overridden)
        def _env_pct(name, default):
            try:
                return float(os.getenv(name, str(default)))
            except Exception:
                return default
        self.lock_pct = _env_pct("E8_PROXIMITY_LOCK_PCTL", 10.0)
        self.strong_pct = _env_pct("E8_PROXIMITY_STRONG_PCTL", 25.0)
        self.moderate_pct = _env_pct("E8_PROXIMITY_MODERATE_PCTL", 50.0)

        # Ray-Based Proximity state variables
        self.use_field_rays: bool = os.getenv("E8_RAY_BASED_PROXIMITY", "0") == "1"
        # Per-dimension probabilistic Hawking caches for ray results
        default_budget = int(os.getenv("E8_RAY_CACHE_SIZE", "1000"))
        default_half = float(os.getenv("E8_RAY_CACHE_HALF_LIFE", "1800"))
        self.ray_cache: Dict[int, HawkingCache] = {dim: HawkingCache(budget=default_budget, half_life_s=default_half) for dim in shell_dims}
        self.ray_stats: Dict[str, int] = {'success': 0, 'fail': 0, 'latency_total': 0, 'cache_hits': 0}
        self.ray_cache_max_size: int = int(os.getenv("E8_RAY_CACHE_SIZE", "1000"))
        self.ray_success_threshold: float = float(os.getenv("E8_RAY_SUCCESS_THRESHOLD", "0.7"))
        self.ray_monitor_only: bool = os.getenv("E8_RAY_MONITOR_ONLY", "1") == "1"  # Start in monitor mode
        # New RAY flags for global geodesic tracer and attention cone
        try:
            self.ray_beta: float = float(os.getenv("E8_RAY_BETA", "0.25"))
        except Exception:
            self.ray_beta = 0.25
        self.ray_use_astar: bool = os.getenv("E8_RAY_USE_ASTAR", "1") == "1"
        self.ray_global_first: bool = os.getenv("E8_RAY_GLOBAL_FIRST", "1") == "1"
        try:
            self.cone_enabled: bool = os.getenv("E8_RAY_CONE_ON", "1") == "1"
            self.c_eff: float = float(os.getenv("E8_RAY_C_EFF", "1.0"))
            self.cone_Lmax_env: float = float(os.getenv("E8_RAY_CONE_LMAX", "0.0"))
        except Exception:
            self.cone_enabled, self.c_eff, self.cone_Lmax_env = True, 1.0, 0.0
        try:
            self.kappa_hit_threshold: float = float(os.getenv("E8_RAY_KAPPA_HIT", "1.0"))
        except Exception:
            self.kappa_hit_threshold = 1.0

        # Geodesic ray tracer wrapper (initialized lazily)
        self._tracer: Optional['GeodesicRayTracer'] = None
        
        # Enhanced EMA-based alert distance system
        self.alert_distance_ema = 0.05  # Initial alert distance
        self.alert_ema_alpha = float(os.getenv("E8_PROXIMITY_EMA_ALPHA", "0.1"))  # EMA smoothing factor
        self.alert_min_distance = float(os.getenv("E8_PROXIMITY_MIN_ALERT", "0.01"))  # Minimum alert threshold
        self.alert_max_distance = float(os.getenv("E8_PROXIMITY_MAX_ALERT", "0.5"))   # Maximum alert threshold
        self.alert_multiplier = float(os.getenv("E8_PROXIMITY_ALERT_MULT", "1.5"))   # Multiplier for EMA
        self.alert_change_log = []  # Log of threshold changes for analysis

    def _get_tracer(self) -> 'GeodesicRayTracer':
        """Lazy-create a GeodesicRayTracer bound to this engine."""
        if getattr(self, '_tracer', None) is None:
            self._tracer = GeodesicRayTracer(self)
        return self._tracer

    # --- Light-cone helpers -------------------------------------------------
    def _get_cone_Lmax(self, step_cadence: float | None = None) -> float:
        """Return the effective geodesic radius limit for attention light-cone.

        If E8_RAY_CONE_LMAX is set (>0), use it. Otherwise, approximate with
        c_eff * cadence, where cadence defaults to 1.0 step if not provided.
        """
        if self.cone_Lmax_env and self.cone_Lmax_env > 0:
            return float(self.cone_Lmax_env)
        cadence = 1.0 if step_cadence is None else float(step_cadence)
        try:
            return float(self.c_eff) * cadence
        except Exception:
            return cadence

    def _adaptive_cone_limit(self, dim: int) -> float:
        """Adaptive L_max tied to wave c_eff and current field energy.

        L_max = base from env or c_eff; modulated by log(1+energy) with clamps.
        """
        try:
            # Prefer Everywhen wave c_eff if available, else fallback to ray c_eff logic
            try:
                if hasattr(self.mind, 'everywhen_wave') and self.mind.everywhen_wave:
                    base = float(self.mind.everywhen_wave.c_eff)
                else:
                    base = float(self._get_cone_Lmax())
            except Exception:
                base = float(self._get_cone_Lmax())
            # Query current field energy if mantle present, else 0
            energy = 0.0
            try:
                if hasattr(self.mind, 'field_mantle') and self.mind.field_mantle:
                    state = self.mind.field_mantle.get_shell_field_state(dim) or {}
                    energy = float(state.get('energy_density', 0.0))
            except Exception:
                energy = 0.0
            # Modulate softly: 1 + log(1+E)
            factor = 1.0 + (np.log(1.0 + max(0.0, energy)) if energy > 0 else 0.0)
            # Clamp factor to avoid runaway
            factor = float(min(max(factor, 0.5), 3.0))
            L_max = float(base) * factor
            # Optional global ceiling from env
            try:
                hard_cap = float(os.getenv('E8_RAY_CONE_HARD_CAP', '0.0'))
                if hard_cap > 0:
                    L_max = min(L_max, hard_cap)
            except Exception:
                pass
            return max(0.0, L_max)
        except Exception:
            return self._get_cone_Lmax()

    def is_within_light_cone(self, origin_id: str, candidate_id: str, dim: int, L_max: float | None = None) -> bool:
        """Check if candidate is inside attention light-cone from origin.

        Uses curvature-weighted global geodesic path length as metric distance.
        Falls back to Euclidean if path tracing fails.
        """
        if not self.cone_enabled:
            return True
        if not origin_id or not candidate_id or origin_id == candidate_id:
            return True
        L_threshold = self._get_cone_Lmax() if L_max is None else float(L_max)
        try:
            # Prefer centralized tracer (which may choose global/eikonal/physics)
            tracer = self._get_tracer()
            path = tracer.trace(origin_id, candidate_id, dim)
            if path and getattr(path, 'success', False):
                return float(getattr(path, 'length', float('inf'))) <= L_threshold
        except Exception:
            pass
        # Fallback: Euclidean estimate
        try:
            shell = self.mind.memory.shells.get(dim)
            if shell and hasattr(shell, 'vectors'):
                a = shell.vectors.get(origin_id)
                b = shell.vectors.get(candidate_id)
                if a is not None and b is not None:
                    return float(np.linalg.norm(a - b)) <= L_threshold
        except Exception:
            pass
        return False

    def update_alert_distance_ema(self, observed_distance: float, component: str = "system", reason: str = "distance_update"):
        """Update alert distance using EMA and log the change."""
        old_distance = self.alert_distance_ema
        
        # EMA update: new_value = alpha * observed + (1 - alpha) * old_value
        new_distance = self.alert_ema_alpha * observed_distance + (1 - self.alert_ema_alpha) * old_distance
        
        # Apply bounds
        new_distance = max(self.alert_min_distance, min(self.alert_max_distance, new_distance))
        
        # Update if significant change
        if abs(new_distance - old_distance) > 1e-6:
            self.alert_distance_ema = new_distance
            
            # Structured logging for threshold changes
            change_record = {
                "timestamp": time.time(),
                "step": getattr(self.mind, 'step_num', 0),
                "component": component,
                "reason": reason,
                "old_distance": old_distance,
                "new_distance": new_distance,
                "observed_distance": observed_distance,
                "ema_alpha": self.alert_ema_alpha
            }
            self.alert_change_log.append(change_record)
            
            # Keep log bounded
            if len(self.alert_change_log) > 100:
                self.alert_change_log.pop(0)
            
            # Log the change
            self.console.log(f"[PROXIMITY] Alert threshold: {component} {old_distance:.4f} → {new_distance:.4f} (observed: {observed_distance:.4f}, reason: {reason})")
            
            # Emit metrics if available
            try:
                if hasattr(self.mind, 'metrics'):
                    self.mind.metrics.gauge('proximity.alert_distance', new_distance)
                    self.mind.metrics.counter('proximity.threshold_changes', tags={'component': component, 'reason': reason})
            except Exception:
                pass

    def get_dynamic_alert_threshold(self) -> float:
        """Get current dynamic alert threshold based on EMA."""
        return max(self.alert_min_distance, self.alert_distance_ema * self.alert_multiplier)

    def filter_by_light_cone(self, origin_id: str, candidate_ids: list[str], dim: int, L_max: float | None = None) -> list[str]:
        """Filter candidates to those within geodesic radius from origin."""
        if (not self.cone_enabled) or not candidate_ids:
            return candidate_ids
        try:
            # Use adaptive limit unless explicit L_max provided
            L_eff = self._adaptive_cone_limit(dim) if L_max is None else float(L_max)
            # brief telemetry
            try:
                self.console.log(f"[Cone] dim={dim} L_max={L_eff:.3f} c_eff={getattr(self.mind, 'everywhen_wave').c_eff if hasattr(self.mind, 'everywhen_wave') and self.mind.everywhen_wave else self.c_eff:.3f}")
            except Exception:
                pass
            return [cid for cid in candidate_ids if self.is_within_light_cone(origin_id, cid, dim, L_eff)]
        except Exception:
            return candidate_ids

    # ---------------- Adaptive Statistics -----------------
    def _maybe_update_stats(self, dim: int, force: bool = False):
        debug = os.getenv("E8_PROXIMITY_DEBUG", "0") == "1"
        if debug:
            self.console.log(f"[PROXIMITY-DEBUG] _maybe_update_stats called for dim={dim}, force={force}")
        
        if not self.adaptive_enabled:
            if debug:
                self.console.log(f"[PROXIMITY-DEBUG] Adaptive not enabled (E8_PROXIMITY_ADAPTIVE={os.getenv('E8_PROXIMITY_ADAPTIVE', '1')})")
            return
            
        kdtree = self.indices.get(dim)
        if kdtree is None:
            if debug:
                self.console.log(f"[PROXIMITY-DEBUG] No KDTree for dim {dim}")
            return
            
        step = int(getattr(self.mind, 'step_num', 0) or 0)
        last_step = self._last_stats_step.get(dim, -10**9)
        if not force and (step - last_step) < self.recalc_interval:
            if debug:
                self.console.log(f"[PROXIMITY-DEBUG] Too soon to recalc: step={step}, last={last_step}, interval={self.recalc_interval}")
            return
            
        data = getattr(kdtree, 'data', None)
        if data is None or not hasattr(data, 'shape'):
            if debug:
                self.console.log(f"[PROXIMITY-DEBUG] No data in KDTree: data={data}, hasattr={hasattr(data, 'shape') if data else False}")
            return
            
        n = data.shape[0]
        min_points = max(6, self.stats_k + 1)  # Reduced from max(12, stats_k + 3) to max(6, stats_k + 1)
        if n < min_points:
            if debug:
                self.console.log(f"[PROXIMITY-DEBUG] Not enough points: n={n}, need={min_points}")
            return
        # Sample indices - adjust sample size for smaller datasets
        sample_size = min(self.stats_sample_size, n, max(6, n // 2))  # Use at least 6 or half the data
        try:
            if sample_size < n:
                idx = np.random.choice(n, size=sample_size, replace=False)
            else:
                idx = np.arange(n)  # Use all points if dataset is small
        except Exception:
            idx = np.arange(min(sample_size, n))
        k_query = min(self.stats_k + 1, n - 1)  # Ensure we don't query more neighbors than available
        
        if debug:
            self.console.log(f"[PROXIMITY-DEBUG] Computing stats: n={n}, sample_size={sample_size}, k_query={k_query}")
            
        try:
            sample_matrix = data[idx]
            dv, _ = kdtree.query(sample_matrix, k=k_query)
            if isinstance(dv, (float, np.floating)):
                if debug:
                    self.console.log(f"[PROXIMITY-DEBUG] Got scalar distance instead of array")
                return
            if dv.ndim == 1:
                if debug:
                    self.console.log(f"[PROXIMITY-DEBUG] Got 1D array instead of 2D: shape={dv.shape}")
                return  # not enough points to produce neighbors
            arr = np.asarray(dv[:,1:].ravel(), dtype=float)
            if debug:
                self.console.log(f"[PROXIMITY-DEBUG] Distance array: shape={arr.shape}, min={arr.min():.4f}, max={arr.max():.4f}")
        except Exception as e:
            if debug:
                self.console.log(f"[PROXIMITY-DEBUG] Query failed: {e}")
            return
        try:
            p10 = float(np.percentile(arr, self.lock_pct))
            pS = float(np.percentile(arr, self.strong_pct))
            pM = float(np.percentile(arr, self.moderate_pct))
        except Exception:
            # Fallback fixed percentiles
            p10 = float(np.percentile(arr, 10))
            pS = float(np.percentile(arr, 25))
            pM = float(np.percentile(arr, 50))
        stats = {
            'mean': float(arr.mean()),
            'std': float(arr.std() + 1e-9),
            'p_lock': p10,
            'p_strong': pS,
            'p_moderate': pM,
            'count': float(len(arr)),
            'k': float(self.stats_k),
            'updated_step': float(step)
        }
        self.local_stats[dim] = stats
        self._last_stats_step[dim] = step
        
        if debug:
            self.console.log(f"[PROXIMITY-DEBUG] Stats updated for dim {dim}: lock={p10:.4f}, strong={pS:.4f}, moderate={pM:.4f}")
            self.console.log(f"[PROXIMITY-DEBUG] Stats stored: {dim in self.local_stats}")

    def classify(self, dim: int, distance: float) -> Dict[str, Any]:
        debug = os.getenv("E8_PROXIMITY_DEBUG", "0") == "1"
        if debug:
            self.console.log(f"[PROXIMITY-DEBUG] classify called: dim={dim}, distance={distance:.4f}")
            
        if not self.adaptive_enabled:
            if debug:
                self.console.log(f"[PROXIMITY-DEBUG] Adaptive disabled, returning raw")
            return {'tier': 'raw', 'score': None, 'stats': None}
        
        # First, try to use fluid manifold proximity if available
        if hasattr(self.mind, 'fluid_mantle') and self.mind.fluid_mantle is not None:
            fluid_classification = self._classify_with_fluid_manifold(distance, debug)
            if fluid_classification:
                return fluid_classification
            
        # Second, try to use field manifold proximity if available
        if hasattr(self.mind, 'field_mantle') and self.mind.field_mantle is not None:
            # Try ray-based proximity first if enabled
            if self.use_field_rays and hasattr(self, 'compute_ray_based_proximity'):
                try:
                    # For ray-based proximity, we need node IDs, not just distance
                    # This is a simplified integration - in practice, we'd need to pass node info
                    ray_result = self._attempt_ray_proximity_fallback(dim, distance)
                    if ray_result:
                        ray_tier = ray_result.get('classification', {}).get('tier')
                        if not self._is_unknown_tier(ray_tier):
                            if debug:
                                self.console.log(f"[PROXIMITY-DEBUG] Using ray-based proximity: {ray_result.get('method')}")
                            return ray_result['classification']
                except Exception as ray_err:
                    if debug:
                        self.console.log(f"[PROXIMITY-DEBUG] Ray proximity failed: {ray_err}")

            # Fall back to traditional field manifold proximity
            field_classification = self.classify_with_field_manifold(dim, distance)
            if field_classification:
                fc_tier = field_classification.get('tier')
                if not self._is_unknown_tier(fc_tier):
                    return field_classification
            
        stats = self.local_stats.get(dim)
        if debug:
            self.console.log(f"[PROXIMITY-DEBUG] Existing stats for dim {dim}: {stats is not None}")
            
        if not stats:
            if debug:
                self.console.log(f"[PROXIMITY-DEBUG] No stats found, trying to update...")
            # try updating lazily
            self._maybe_update_stats(dim, force=True)
            stats = self.local_stats.get(dim)
            if debug:
                self.console.log(f"[PROXIMITY-DEBUG] After update, stats found: {stats is not None}")
                
        if not stats:
            if debug:
                self.console.log(f"[PROXIMITY-DEBUG] Still no stats, checking for fallback classification...")
            
            # Fallback classification for cases with insufficient data for full stats
            kdtree = self.indices.get(dim)
            if kdtree is not None and hasattr(kdtree, 'data') and kdtree.data.shape[0] >= 3:
                # Simple fallback: use fixed percentiles based on typical E8 lattice distances
                if debug:
                    self.console.log(f"[PROXIMITY-DEBUG] Using fallback classification with {kdtree.data.shape[0]} points")
                
                fallback_stats = {
                    'mean': float(distance * 2.0),  # Rough estimate
                    'p_lock': 0.3,    # Fixed conservative thresholds for E8 lattice
                    'p_strong': 0.6,
                    'p_moderate': 1.0,
                    'fallback': True
                }
                
                d = float(distance)
                if d <= fallback_stats['p_lock']:
                    tier = 'lock'
                elif d <= fallback_stats['p_strong']:
                    tier = 'strong'
                elif d <= fallback_stats['p_moderate']:
                    tier = 'moderate'
                else:
                    tier = 'weak'
                    
                score = 1.0 - min(1.0, max(0.0, d / fallback_stats['p_moderate']))
                
                if debug:
                    self.console.log(f"[PROXIMITY-DEBUG] Fallback classification: tier={tier}, score={score:.3f}")
                
                return {'tier': tier, 'score': float(score), 'stats': fallback_stats}
            
            if debug:
                self.console.log(f"[PROXIMITY-DEBUG] No fallback possible, returning raw")
            return {'tier': 'raw', 'score': None, 'stats': None}
        d = float(distance)
        # Determine tier
        if d <= stats['p_lock']:
            tier = 'lock'
        elif d <= stats['p_strong']:
            tier = 'strong'
        elif d <= stats['p_moderate']:
            tier = 'moderate'
        else:
            tier = 'weak'
        # Normalized score (invert distance within [p_lock, p_moderate])
        span = max(1e-9, stats['p_moderate'] - stats['p_lock'])
        score = 1.0 - min(1.0, max(0.0, (d - stats['p_lock']) / span))
        # Approx percentile via piecewise interpolation
        # lock boundary ~ lock_pct, strong boundary ~ strong_pct, moderate boundary ~ moderate_pct
        lock_pct = self.lock_pct
        strong_pct = self.strong_pct
        moderate_pct = self.moderate_pct
        if d <= stats['p_lock']:
            perc = lock_pct * (d / max(1e-9, stats['p_lock'])) if stats['p_lock'] > 0 else 0.0
        elif d <= stats['p_strong']:
            frac = (d - stats['p_lock']) / max(1e-9, stats['p_strong'] - stats['p_lock'])
            perc = lock_pct + frac * (strong_pct - lock_pct)
        elif d <= stats['p_moderate']:
            frac = (d - stats['p_strong']) / max(1e-9, stats['p_moderate'] - stats['p_strong'])
            perc = strong_pct + frac * (moderate_pct - strong_pct)
        else:
            # Extrapolate beyond moderate up to 100
            tail_span = max(1e-9, stats['p_moderate'])
            frac = (d - stats['p_moderate']) / tail_span
            perc = min(100.0, moderate_pct + frac * (100.0 - moderate_pct))
        return {'tier': tier, 'score': float(score), 'percentile': float(perc), 'stats': stats}

    def _is_unknown_tier(self, tier: Optional[str]) -> bool:
        """Normalize various 'unknown' tier markers to a boolean.

        Some classify() implementations return 'raw' or other sentinel values
        when no adaptive stats are available. Treat those as unknown for
        downstream gating logic.
        """
        return tier in (None, 'raw', 'unknown', '')

    def _attempt_ray_proximity_fallback(self, dim: int, distance: float) -> Optional[Dict[str, Any]]:
        """
        Attempt ray-based proximity when we don't have explicit node information.

        This is a fallback integration that works with distance-only information.
        """
        try:
            # For now, return None to use traditional field manifold
            # In a full implementation, this would attempt ray tracing with estimated node positions
            return None

        except Exception as e:
            self.console.log(f"[ProximityEngine] Ray proximity fallback failed: {e}")
            return None

    def integrate_ray_tracing_into_classification(self, source_node: str = None, target_node: str = None,
                                                 dim: int = None, distance: float = None) -> Dict[str, Any]:
        """
        Direct integration point for ray-based proximity with full node information.

        This method can be called when node IDs are available for optimal ray tracing.
        """
        try:
            if source_node and target_node and dim is not None:
                # Use full ray-based proximity
                result = self.compute_ray_based_proximity(source_node, target_node, dim, distance or 0.0)
                return result
            elif dim is not None and distance is not None:
                # Attempt fallback ray proximity
                result = self._attempt_ray_proximity_fallback(dim, distance)
                if result:
                    return result

            # Fall back to traditional classification
            return {'method': 'traditional', 'classification': self.classify(dim, distance)}

        except Exception as e:
            self.console.log(f"[ProximityEngine] Ray tracing integration failed: {e}")
            return {'method': 'error', 'classification': {'tier': 'raw', 'score': None, 'stats': None}}

    def get_enhanced_proximity_summary(self) -> Dict[str, Any]:
        """
        Get enhanced proximity summary with ray tracing statistics.
        """
        try:
            # Get traditional stats
            ray_stats = self.get_ray_stats_summary()

            # Add cache statistics
            cache_stats = self.get_cache_statistics()

            # Combine all proximity statistics
            summary = {
                'ray_tracing_enabled': self.use_field_rays,
                'monitor_only_mode': self.ray_monitor_only,
                'ray_success_rate': self.get_ray_success_rate(),
                'cache_size': sum(len(c.store) for c in self.ray_cache.values()),
                'cache_hit_rate': cache_stats.get('cache_hit_rate', 0.0),
                'avg_ray_latency_ms': ray_stats.get('avg_latency_ms', 0.0),
                'total_ray_attempts': ray_stats.get('total_attempts', 0),
                'ray_successes': ray_stats.get('successes', 0),
                'ray_failures': ray_stats.get('failures', 0),
                'cache_hits': ray_stats.get('cache_hits', 0),
                'cache_max_size': self.ray_cache_max_size,
                'cache_utilization_percent': cache_stats.get('utilization_percent', 0.0),
                'adaptive_enabled': self.adaptive_enabled,
                'stats_sample_size': self.stats_sample_size,
                'recalc_interval': self.recalc_interval
            }

            return summary

        except Exception as e:
            self.console.log(f"[ProximityEngine] Enhanced proximity summary failed: {e}")
            return {}

    # --- [END ADD] ---

    def _classify_with_fluid_manifold(self, distance: float, debug: bool = False) -> Optional[Dict[str, Any]]:
        """Classify proximity using the continuous fluid manifold structure with E8 geometric awareness."""
        try:
            fluid_mantle = self.mind.fluid_mantle
            if not fluid_mantle or not hasattr(fluid_mantle, 'density_field'):
                return None
                
            # Sample density field to understand consciousness distribution
            density_values = list(fluid_mantle.density_field.values())
            if len(density_values) < 5:  # Need some data
                return None
                
            density_array = np.array(density_values)
            density_mean = float(np.mean(density_array))
            density_std = float(np.std(density_array))
            
            # Sample velocity field to understand flow dynamics  
            velocity_values = [np.linalg.norm(v) for v in fluid_mantle.velocity_field.values()]
            if velocity_values:
                velocity_mean = float(np.mean(velocity_values))
            else:
                velocity_mean = 0.0
                
            # E8 Geometric Analysis (8D to 248D consciousness space)
            # Analyze dimensional distribution across the E8 lattice spectrum
            active_dimensions = set()
            dimensional_density = {}
            
            # Sample consciousness coordinates to determine active E8 subspaces
            for coord_key in list(fluid_mantle.density_field.keys())[:20]:  # Sample subset
                if isinstance(coord_key, (tuple, list)) and len(coord_key) >= 8:
                    # Determine effective dimensionality of this consciousness region
                    coord_vec = np.array(coord_key[:min(len(coord_key), 248)])  # E8 max 248D
                    nonzero_dims = np.count_nonzero(np.abs(coord_vec) > 1e-6)
                    active_dimensions.add(nonzero_dims)
                    
                    # Map to E8 geometric tiers: Core(8-16D), Shell(32-64D), Extended(128-248D)
                    if nonzero_dims <= 16:
                        geo_tier = 'core'
                    elif nonzero_dims <= 64:
                        geo_tier = 'shell'
                    else:
                        geo_tier = 'extended'
                        
                    dimensional_density[geo_tier] = dimensional_density.get(geo_tier, 0) + 1
                        
            # Determine dominant E8 geometric region
            max_tier = max(dimensional_density.items(), key=lambda x: x[1])[0] if dimensional_density else 'core'
            avg_active_dims = np.mean(list(active_dimensions)) if active_dimensions else 8
            geo_complexity = len(active_dimensions) / 20.0 if active_dimensions else 0.1  # Spread measure
                
            # Fluid-based proximity classification with E8 geometric factors
            # High density + low velocity = strong binding (lock/strong)
            # Low density + high velocity = weak binding (weak)
            # E8 geometry affects coupling strength: Core(8-16D) = tight, Extended(128-248D) = loose
            
            d = float(distance)
            
            # Adaptive thresholds based on current consciousness state + E8 geometry
            base_threshold = 0.5
            density_factor = (density_mean + 1e-6) / (density_std + 1e-6)  # Signal-to-noise ratio
            velocity_factor = velocity_mean
            
            # E8 geometric modulation: deeper dimensions create looser coupling
            if max_tier == 'core':  # 8-16D: Tight coupling, strong binding
                geo_factor = 0.8 - 0.2 * geo_complexity  # 0.6-0.8 range
                coupling_type = "CORE-MANIFOLD"
            elif max_tier == 'shell':  # 32-64D: Medium coupling  
                geo_factor = 1.0 + 0.3 * geo_complexity  # 1.0-1.3 range
                coupling_type = "SHELL-MANIFOLD"
            else:  # 128-248D: Loose coupling, extended binding
                geo_factor = 1.4 + 0.6 * geo_complexity  # 1.4-2.0 range
                coupling_type = "EXTENDED-MANIFOLD"
            
            # Adjust thresholds based on fluid dynamics + E8 geometry
            lock_threshold = base_threshold * geo_factor * (1.0 - 0.3 * density_factor) * (1.0 + 0.2 * velocity_factor)
            strong_threshold = base_threshold * geo_factor * (1.0 + 0.2 * density_factor) * (1.0 + 0.3 * velocity_factor)
            moderate_threshold = base_threshold * geo_factor * (1.5 + 0.5 * velocity_factor)
            
            lock_threshold = max(0.05, min(0.6, lock_threshold))
            strong_threshold = max(0.2, min(1.2, strong_threshold))  
            moderate_threshold = max(0.5, min(2.0, moderate_threshold))
            
            if d <= lock_threshold:
                tier = 'lock'
            elif d <= strong_threshold:
                tier = 'strong' 
            elif d <= moderate_threshold:
                tier = 'moderate'
            else:
                tier = 'weak'
                
            # Score based on position within the tier
            if tier == 'lock':
                score = 1.0 - (d / lock_threshold) * 0.2  # 0.8-1.0 range
            elif tier == 'strong':
                score = 0.8 - ((d - lock_threshold) / (strong_threshold - lock_threshold)) * 0.3  # 0.5-0.8 range
            elif tier == 'moderate':
                score = 0.5 - ((d - strong_threshold) / (moderate_threshold - strong_threshold)) * 0.3  # 0.2-0.5 range
            else:
                score = max(0.0, 0.2 - (d - moderate_threshold) / moderate_threshold * 0.2)  # 0.0-0.2 range
                
            fluid_stats = {
                'mean': density_mean,
                'density_mean': density_mean,
                'density_std': density_std,
                'velocity_mean': velocity_mean,
                'p_lock': lock_threshold,
                'p_strong': strong_threshold,
                'p_moderate': moderate_threshold,
                'density_factor': density_factor,
                'fluid_based': True,
                # E8 Geometric Information
                'e8_tier': max_tier,
                'e8_dims': avg_active_dims,
                'e8_complexity': geo_complexity,
                'coupling_type': coupling_type,
                'geo_factor': geo_factor
            }
            
            if debug:
                self.console.log(f"[PROXIMITY-DEBUG] Fluid classification: tier={tier}, score={score:.3f}")
                self.console.log(f"[PROXIMITY-DEBUG] Fluid thresholds: lock={lock_threshold:.3f}, strong={strong_threshold:.3f}, mod={moderate_threshold:.3f}")
                
            return {'tier': tier, 'score': float(score), 'stats': fluid_stats}
            
        except Exception as e:
            if debug:
                self.console.log(f"[PROXIMITY-DEBUG] Fluid classification failed: {e}")
            return None

    def get_avg_temperature(self, dim: int) -> float:
        """Cached average temperature per dimension to avoid repeated graph lookups."""
        if not hasattr(self.mind, 'memory'):
            return 0.0
        step = int(getattr(self.mind, 'step_num', 0) or 0)
        cached = self._temp_cache.get(dim)
        if cached and (step - cached[1]) < self.temp_cache_interval:
            return cached[0]
        shell = self.mind.dimensional_shells.get(dim)
        if not shell or not getattr(shell, 'vectors', None):
            self._temp_cache[dim] = (0.0, step)
            return 0.0
        temps = []
        try:
            for nid in list(shell.vectors.keys())[:128]:
                node = self.mind.memory.graph_db.get_node(nid) or {}
                t = node.get('temperature')
                if isinstance(t, (int, float)):
                    temps.append(float(t))
        except Exception:
            pass
        avg_t = float(np.mean(temps)) if temps else 0.0
        self._temp_cache[dim] = (avg_t, step)
        return avg_t

    def update_shell_index(self, dim: int, shell: DimensionalShell):
        if dim not in self.indices: return
        matrix, node_ids = shell.get_all_vectors_as_matrix()
        if matrix is not None and node_ids and matrix.shape[0] > 0:
            try:
                self.indices[dim] = KDTree(matrix)
                self.id_maps[dim] = node_ids
            except Exception as e:
                self.console.log(f"[ProximityEngine] Failed to build KDTree for dim {dim}: {e}")
                self.indices[dim] = None
                self.id_maps[dim] = []
        else:
            self.indices[dim] = None
            self.id_maps[dim] = []
        # Lazily update stats after rebuilding index
        try:
            self._maybe_update_stats(dim)
        except Exception:
            pass

    def find_similar_in_shell(self, query_vector: np.ndarray, dim: int, k: int = 5) -> List[tuple[str, float]]:
        kdtree = self.indices.get(dim)
        id_map = self.id_maps.get(dim)
        if kdtree is None or not id_map: return []
        num_points = kdtree.n
        if k > num_points: k = num_points
        if k == 0: return []
        distances, indices = kdtree.query(query_vector, k=k)
        if k == 1 and isinstance(indices, (int, np.integer)):
            return [(id_map[int(indices)], float(distances))]
        return [(id_map[int(i)], float(d)) for d, i in zip(distances, indices)]

    def cross_dimensional_query(self, query_vector: np.ndarray, source_dim: int, target_dim: int, k: int = 1) -> List[tuple[str, float]]:
        if source_dim == target_dim:
            return self.find_similar_in_shell(query_vector, target_dim, k)

        autoencoder_ready = TORCH_AVAILABLE and getattr(self.mind, 'autoencoder', None) is not None
        if autoencoder_ready and getattr(self.mind.autoencoder, 'project_between_dim', None):
            try:
                with torch.no_grad():
                    source_array = np.asarray(query_vector, dtype=np.float32)
                    source_tensor = torch.from_numpy(source_array).float().unsqueeze(0)
                    projected_tensor = self.mind.autoencoder.project_between_dim(
                        source_tensor, source_dim=source_dim, target_dim=target_dim
                    )
                if projected_tensor is not None:
                    projected_vector = safe_tensor_to_numpy(projected_tensor.squeeze(0))
                    results = self.find_similar_in_shell(projected_vector, target_dim, k)
                    if results:
                        return results
            except Exception:
                pass

        try:
            resized = np.asarray(query_vector, dtype=np.float32).reshape(-1)
            if resized.size != target_dim:
                if resized.size > target_dim:
                    resized = resized[:target_dim]
                else:
                    resized = np.pad(resized, (0, target_dim - resized.size), mode='constant')
            fallback = self.find_similar_in_shell(resized, target_dim, k)
            if fallback:
                return fallback
        except Exception:
            pass

        try:
            neighbors = self.find_similar_in_shell(query_vector, source_dim, max(k + 1, 2))
            if neighbors:
                return neighbors[:k]
        except Exception:
            pass
        return []

    def hybrid_rerank_query(self, query_vector: np.ndarray, rerank_shell_dim: int, initial_k: int = 20, final_k: int = 5) -> List[tuple[str, float]]:
        if rerank_shell_dim not in self.shell_dims or not TORCH_AVAILABLE or self.mind.autoencoder is None:
            return self.mind.memory.find_similar_in_main_storage(query_vector, k=final_k)

        initial_candidates = self.mind.memory.find_similar_in_main_storage(query_vector, k=initial_k)
        if not initial_candidates:
            return []

        candidate_ids = [nid for nid, _ in initial_candidates]
        # Metric-causal attention: optionally gate by geodesic light-cone (use our own helper)
        try:
            if getattr(self, 'cone_enabled', False):
                # Use the most recent active cognitive node as origin, else fallback to last accessed node in this shell
                origin_id = None
                try:
                    if hasattr(self.mind, 'get_active_cognitive_nodes'):
                        recent = self.mind.get_active_cognitive_nodes() or []
                        origin_id = recent[0] if recent else None
                except Exception:
                    origin_id = None
                if not origin_id:
                    try:
                        # Fallback: use most recently touched node from graph memory
                        if hasattr(self.mind.memory, 'last_accessed_node_id'):
                            origin_id = self.mind.memory.last_accessed_node_id
                    except Exception:
                        origin_id = None
                if origin_id:
                    filtered_ids = self.filter_by_light_cone(origin_id, candidate_ids, rerank_shell_dim)
                    if filtered_ids:
                        candidate_ids = filtered_ids
        except Exception:
            pass
        rerank_shell = self.mind.dimensional_shells[rerank_shell_dim]
        candidate_vectors_low_dim = []
        valid_candidate_ids = []
        for nid in candidate_ids:
            vec = rerank_shell.get_vector(nid)
            if vec is not None:
                candidate_vectors_low_dim.append(vec)
                valid_candidate_ids.append(nid)
        if not valid_candidate_ids:
            return initial_candidates[:final_k]

        with torch.no_grad():
            query_tensor_high_dim = torch.from_numpy(query_vector).float().unsqueeze(0)
            query_tensor_low_dim = self.mind.autoencoder.project_to_dim(query_tensor_high_dim, rerank_shell_dim)
        if query_tensor_low_dim is None:
            return initial_candidates[:final_k]

        query_vector_low_dim = safe_tensor_to_numpy(query_tensor_low_dim.squeeze(0))
        candidate_matrix = np.array(candidate_vectors_low_dim)
        distances = cosine_distances(query_vector_low_dim.reshape(1, -1), candidate_matrix).flatten()
        reranked_results = sorted(zip(valid_candidate_ids, distances.tolist()), key=lambda item: item[1])
        return reranked_results[:final_k]

    def _classify_with_field_manifold_base(self, dim: int, distance: float, query_pos: np.ndarray = None,
                                          target_pos: np.ndarray = None) -> Dict[str, Any]:
        """Classify proximity using the hyperdimensional field manifold with Maxwell field evolution and rotor sources.

        This method integrates with HyperdimensionalFieldMantle to provide field-aware proximity classification
        that considers electromagnetic field dynamics, rotor sources, and E8 geometric constraints.

        Args:
            dim: Dimensional shell dimension
            distance: Raw proximity distance
            query_vector: Optional query vector for field-based analysis

        Returns:
            Dictionary with field-aware classification including tier, score, and field statistics
        """
        try:
            # Check if HyperdimensionalFieldMantle is available
            if not hasattr(self.mind, 'field_mantle') or self.mind.field_mantle is None:
                self.console.log("[ProximityEngine] No field mantle available for field-aware classification")
                return {'tier': 'raw', 'score': None, 'field_stats': None}

            field_mantle = self.mind.field_mantle

            # Get current field state for the dimensional shell
            field_state = field_mantle.get_shell_field_state(dim)
            if not field_state:
                self.console.log(f"[ProximityEngine] No field state available for dimension {dim}")
                return {'tier': 'raw', 'score': None, 'field_stats': None}

            # Extract field components
            E_field = field_state.get('E', np.zeros(dim))
            B_field = field_state.get('B', np.zeros(dim))
            rotor_sources = field_state.get('rotor_sources', {})
            field_energy = field_state.get('energy_density', 0.0)

            # Calculate field-based proximity metrics
            d = float(distance)

            # E-field coupling strength (closer points have stronger E-field interaction)
            e_field_strength = np.linalg.norm(E_field)
            e_coupling_factor = 1.0 / (1.0 + d * e_field_strength)

            # B-field topology influence (magnetic field creates binding patterns)
            b_field_strength = np.linalg.norm(B_field)
            b_topology_factor = np.exp(-d * b_field_strength)  # Exponential decay with distance

            # Rotor source influence (rotor fields create rotational binding)
            rotor_influence = 0.0
            if rotor_sources:
                # Average rotor strength across available sources
                rotor_strengths = [np.linalg.norm(rotor) for rotor in rotor_sources.values()]
                if rotor_strengths:
                    avg_rotor_strength = np.mean(rotor_strengths)
                    rotor_influence = avg_rotor_strength / (1.0 + d)  # Distance-attenuated rotor influence

            # Maxwell field evolution factor (field dynamics affect binding stability)
            maxwell_factor = field_mantle.compute_maxwell_evolution_factor(dim, d)

            # E8 geometric constraints (higher dimensions have different field propagation)
            geometric_factor = 1.0
            if dim <= 16:  # Core dimensions: tight field coupling
                geometric_factor = 0.8 + 0.2 * (dim / 16.0)
            elif dim <= 64:  # Shell dimensions: medium field coupling
                geometric_factor = 1.0 + 0.3 * ((dim - 16) / 48.0)
            else:  # Extended dimensions: loose field coupling
                geometric_factor = 1.4 + 0.6 * min(1.0, (dim - 64) / 184.0)

            # Energy density modulation (higher energy regions create stronger binding)
            energy_factor = 1.0 + np.log(1.0 + field_energy)

            # Combined field proximity score
            field_proximity_score = (
                0.3 * e_coupling_factor +
                0.25 * b_topology_factor +
                0.2 * rotor_influence +
                0.15 * maxwell_factor +
                0.1 * geometric_factor
            ) * energy_factor

            # Adaptive thresholds based on field dynamics
            base_lock_threshold = 0.7
            base_strong_threshold = 0.5
            base_moderate_threshold = 0.3

            # Field state influences thresholds
            field_intensity = (e_field_strength + b_field_strength) / 2.0
            threshold_modifier = 0.5 + 0.5 * np.tanh(field_intensity)  # Range: 0-1

            lock_threshold = base_lock_threshold * threshold_modifier
            strong_threshold = base_strong_threshold * threshold_modifier
            moderate_threshold = base_moderate_threshold * threshold_modifier

            # Ensure reasonable threshold ranges
            lock_threshold = max(0.1, min(0.9, lock_threshold))
            strong_threshold = max(0.2, min(0.8, strong_threshold))
            moderate_threshold = max(0.1, min(0.6, moderate_threshold))

            # Classify based on field proximity score
            if field_proximity_score >= lock_threshold:
                tier = 'lock'
                confidence = (field_proximity_score - lock_threshold) / (1.0 - lock_threshold)
            elif field_proximity_score >= strong_threshold:
                tier = 'strong'
                confidence = (field_proximity_score - strong_threshold) / (lock_threshold - strong_threshold)
            elif field_proximity_score >= moderate_threshold:
                tier = 'moderate'
                confidence = (field_proximity_score - moderate_threshold) / (strong_threshold - moderate_threshold)
            else:
                tier = 'weak'
                confidence = field_proximity_score / moderate_threshold

            # Normalize confidence to 0-1 range
            confidence = max(0.0, min(1.0, confidence))

            # Field-based statistics
            field_stats = {
                'e_field_strength': float(e_field_strength),
                'b_field_strength': float(b_field_strength),
                'rotor_influence': float(rotor_influence),
                'field_energy': float(field_energy),
                'maxwell_factor': float(maxwell_factor),
                'geometric_factor': float(geometric_factor),
                'energy_factor': float(energy_factor),
                'field_proximity_score': float(field_proximity_score),
                'lock_threshold': float(lock_threshold),
                'strong_threshold': float(strong_threshold),
                'moderate_threshold': float(moderate_threshold),
                'field_intensity': float(field_intensity),
                'threshold_modifier': float(threshold_modifier),
                'field_based': True
            }

            self.console.log(f"[ProximityEngine] Field manifold classification: tier={tier}, score={field_proximity_score:.3f}, confidence={confidence:.3f}")

            return {
                'tier': tier,
                'score': float(field_proximity_score),
                'confidence': float(confidence),
                'stats': field_stats,
                'field_components': {
                    'E': E_field.tolist() if hasattr(E_field, 'tolist') else E_field,
                    'B': B_field.tolist() if hasattr(B_field, 'tolist') else B_field,
                    'rotor_sources': {k: v.tolist() if hasattr(v, 'tolist') else v for k, v in rotor_sources.items()}
                }
            }

        except Exception as e:
            self.console.log(f"[ProximityEngine] Field manifold classification failed: {e}")
            return {'tier': 'raw', 'score': None, 'field_stats': None}

    # --- [ADD REFRACTION METHODS TO ProximityEngine] ---
    def _effective_index(self, position: np.ndarray, horizon_layers: Dict[str, 'HorizonLayer']) -> float:
        """
        Compute effective refractive index at position based on horizon field gradients.
        Returns n_eff where n > 1 indicates slower propagation (stronger coupling).
        """
        try:
            if not horizon_layers:
                return 1.0

            # Base refractive index
            n_base = 1.0

            # Accumulate refraction from all horizons
            for horizon_name, horizon in horizon_layers.items():
                if horizon.indices and len(horizon.indices) > 0:
                    # Compute distance to horizon boundary
                    if horizon.pos is not None and len(horizon.pos) > 0:
                        distances = np.linalg.norm(horizon.pos - position[:3], axis=1)
                        min_dist = np.min(distances)

                        # Refractive index increases near boundaries (Snell's law analog)
                        # n_eff = n_base + k / (d + epsilon) where k controls strength
                        k = 0.5  # Refractive strength parameter
                        epsilon = 1e-6  # Prevent division by zero
                        n_increment = k / (min_dist + epsilon)
                        n_base += n_increment

            # Clamp to reasonable range
            return max(1.0, min(3.0, n_base))

        except Exception as e:
            self.console.log(f"[ProximityEngine] Effective index computation failed: {e}")
            return 1.0

    def _refract(self, incident_vec: np.ndarray, normal: np.ndarray, n1: float, n2: float) -> np.ndarray:
        """
        Compute refracted vector using Snell's law analog for E8 proximity rays.
        incident_vec: incoming direction vector
        normal: surface normal (pointing toward incident side)
        n1, n2: refractive indices of incident and refracted media
        """
        try:
            # Normalize vectors
            incident = incident_vec / (np.linalg.norm(incident_vec) + 1e-12)
            normal = normal / (np.linalg.norm(normal) + 1e-12)

            # Compute angle of incidence
            cos_theta_i = -np.dot(incident, normal)  # Normal points toward incident side
            cos_theta_i = np.clip(cos_theta_i, -1.0, 1.0)

            # Snell's law: n1 * sin(theta_i) = n2 * sin(theta_r)
            sin_theta_i = np.sqrt(1.0 - cos_theta_i**2)
            sin_theta_r = (n1 / n2) * sin_theta_i

            # Check for total internal reflection
            if sin_theta_r > 1.0:
                # Total internal reflection - reflect instead
                return incident - 2.0 * cos_theta_i * normal

            cos_theta_r = np.sqrt(1.0 - sin_theta_r**2)

            # Compute refracted vector
            refracted = (n1 / n2) * incident + (n1 / n2 * cos_theta_i - cos_theta_r) * normal
            refracted = refracted / (np.linalg.norm(refracted) + 1e-12)

            return refracted

        except Exception as e:
            self.console.log(f"[ProximityEngine] Refraction computation failed: {e}")
            return incident_vec

    def _cross_horizons_with_refraction(self, start_pos: np.ndarray, end_pos: np.ndarray,
                                       horizon_layers: Dict[str, 'HorizonLayer']) -> Tuple[np.ndarray, float]:
        """
        Trace ray from start_pos to end_pos with refraction through horizon boundaries.
        Returns (refracted_direction, total_path_length)
        """
        try:
            if not horizon_layers:
                direction = end_pos - start_pos
                return direction / np.linalg.norm(direction), np.linalg.norm(direction)

            # Start with straight line
            current_pos = start_pos.copy()
            current_dir = end_pos - start_pos
            current_dir = current_dir / (np.linalg.norm(current_dir) + 1e-12)
            total_path = 0.0

            # Find all horizon crossings
            crossings = []
            for horizon_name, horizon in horizon_layers.items():
                if horizon.indices and horizon.pos is not None and len(horizon.pos) > 0:
                    # Check for intersections with horizon boundary
                    for i, h_pos in enumerate(horizon.pos):
                        if horizon.normals is not None and i < len(horizon.normals):
                            # Simple ray-plane intersection
                            normal = horizon.normals[i]
                            denom = np.dot(current_dir, normal)
                            if abs(denom) > 1e-6:
                                t = np.dot(h_pos - current_pos, normal) / denom
                                if t > 0:  # Intersection in positive direction
                                    cross_pos = current_pos + t * current_dir
                                    crossings.append((t, cross_pos, normal, horizon_name))

            # Sort crossings by distance
            crossings.sort(key=lambda x: x[0])

            # Apply refraction at each crossing
            for t, cross_pos, normal, horizon_name in crossings:
                # Move to crossing point
                segment_length = t
                total_path += segment_length
                current_pos = cross_pos

                # Compute refractive indices
                n1 = self._effective_index(current_pos - 1e-6 * normal, horizon_layers)  # Just outside
                n2 = self._effective_index(current_pos + 1e-6 * normal, horizon_layers)  # Just inside

                # Refract the ray
                current_dir = self._refract(current_dir, normal, n1, n2)

            # Add remaining distance to end point
            remaining_dir = end_pos - current_pos
            remaining_dist = np.linalg.norm(remaining_dir)
            total_path += remaining_dist

            return current_dir, total_path

        except Exception as e:
            self.console.log(f"[ProximityEngine] Horizon refraction tracing failed: {e}")
            # Fallback to straight line
            direction = end_pos - start_pos
            return direction / np.linalg.norm(direction), np.linalg.norm(direction)

    def classify_with_field_manifold(self, dim: int, distance: float, query_pos: np.ndarray = None,
                                    target_pos: np.ndarray = None) -> Dict[str, Any]:
        """
        Enhanced field manifold classification with horizon refraction support.
        """
        try:
            # Get base field classification
            base_result = self._classify_with_field_manifold_base(dim, distance, query_pos, target_pos)
            if not base_result:
                return base_result
            br_tier = base_result.get('tier')
            if self._is_unknown_tier(br_tier):
                return base_result

            # Apply horizon refraction if positions available and horizons enabled
            if (query_pos is not None and target_pos is not None and
                hasattr(self.mind, 'horizon_manager') and self.mind.horizon_manager):

                try:
                    horizon_layers = self.mind.horizon_manager.get_horizon_layers()
                    if horizon_layers:
                        # Compute refracted path
                        refracted_dir, path_length = self._cross_horizons_with_refraction(
                            query_pos, target_pos, horizon_layers
                        )

                        # Adjust distance based on refracted path
                        refracted_distance = path_length

                        # Reclassify with refracted distance
                        if abs(refracted_distance - distance) > 1e-6:
                            # Recalculate tier based on refracted distance
                            field_proximity_score = base_result.get('score', 0.5)
                            # Adjust score based on refraction (longer path = weaker coupling)
                            refraction_factor = distance / (refracted_distance + 1e-6)
                            adjusted_score = field_proximity_score * refraction_factor

                            # Re-tier based on adjusted score
                            if adjusted_score >= base_result.get('stats', {}).get('lock_threshold', 0.7):
                                new_tier = 'lock'
                            elif adjusted_score >= base_result.get('stats', {}).get('strong_threshold', 0.5):
                                new_tier = 'strong'
                            elif adjusted_score >= base_result.get('stats', {}).get('moderate_threshold', 0.3):
                                new_tier = 'moderate'
                            else:
                                new_tier = 'weak'

                            # Update result with refraction info
                            base_result['tier'] = new_tier
                            base_result['score'] = float(adjusted_score)
                            base_result['refracted_distance'] = float(refracted_distance)
                            base_result['refraction_factor'] = float(refraction_factor)
                            base_result['refracted'] = True

                except Exception as refract_err:
                    self.console.log(f"[ProximityEngine] Horizon refraction in classification failed: {refract_err}")
                    base_result['refracted'] = False

            return base_result

        except Exception as e:
            self.console.log(f"[ProximityEngine] Enhanced field manifold classification failed: {e}")
            return {'tier': 'raw', 'score': None, 'field_stats': None}

    def get_ray_success_rate(self) -> float:
        """Get the success rate of ray-based proximity computations."""
        total_attempts = self.ray_stats['success'] + self.ray_stats['fail']
        if total_attempts == 0:
            return 0.0
        return self.ray_stats['success'] / total_attempts

    def get_ray_stats_summary(self) -> Dict[str, Any]:
        """Get comprehensive ray tracing statistics for summary prints."""
        total_attempts = self.ray_stats['success'] + self.ray_stats['fail']
        avg_latency = 0.0
        if total_attempts > 0:
            avg_latency = self.ray_stats['latency_total'] / total_attempts

        return {
            'success_rate': self.get_ray_success_rate(),
            'total_attempts': total_attempts,
            'successes': self.ray_stats['success'],
            'failures': self.ray_stats['fail'],
            'cache_hits': self.ray_stats['cache_hits'],
                'cache_size': sum(len(c.store) for c in self.ray_cache.values()),
            'avg_latency_ms': avg_latency * 1000,  # Convert to milliseconds
            'use_field_rays': self.use_field_rays,
            'monitor_only': self.ray_monitor_only
        }

    def compute_refractive_index(self, position: np.ndarray, dim: int) -> float:
        """
        Compute refractive index n(x) at a position using field energy density and curvature.

        n(x) = 1 + a�u(x) + ߷L(x)

        Where:
        - u(x) is field energy density
        - L(x) is local curvature/Ricci scalar
        - a, � are tunable parameters
        """
        try:
            # Get field state for the dimensional shell
            if not hasattr(self.mind, 'field_mantle') or self.mind.field_mantle is None:
                return 1.0

            field_mantle = self.mind.field_mantle
            field_state = field_mantle.get_shell_field_state(dim)

            if not field_state:
                return 1.0

            # Extract field energy density u(x)
            energy_density = field_state.get('energy_density', 0.0)

            # Compute local curvature approximation L(x)
            # For E8 manifolds, we use a combination of field gradients and shell geometry
            curvature = self._compute_local_curvature(position, dim, field_state)

            # Tunable parameters (could be made configurable)
            alpha = float(os.getenv("E8_REFRACTION_ALPHA", "0.1"))  # Energy density coefficient
            beta = float(os.getenv("E8_REFRACTION_BETA", "0.05"))  # Curvature coefficient

            # Compute refractive index: n(x) = 1 + a�u(x) + ߷L(x)
            n_x = 1.0 + alpha * energy_density + beta * curvature

            # Clamp to reasonable range for stability
            n_min = float(os.getenv("E8_REFRACTION_N_MIN", "0.5"))
            n_max = float(os.getenv("E8_REFRACTION_N_MAX", "3.0"))

            return float(np.clip(n_x, n_min, n_max))

        except Exception as e:
            self.console.log(f"[ProximityEngine] Refractive index computation failed: {e}")
            return 1.0

    def _compute_local_curvature(self, position: np.ndarray, dim: int, field_state: Dict[str, Any]) -> float:
        """
        Compute local curvature approximation for refractive index calculation.

        Uses field gradients and shell geometry to estimate Ricci-like scalar.
        """
        try:
            # Get field components
            E_field = field_state.get('E', np.zeros(dim))
            B_field = field_state.get('B', np.zeros(dim))

            # Compute field gradients (simplified approximation)
            e_gradient_norm = np.linalg.norm(E_field)  # |?E|
            b_gradient_norm = np.linalg.norm(B_field)  # |?B|

            # Shell geometry factor based on dimension
            if dim <= 16:  # Core dimensions: higher curvature
                geom_factor = 1.0 + (16 - dim) / 16.0
            elif dim <= 64:  # Shell dimensions: medium curvature
                geom_factor = 0.8 + (dim - 16) / 48.0
            else:  # Extended dimensions: lower curvature
                geom_factor = 0.5 + min(0.3, (dim - 64) / 184.0)

            # Combine into curvature estimate
            # Higher gradients + higher dimensional complexity = higher curvature
            curvature = (e_gradient_norm + b_gradient_norm) * geom_factor

            # Add position-dependent variation
            if len(position) >= 3:
                # Use first 3 components for spatial variation
                spatial_variation = 0.1 * np.sin(np.sum(position[:3]))
                curvature += spatial_variation

            return float(curvature)

        except Exception as e:
            self.console.log(f"[ProximityEngine] Local curvature computation failed: {e}")
            return 0.0

    def solve_eikonal_equation(self, source_node: str, target_node: str, dim: int,
                              max_iterations: int = 1000) -> Optional[PathAsset]:
        """
        Solve the eikonal equation ||?T(x)|| = n(x) to find the minimum optical path.

        Uses a simplified fast marching method adapted for graph structures.
        Returns PathAsset with the computed path or None if solution fails.
        """
        start_time = time.time()

        try:
            # Get shell and node information
            shell = self.mind.dimensional_shells.get(dim)
            if not shell or not hasattr(shell, 'vectors') or not shell.vectors:
                return None

            # Get node positions
            source_pos = shell.vectors.get(source_node)
            target_pos = shell.vectors.get(target_node)

            if source_pos is None or target_pos is None:
                return None

            # Initialize travel time field T(x)
            node_ids = list(shell.vectors.keys())
            travel_times = {node_id: float('inf') for node_id in node_ids}
            travel_times[source_node] = 0.0

            # Priority queue for fast marching (node_id, travel_time)
            pq = [(0.0, source_node)]
            visited = set()
            predecessors = {}  # For path reconstruction

            # Fast marching iterations
            iterations = 0
            converged = False

            while pq and iterations < max_iterations:
                iterations += 1

                # Get node with smallest travel time
                current_time, current_node = heapq.heappop(pq)

                if current_node in visited:
                    continue

                visited.add(current_node)

                # Check if we've reached the target
                if current_node == target_node:
                    converged = True
                    break

                # Skip if travel time is infinite (isolated node)
                if current_time == float('inf'):
                    continue

                # Get current position and refractive index
                current_pos = shell.vectors[current_node]
                n_current = self.compute_refractive_index(current_pos, dim)

                # Find neighboring nodes (simplified: use k-NN approximation)
                neighbors = self._find_graph_neighbors(current_node, dim, k=10)

                for neighbor_id in neighbors:
                    if neighbor_id in visited:
                        continue

                    neighbor_pos = shell.vectors.get(neighbor_id)
                    if neighbor_pos is None:
                        continue

                    # Compute refractive index at neighbor
                    n_neighbor = self.compute_refractive_index(neighbor_pos, dim)

                    # Compute edge length and average refractive index
                    edge_length = np.linalg.norm(neighbor_pos - current_pos)
                    n_avg = (n_current + n_neighbor) / 2.0

                    # Update travel time using eikonal equation
                    # T_neighbor <= T_current + n_avg * edge_length
                    new_time = current_time + n_avg * edge_length

                    # Update if better path found
                    if new_time < travel_times[neighbor_id]:
                        travel_times[neighbor_id] = new_time
                        predecessors[neighbor_id] = current_node
                        heapq.heappush(pq, (new_time, neighbor_id))

            # Reconstruct path if converged
            if not converged or target_node not in predecessors:
                computation_time = time.time() - start_time
                return PathAsset(
                    waypoints=[source_node, target_node],
                    length=np.linalg.norm(target_pos - source_pos),
                    action=float('inf'),
                    attenuation=1.0,  # Maximum attenuation for failed paths
                    success=False,
                    computation_time=computation_time
                )

            # Reconstruct the path
            path = []
            current = target_node
            while current != source_node:
                path.append(current)
                current = predecessors.get(current)
                if current is None:  # Broken path
                    break
            path.append(source_node)
            path.reverse()

            # Compute path metrics
            total_length = 0.0
            total_action = 0.0
            attenuation = 0.0

            positions = []
            for i in range(len(path) - 1):
                node_a = path[i]
                node_b = path[i + 1]

                pos_a = shell.vectors[node_a]
                pos_b = shell.vectors[node_b]

                positions.append(pos_a)

                # Edge metrics
                edge_length = np.linalg.norm(pos_b - pos_a)
                n_a = self.compute_refractive_index(pos_a, dim)
                n_b = self.compute_refractive_index(pos_b, dim)
                n_avg = (n_a + n_b) / 2.0

                total_length += edge_length
                total_action += n_avg * edge_length
                attenuation += 0.01 * edge_length  # Simple distance-based attenuation

            positions.append(shell.vectors[path[-1]])

            # Normalize attenuation to [0,1]
            attenuation = min(1.0, attenuation)

            computation_time = time.time() - start_time

            # Create path asset
            path_asset = PathAsset(
                waypoints=path,
                length=total_length,
                action=total_action,
                attenuation=attenuation,
                positions=positions,
                computation_time=computation_time,
                success=True
            )

            return path_asset

        except Exception as e:
            self.console.log(f"[ProximityEngine] Eikonal equation solver failed: {e}")
            computation_time = time.time() - start_time
            return PathAsset(
                waypoints=[source_node, target_node],
                length=0.0,
                action=float('inf'),
                attenuation=1.0,
                success=False,
                computation_time=computation_time
            )

    def _global_geodesic_path(self, source_node: str, target_node: str, dim: int,
                               k: int = 10, max_expansions: int = 10000,
                               use_astar: Optional[bool] = None,
                               beta: Optional[float] = None) -> Optional[PathAsset]:
        """
        Global curvature-weighted shortest path over k-NN graph using Dijkstra/A*.

        Edge cost: L_ij = ||x_i - x_j|| * 0.5 * (n_i + n_j), with n_i = exp(beta * |kappa_i|).

        Returns PathAsset or None if not computable.
        """
        start_t = time.time()
        try:
            shell = self.mind.dimensional_shells.get(dim)
            if not shell or not getattr(shell, 'vectors', None):
                return None
            if source_node not in shell.vectors or target_node not in shell.vectors:
                return None

            use_astar = self.ray_use_astar if use_astar is None else bool(use_astar)
            beta = self.ray_beta if beta is None else float(beta)

            # Small caches for speed during search
            pos_cache: Dict[str, np.ndarray] = {}
            n_cache: Dict[str, float] = {}

            def get_pos(nid: str) -> Optional[np.ndarray]:
                p = pos_cache.get(nid)
                if p is not None:
                    return p
                p = shell.vectors.get(nid)
                if p is not None:
                    pos_cache[nid] = p
                return p

            # Refractive index from curvature proxy
            def get_index(nid: str) -> float:
                val = n_cache.get(nid)
                if val is not None:
                    return val
                p = get_pos(nid)
                if p is None:
                    n_cache[nid] = 1.0
                    return 1.0
                # Use existing compute_refractive_index which already blends energy+curvature
                n_val = float(self.compute_refractive_index(p, dim))
                # Optional beta amplification on curvature via exp(beta*|kappa|) like term
                try:
                    # Estimate local curvature using existing helper
                    field_state = getattr(self.mind, 'field_mantle', None)
                    if field_state is not None and getattr(self.mind.field_mantle, 'get_shell_field_state', None):
                        fs = self.mind.field_mantle.get_shell_field_state(dim) or {}
                        kappa = abs(self._compute_local_curvature(p, dim, fs))
                        n_val = float(n_val * np.exp(beta * float(kappa)))
                except Exception:
                    pass
                # Clamp
                n_min = float(os.getenv("E8_REFRACTION_N_MIN", "0.5"))
                n_max = float(os.getenv("E8_REFRACTION_N_MAX", "3.0"))
                n_val = float(np.clip(n_val, n_min, n_max))
                n_cache[nid] = n_val
                return n_val

            source = source_node
            target = target_node

            # Heuristic for A*
            tgt_pos = get_pos(target)
            def heuristic(nid: str) -> float:
                if not use_astar or tgt_pos is None:
                    return 0.0
                p = get_pos(nid)
                if p is None:
                    return 0.0
                # optimistic cost: Euclidean distance * min refractive index
                d = float(np.linalg.norm(tgt_pos - p))
                return d

            open_heap: List[Tuple[float, str]] = []
            heapq.heappush(open_heap, (0.0, source))
            came_from: Dict[str, str] = {}
            g_score: Dict[str, float] = {source: 0.0}
            visited: set[str] = set()

            expansions = 0
            while open_heap and expansions < max_expansions:
                _, current = heapq.heappop(open_heap)
                if current in visited:
                    continue
                visited.add(current)
                if current == target:
                    break
                expansions += 1

                # On-the-fly neighbors via KDTree
                neighbors = self._find_graph_neighbors(current, dim, k=k)
                cur_pos = get_pos(current)
                if cur_pos is None:
                    continue
                n_cur = get_index(current)

                for nb in neighbors:
                    if nb in visited:
                        continue
                    nb_pos = get_pos(nb)
                    if nb_pos is None:
                        continue
                    n_nb = get_index(nb)
                    d_ij = float(np.linalg.norm(nb_pos - cur_pos))
                    if d_ij <= 0:
                        continue
                    cost = d_ij * 0.5 * (n_cur + n_nb)
                    tentative = g_score[current] + cost
                    if tentative < g_score.get(nb, float('inf')):
                        came_from[nb] = current
                        g_score[nb] = tentative
                        f = tentative + heuristic(nb)
                        heapq.heappush(open_heap, (f, nb))

            if target not in came_from and source != target:
                # No path found
                return PathAsset(
                    waypoints=[source, target],
                    length=float(np.linalg.norm(get_pos(target) - get_pos(source))) if (get_pos(target) is not None and get_pos(source) is not None) else 0.0,
                    action=float('inf'),
                    attenuation=1.0,
                    positions=[get_pos(source), get_pos(target)] if (get_pos(source) is not None and get_pos(target) is not None) else None,
                    shell_crossings=[], horizon_gates=[],
                )

            # Reconstruct path
            path_nodes = [target]
            cur = target
            while cur != source and cur in came_from:
                cur = came_from[cur]
                path_nodes.append(cur)
            path_nodes.reverse()

            # Compute metrics
            total_L = 0.0
            total_S = 0.0
            attn = 0.0
            positions: List[np.ndarray] = []
            for i in range(len(path_nodes) - 1):
                a, b = path_nodes[i], path_nodes[i+1]
                pa, pb = get_pos(a), get_pos(b)
                positions.append(pa)
                seg = float(np.linalg.norm(pb - pa))
                n_a, n_b = get_index(a), get_index(b)
                n_avg = 0.5 * (n_a + n_b)
                total_L += seg
                total_S += n_avg * seg
                attn += 0.01 * seg
            if path_nodes:
                last_p = get_pos(path_nodes[-1])
                if last_p is not None:
                    positions.append(last_p)
            attn = min(1.0, attn)

            asset = PathAsset(
                waypoints=path_nodes,
                length=total_L,
                action=total_S,
                attenuation=attn,
                positions=positions,
            )
            asset.computation_time = time.time() - start_t
            return asset
        except Exception as e:
            try:
                self.console.log(f"[ProximityEngine] Global geodesic failed: {e}")
            except Exception:
                pass
            return None

    def _find_graph_neighbors(self, node_id: str, dim: int, k: int = 10) -> List[str]:
        """
        Find neighboring nodes in the graph structure.
        Uses KD-tree approximation since we don't have explicit graph edges.
        """
        try:
            shell = self.mind.dimensional_shells.get(dim)
            if not shell or not hasattr(shell, 'vectors'):
                return []

            node_pos = shell.vectors.get(node_id)
            if node_pos is None:
                return []

            # Use KD-tree to find k nearest neighbors
            neighbors = self.find_similar_in_shell(node_pos, dim, k + 1)  # +1 to exclude self

            # Extract node IDs and exclude self
            neighbor_ids = []
            for nid, _ in neighbors:
                if nid != node_id:
                    neighbor_ids.append(nid)
                if len(neighbor_ids) >= k:
                    break

            return neighbor_ids

        except Exception as e:
            self.console.log(f"[ProximityEngine] Graph neighbor finding failed: {e}")
            return []

    def trace_geodesic_ray(self, source_node: str, target_node: str, dim: int,
                          max_steps: int = 1000, step_size: float = 0.1) -> Optional[PathAsset]:
        """
        Trace null-like geodesic using Hamiltonian mechanics.

        Integrates H(x,p) = (1/2)g^{�?}(x)p_� p_? = 0 using leapfrog integration
        adapted for discrete graph structures.
        """
        start_time = time.time()

        try:
            # Get shell and node information
            shell = self.mind.dimensional_shells.get(dim)
            if not shell or not hasattr(shell, 'vectors') or not shell.vectors:
                return None

            # Get node positions
            source_pos = shell.vectors.get(source_node)
            target_pos = shell.vectors.get(target_node)

            if source_pos is None or target_pos is None:
                return None

            # Initialize geodesic trajectory
            current_pos = source_pos.copy()
            target_direction = target_pos - source_pos
            target_direction = target_direction / (np.linalg.norm(target_direction) + 1e-12)

            # Initialize momentum (null geodesic: light-like)
            momentum = target_direction.copy()  # Initial direction

            # Trajectory storage
            positions = [current_pos.copy()]
            waypoints = [source_node]
            total_action = 0.0
            attenuation = 0.0

            # Leapfrog integration
            step = 0
            converged = False

            while step < max_steps:
                step += 1

                # Check convergence (close to target)
                distance_to_target = np.linalg.norm(current_pos - target_pos)
                if distance_to_target < step_size:
                    converged = True
                    break

                # Compute metric at current position
                g_metric = self._compute_metric_tensor(current_pos, dim)

                # Hamiltonian constraint: H = (1/2)g^{�?}p_� p_? = 0 (null geodesic)
                hamiltonian = 0.5 * np.dot(momentum, np.dot(g_metric, momentum))

                # Compute Christoffel symbols for geodesic equation
                christoffel = self._compute_christoffel_symbols(current_pos, dim)

                # Update momentum using geodesic equation
                # dp^�/d? = -G^�_{a�} p^a p^�
                momentum_update = np.zeros_like(momentum)
                for mu in range(len(momentum)):
                    for alpha in range(len(momentum)):
                        for beta in range(len(momentum)):
                            momentum_update[mu] -= christoffel[mu][alpha][beta] * momentum[alpha] * momentum[beta]

                # Normalize momentum to maintain null condition
                momentum = momentum + step_size * momentum_update
                momentum_norm = np.sqrt(np.dot(momentum, np.dot(g_metric, momentum)))
                if momentum_norm > 1e-12:
                    momentum = momentum / momentum_norm

                # Update position
                # dx^�/d? = p^�
                position_update = np.dot(g_metric, momentum)
                current_pos = current_pos + step_size * position_update

                # Store position
                positions.append(current_pos.copy())

                # Find nearest node for waypoint tracking
                nearest_node = self._find_nearest_node(current_pos, dim)
                if nearest_node and nearest_node not in waypoints:
                    waypoints.append(nearest_node)

                # Accumulate action (optical path)
                n_local = self.compute_refractive_index(current_pos, dim)
                total_action += n_local * step_size

                # Accumulate attenuation
                attenuation += 0.001 * step_size  # Small distance-based attenuation

                # Check for horizon crossings
                if self._check_horizon_crossing(current_pos, dim):
                    # Apply refraction at horizon
                    momentum = self._refract_at_horizon(current_pos, momentum, dim)
                    attenuation += 0.1  # Additional attenuation at horizons

            # Add target node if not already included
            if target_node not in waypoints:
                waypoints.append(target_node)

            # Compute final metrics
            total_length = np.linalg.norm(target_pos - source_pos)
            attenuation = min(1.0, attenuation)

            computation_time = time.time() - start_time

            # Create path asset
            path_asset = PathAsset(
                waypoints=waypoints,
                length=total_length,
                action=total_action,
                attenuation=attenuation,
                positions=positions,
                computation_time=computation_time,
                success=converged
            )

            return path_asset

        except Exception as e:
            self.console.log(f"[ProximityEngine] Geodesic ray tracing failed: {e}")
            computation_time = time.time() - start_time
            return PathAsset(
                waypoints=[source_node, target_node],
                length=0.0,
                action=float('inf'),
                attenuation=1.0,
                success=False,
                computation_time=computation_time
            )

    def _compute_metric_tensor(self, position: np.ndarray, dim: int) -> np.ndarray:
        """
        Compute the metric tensor g_{�?} at a position.

        Uses field manifold information to construct an effective metric.
        """
        try:
            # Base Minkowski-like metric
            g_metric = np.eye(min(dim, 8))  # Limit to 8D for computational efficiency

            # Add field contributions
            if hasattr(self.mind, 'field_mantle') and self.mind.field_mantle:
                field_state = self.mind.field_mantle.get_shell_field_state(dim)
                if field_state:
                    energy_density = field_state.get('energy_density', 0.0)
                    # Modify metric based on field energy
                    field_factor = 1.0 + 0.1 * energy_density
                    g_metric = g_metric * field_factor

            return g_metric

        except Exception as e:
            self.console.log(f"[ProximityEngine] Metric tensor computation failed: {e}")
            return np.eye(min(dim, 8))

    def _compute_christoffel_symbols(self, position: np.ndarray, dim: int) -> np.ndarray:
        """
        Compute Christoffel symbols G^�_{a�} for geodesic equation.

        Simplified computation using finite differences.
        """
        try:
            epsilon = 1e-6
            dim_limit = min(dim, 8)
            christoffel = np.zeros((dim_limit, dim_limit, dim_limit))

            # Compute partial derivatives of metric using finite differences
            for mu in range(dim_limit):
                for alpha in range(dim_limit):
                    for beta in range(dim_limit):
                        # G^�_{a�} = (1/2) g^{�s} (?_a g_{�s} + ?_� g_{as} - ?_s g_{a�})

                        # Simplified: assume weak field approximation
                        # In weak field, Christoffel symbols are small
                        metric_variation = 0.01 * np.sin(np.sum(position[:3])) if len(position) >= 3 else 0.0
                        christoffel[mu][alpha][beta] = metric_variation

            return christoffel

        except Exception as e:
            self.console.log(f"[ProximityEngine] Christoffel symbols computation failed: {e}")
            return np.zeros((min(dim, 8), min(dim, 8), min(dim, 8)))

    def _find_nearest_node(self, position: np.ndarray, dim: int) -> Optional[str]:
        """
        Find the nearest node to a given position.
        """
        try:
            shell = self.mind.dimensional_shells.get(dim)
            if not shell or not hasattr(shell, 'vectors'):
                return None

            min_distance = float('inf')
            nearest_node = None

            for node_id, node_pos in shell.vectors.items():
                distance = np.linalg.norm(node_pos - position)
                if distance < min_distance:
                    min_distance = distance
                    nearest_node = node_id

            return nearest_node

        except Exception as e:
            self.console.log(f"[ProximityEngine] Nearest node finding failed: {e}")
            return None

    def _check_horizon_crossing(self, position: np.ndarray, dim: int) -> bool:
        """
        Check if position crosses a horizon boundary.
        """
        try:
            # Simplified horizon check based on field gradients
            if hasattr(self.mind, 'field_mantle') and self.mind.field_mantle:
                field_state = self.mind.field_mantle.get_shell_field_state(dim)
                if field_state:
                    energy_density = field_state.get('energy_density', 0.0)
                    # High energy density regions indicate horizon-like boundaries
                    return energy_density > 1.0
            return False

        except Exception as e:
            return False

    def _refract_at_horizon(self, position: np.ndarray, momentum: np.ndarray, dim: int) -> np.ndarray:
        """
        Apply Snell-like refraction at horizon crossing: n1�sin?1 = n2�sin?2

        Uses refractive indices and surface normal to compute refracted momentum.
        """
        try:
            # Get refractive indices for both sides of horizon
            normal = self._get_horizon_normal(position, dim)
            if normal is None:
                return momentum

            # Compute refractive indices
            n1 = self.compute_refractive_index(position - 1e-6 * normal, dim)  # Incident side
            n2 = self.compute_refractive_index(position + 1e-6 * normal, dim)  # Refracted side

            # Normalize momentum (incident direction)
            incident_dir = momentum / (np.linalg.norm(momentum) + 1e-12)

            # Compute angle of incidence
            cos_theta_i = -np.dot(incident_dir, normal)  # Normal points toward incident side
            cos_theta_i = np.clip(cos_theta_i, -1.0, 1.0)
            sin_theta_i = np.sqrt(1.0 - cos_theta_i**2)

            # Snell's law: n1 * sin(?1) = n2 * sin(?2)
            sin_theta_r = (n1 / n2) * sin_theta_i

            # Check for total internal reflection
            if sin_theta_r > 1.0:
                self.console.log(f"[ProximityEngine] Total internal reflection at horizon (n1={n1:.3f}, n2={n2:.3f})")
                # Try auxiliary gate
                return self._try_auxiliary_gate_refraction(position, momentum, dim)

            cos_theta_r = np.sqrt(1.0 - sin_theta_r**2)

            # Compute refracted direction
            # r = (n1/n2) * i + [(n1/n2)cos?_i - cos?_r] * n
            refracted_dir = (n1 / n2) * incident_dir + (n1 / n2 * cos_theta_i - cos_theta_r) * normal
            refracted_dir = refracted_dir / (np.linalg.norm(refracted_dir) + 1e-12)

            return refracted_dir

        except Exception as e:
            self.console.log(f"[ProximityEngine] Snell refraction failed: {e}")
            return momentum

    def _get_horizon_normal(self, position: np.ndarray, dim: int) -> Optional[np.ndarray]:
        """
        Compute horizon surface normal at a position.
        """
        try:
            # Use field gradients to determine horizon orientation
            if hasattr(self.mind, 'field_mantle') and self.mind.field_mantle:
                field_state = self.mind.field_mantle.get_shell_field_state(dim)
                if field_state:
                    E_field = field_state.get('E', np.zeros(dim))
                    B_field = field_state.get('B', np.zeros(dim))

                    # Normal is perpendicular to field gradient
                    gradient = E_field + B_field  # Simplified field gradient
                    if np.linalg.norm(gradient) > 1e-12:
                        normal = gradient / np.linalg.norm(gradient)
                        return normal

            # Fallback: radial normal from origin
            if len(position) >= 3:
                radial_dir = position[:3] / (np.linalg.norm(position[:3]) + 1e-12)
                return radial_dir

            return None

        except Exception as e:
            self.console.log(f"[ProximityEngine] Horizon normal computation failed: {e}")
            return None

    def _try_auxiliary_gate_refraction(self, position: np.ndarray, momentum: np.ndarray, dim: int) -> np.ndarray:
        """
        Try refraction through auxiliary gates when direct refraction fails.

        Uses alternative horizon crossing points with different refractive indices.
        """
        try:
            # Find nearby auxiliary gates
            auxiliary_gates = self._find_auxiliary_gates(position, dim)

            for gate_pos in auxiliary_gates:
                try:
                    # Try refraction through auxiliary gate
                    gate_momentum = self._refract_at_horizon(gate_pos, momentum, dim)

                    # Check if refraction succeeded (not total internal reflection)
                    if not np.array_equal(gate_momentum, momentum):
                        self.console.log(f"[ProximityEngine] Successful auxiliary gate refraction")
                        return gate_momentum

                except Exception:
                    continue

            # All auxiliary gates failed, return original momentum
            self.console.log(f"[ProximityEngine] All auxiliary gate refractions failed")
            return momentum

        except Exception as e:
            self.console.log(f"[ProximityEngine] Auxiliary gate refraction failed: {e}")
            return momentum

    def _find_auxiliary_gates(self, position: np.ndarray, dim: int, num_gates: int = 3) -> List[np.ndarray]:
        """
        Find auxiliary horizon crossing gates near a position.
        """
        try:
            gates = []

            # Generate auxiliary positions around the original
            for i in range(num_gates):
                # Create offset in different directions
                offset = np.random.normal(0, 0.1, size=len(position))
                gate_pos = position + offset
                gates.append(gate_pos)

            return gates

        except Exception as e:
            self.console.log(f"[ProximityEngine] Auxiliary gate finding failed: {e}")
            return []

    def apply_horizon_refraction_to_path(self, path_asset: PathAsset, dim: int) -> PathAsset:
        """
        Apply horizon refraction corrections to an existing path.

        Returns updated PathAsset with refraction effects.
        """
        try:
            if not path_asset.positions or len(path_asset.positions) < 2:
                return path_asset

            updated_positions = [path_asset.positions[0].copy()]
            updated_momentum = None
            shell_crossings = []
            horizon_gates = []

            for i in range(len(path_asset.positions) - 1):
                current_pos = path_asset.positions[i]
                next_pos = path_asset.positions[i + 1]

                # Compute direction
                direction = next_pos - current_pos
                direction_norm = np.linalg.norm(direction)

                if direction_norm < 1e-12:
                    continue

                direction = direction / direction_norm

                # Initialize momentum if not set
                if updated_momentum is None:
                    updated_momentum = direction
                else:
                    # Blend with previous momentum for smooth trajectory
                    updated_momentum = 0.8 * updated_momentum + 0.2 * direction
                    updated_momentum = updated_momentum / (np.linalg.norm(updated_momentum) + 1e-12)

                # Check for horizon crossing
                if self._check_horizon_crossing(current_pos, dim):
                    # Apply refraction
                    refracted_momentum = self._refract_at_horizon(current_pos, updated_momentum, dim)

                    # Record crossing
                    crossing_info = {
                        'position': current_pos.tolist(),
                        'shell_from': dim,
                        'shell_to': dim,  # Same shell, different region
                        'refraction_factor': np.linalg.norm(refracted_momentum) / np.linalg.norm(updated_momentum),
                        'gate': f"horizon_{len(shell_crossings)}"
                    }
                    shell_crossings.append(crossing_info)
                    horizon_gates.append(crossing_info['gate'])

                    updated_momentum = refracted_momentum

                # Update position with refracted momentum
                step_size = np.linalg.norm(next_pos - current_pos)
                new_pos = current_pos + step_size * updated_momentum
                updated_positions.append(new_pos)

            # Create updated path asset
            updated_asset = PathAsset(
                waypoints=path_asset.waypoints,
                length=path_asset.length,
                action=path_asset.action,
                attenuation=path_asset.attenuation,
                shell_crossings=shell_crossings,
                horizon_gates=horizon_gates,
                positions=updated_positions,
                computation_time=path_asset.computation_time,
                success=path_asset.success
            )

            return updated_asset

        except Exception as e:
            self.console.log(f"[ProximityEngine] Path refraction application failed: {e}")
            return path_asset

    def classify_path_proximity(self, path_asset: PathAsset, dim: int) -> Dict[str, Any]:
        """
        Classify proximity based on ray-traced path using optical path scoring.

        Returns classification with tier, score, and path-based statistics.
        """
        try:
            if not path_asset or not path_asset.success:
                return {'tier': 'raw', 'score': None, 'path_stats': None}

            # Compute path score: s = exp(-? S) * (1-A)
            lambda_param = float(os.getenv("E8_PATH_LAMBDA", "0.1"))  # Action sensitivity parameter
            path_score = path_asset.score

            # Apply online threshold calibration
            thresholds = self._get_adaptive_path_thresholds(dim)
            lock_threshold = thresholds['lock']
            strong_threshold = thresholds['strong']
            moderate_threshold = thresholds['moderate']

            # Classify based on score
            if path_score >= lock_threshold:
                tier = 'lock'
            elif path_score >= strong_threshold:
                tier = 'strong'
            elif path_score >= moderate_threshold:
                tier = 'moderate'
            else:
                tier = 'weak'

            # Compute percentile for detailed ranking
            percentile = self._compute_path_percentile(path_score, dim)

            # Semantic reranking with graph features
            reranked_score = self._apply_semantic_reranking(path_asset, path_score, dim)

            # Path-based statistics
            path_stats = {
                'path_score': float(path_score),
                'reranked_score': float(reranked_score),
                'action': float(path_asset.action),
                'attenuation': float(path_asset.attenuation),
                'length': float(path_asset.length),
                'waypoints': len(path_asset.waypoints),
                'crossings': len(path_asset.shell_crossings),
                'gates': path_asset.horizon_gates,
                'lock_threshold': float(lock_threshold),
                'strong_threshold': float(strong_threshold),
                'moderate_threshold': float(moderate_threshold),
                'lambda_param': float(lambda_param),
                'percentile': float(percentile),
                'computation_time_ms': path_asset.computation_time * 1000
            }

            return {
                'tier': tier,
                'score': float(reranked_score),
                'percentile': float(percentile),
                'stats': path_stats,
                'path_asset': path_asset
            }

        except Exception as e:
            self.console.log(f"[ProximityEngine] Path proximity classification failed: {e}")
            return {'tier': 'raw', 'score': None, 'path_stats': None}

    def _get_adaptive_path_thresholds(self, dim: int) -> Dict[str, float]:
        """
        Get adaptive thresholds for path-based classification.

        Uses moving quantiles calibrated from successful path scores.
        """
        try:
            # Get recent path scores for this dimension
            recent_scores = self._get_recent_path_scores(dim, window=50)

            if len(recent_scores) < 10:
                # Fallback to fixed thresholds
                return {
                    'lock': 0.7,
                    'strong': 0.5,
                    'moderate': 0.3
                }

            # Compute quantiles
            scores_array = np.array(recent_scores)
            lock_pct = float(os.getenv("E8_PATH_LOCK_PCT", "80"))  # Top 20%
            strong_pct = float(os.getenv("E8_PATH_STRONG_PCT", "60"))  # Top 40%
            moderate_pct = float(os.getenv("E8_PATH_MODERATE_PCT", "30"))  # Top 70%

            lock_threshold = float(np.percentile(scores_array, lock_pct))
            strong_threshold = float(np.percentile(scores_array, strong_pct))
            moderate_threshold = float(np.percentile(scores_array, moderate_pct))

            return {
                'lock': lock_threshold,
                'strong': strong_threshold,
                'moderate': moderate_threshold
            }

        except Exception as e:
            self.console.log(f"[ProximityEngine] Adaptive threshold computation failed: {e}")
            return {'lock': 0.7, 'strong': 0.5, 'moderate': 0.3}

    def _get_recent_path_scores(self, dim: int, window: int = 50) -> List[float]:
        """
        Get recent successful path scores for threshold calibration.
        """
        try:
            # This would typically come from a history buffer
            # For now, return some reasonable default scores
            if not hasattr(self, '_path_score_history'):
                self._path_score_history = {d: [] for d in self.shell_dims}

            history = self._path_score_history.get(dim, [])

            if len(history) < window:
                # Generate synthetic scores for bootstrapping
                base_scores = np.random.beta(2, 5, size=window)  # Skewed toward lower scores
                history.extend(base_scores.tolist())
                self._path_score_history[dim] = history[-window:]  # Keep only recent

            return history[-window:]

        except Exception as e:
            self.console.log(f"[ProximityEngine] Recent path scores retrieval failed: {e}")
            return [0.5] * 10  # Fallback

    def _compute_path_percentile(self, score: float, dim: int) -> float:
        """
        Compute percentile ranking for a path score.
        """
        try:
            recent_scores = self._get_recent_path_scores(dim)
            if not recent_scores:
                return 50.0

            scores_array = np.array(recent_scores)
            percentile = float(np.sum(scores_array <= score) / len(scores_array) * 100)

            return percentile

        except Exception as e:
            return 50.0

    def _apply_semantic_reranking(self, path_asset: PathAsset, base_score: float, dim: int) -> float:
        """
        Apply semantic reranking based on graph coherence and novelty features.
        """
        try:
            rerank_factor = 1.0

            # Coherence bonus: paths with more crossings may be more meaningful
            coherence_bonus = min(0.2, len(path_asset.shell_crossings) * 0.05)
            rerank_factor += coherence_bonus

            # Novelty bonus: longer paths through diverse regions
            waypoint_diversity = len(set(path_asset.waypoints)) / len(path_asset.waypoints)
            novelty_bonus = (waypoint_diversity - 0.5) * 0.1  # Bonus for diversity > 0.5
            rerank_factor += novelty_bonus

            # Attenuation penalty: high attenuation reduces score
            attenuation_penalty = path_asset.attenuation * 0.3
            rerank_factor -= attenuation_penalty

            # Gate usage bonus: successful horizon crossings
            gate_bonus = min(0.15, len(path_asset.horizon_gates) * 0.05)
            rerank_factor += gate_bonus

            # Apply reranking
            reranked_score = base_score * rerank_factor

            # Clamp to reasonable range
            reranked_score = np.clip(reranked_score, 0.0, 1.0)

            return float(reranked_score)

        except Exception as e:
            self.console.log(f"[ProximityEngine] Semantic reranking failed: {e}")
            return base_score

    def update_path_score_history(self, path_asset: PathAsset, dim: int):
        """
        Update the path score history for threshold calibration.
        """
        try:
            if not hasattr(self, '_path_score_history'):
                self._path_score_history = {d: [] for d in self.shell_dims}

            if path_asset.success and path_asset.score > 0:
                history = self._path_score_history.get(dim, [])
                history.append(path_asset.score)

                # Keep only recent scores
                max_history = int(os.getenv("E8_PATH_HISTORY_SIZE", "100"))
                self._path_score_history[dim] = history[-max_history:]

        except Exception as e:
            self.console.log(f"[ProximityEngine] Path score history update failed: {e}")

    def get_cached_path(self, source_node: str, target_node: str, dim: int) -> Optional[PathAsset]:
        """
        Retrieve cached path between two nodes.

        Returns PathAsset if cached and valid, None otherwise.
        """
        try:
            cache_key = self._make_cache_key(source_node, target_node, dim)
            # make per-dim key (cache stores entries per-dimension)
            dim_cache = self.ray_cache.get(dim)
            if not dim_cache:
                return None
            cached_asset = dim_cache.get(cache_key)
            if cached_asset is None:
                return None
            # Check if cache is still valid
            if self._is_cache_valid(cached_asset, dim):
                self.ray_stats['cache_hits'] += 1
                return cached_asset
            else:
                # Remove invalid cache entry
                try:
                    dim_cache.store.pop(cache_key, None)
                except Exception:
                    pass
            return None

        except Exception as e:
            self.console.log(f"[ProximityEngine] Cache retrieval failed: {e}")
            return None

    def cache_path(self, path_asset: PathAsset, source_node: str, target_node: str, dim: int):
        """
        Cache a computed path asset.

        Implements LRU eviction when cache size exceeds limit.
        """
        try:
            if not path_asset or not path_asset.success:
                return

            cache_key = self._make_cache_key(source_node, target_node, dim)

            cache_key = (cache_key[0], cache_key[1]) if isinstance(cache_key, tuple) and len(cache_key) >= 2 else cache_key
            dim_cache = self.ray_cache.get(dim)
            if dim_cache is None:
                dim_cache = HawkingCache(budget=self.ray_cache_max_size)
                self.ray_cache[dim] = dim_cache

            # Put into probabilistic Hawking cache
            try:
                dim_cache.put(cache_key, path_asset)
            except Exception:
                try:
                    dim_cache.store[cache_key] = (path_asset, time.time(), 1)
                except Exception:
                    pass

        except Exception as e:
            self.console.log(f"[ProximityEngine] Path caching failed: {e}")

    def _make_cache_key(self, source_node: str, target_node: str, dim: int) -> Tuple[str, str, int]:
        """
        Create ordered cache key for node pair.
        """
        # Ensure consistent ordering
        if source_node <= target_node:
            return (source_node, target_node, dim)
        else:
            return (target_node, source_node, dim)

    def _is_cache_valid(self, path_asset: PathAsset, dim: int) -> bool:
        """
        Check if cached path is still valid.

        Invalidates cache on shell rebuilds or large curvature changes.
        """
        try:
            # Check computation time (invalidate old caches)
            max_cache_age = float(os.getenv("E8_CACHE_MAX_AGE", "3600"))  # 1 hour default
            if time.time() - path_asset.computation_time > max_cache_age:
                return False

            # Check for shell rebuilds
            shell = self.mind.dimensional_shells.get(dim)
            if shell and hasattr(shell, '_last_rebuild_time'):
                last_rebuild = getattr(shell, '_last_rebuild_time', 0)
                if path_asset.computation_time < last_rebuild:
                    return False

            # Check for significant field changes
            if hasattr(self.mind, 'field_mantle') and self.mind.field_mantle:
                current_energy = self._get_current_field_energy(dim)
                cached_energy = getattr(path_asset, '_cached_energy', current_energy)

                energy_change_threshold = float(os.getenv("E8_CACHE_ENERGY_THRESHOLD", "0.1"))
                if abs(current_energy - cached_energy) > energy_change_threshold:
                    return False

            return True

        except Exception as e:
            self.console.log(f"[ProximityEngine] Cache validation failed: {e}")
            return False

    def _evict_lru_cache_entry(self, dim: int):
        """
        Evict least recently used cache entry for this dimension.
        """
        try:
            # Find oldest entry for this dimension using its HawkingCache store
            dim_cache = self.ray_cache.get(dim)
            if not dim_cache:
                return
            oldest_key = None
            oldest_time = float('inf')
            for key, entry in dim_cache.store.items():
                asset = entry[0] if isinstance(entry, tuple) else entry
                try:
                    ct = getattr(asset, 'computation_time', 0)
                except Exception:
                    ct = 0
                if ct < oldest_time:
                    oldest_time = ct
                    oldest_key = key

            if oldest_key:
                dim_cache.store.pop(oldest_key, None)
                self.console.log(f"[ProximityEngine] Evicted LRU cache entry: {oldest_key}")

        except Exception as e:
            self.console.log(f"[ProximityEngine] LRU eviction failed: {e}")

    def _get_current_field_energy(self, dim: int) -> float:
        """
        Get current field energy density for cache validation.
        """
        try:
            if hasattr(self.mind, 'field_mantle') and self.mind.field_mantle:
                field_state = self.mind.field_mantle.get_shell_field_state(dim)
                if field_state:
                    return field_state.get('energy_density', 0.0)
            return 0.0

        except Exception as e:
            return 0.0

    def invalidate_dimension_cache(self, dim: int):
        """
        Invalidate all cached paths for a specific dimension.
        """
        try:
            dim_cache = self.ray_cache.get(dim)
            if not dim_cache:
                self.console.log(f"[ProximityEngine] Invalidated 0 cache entries for dim {dim}")
                return
            removed = len(dim_cache.store)
            self.ray_cache[dim] = HawkingCache(budget=self.ray_cache_max_size)
            self.console.log(f"[ProximityEngine] Invalidated {removed} cache entries for dim {dim}")

        except Exception as e:
            self.console.log(f"[ProximityEngine] Cache invalidation failed: {e}")

    def clear_path_cache(self):
        """
        Clear all cached paths.
        """
        try:
            cache_size = sum(len(c.store) for c in self.ray_cache.values())
            self.ray_cache = {dim: HawkingCache(budget=self.ray_cache_max_size) for dim in self.shell_dims}
            self.console.log(f"[ProximityEngine] Cleared {cache_size} cached paths")

        except Exception as e:
            self.console.log(f"[ProximityEngine] Cache clearing failed: {e}")

    def get_cache_statistics(self) -> Dict[str, Any]:
        """
        Get detailed cache statistics.
        """
        try:
            total_entries = 0
            dim_counts = {}
            total_computation_time = 0.0
            total_scores = []

            for dim, cache in self.ray_cache.items():
                for key, entry in cache.store.items():
                    asset = entry[0] if isinstance(entry, tuple) else entry
                    total_entries += 1
                    dim_counts[dim] = dim_counts.get(dim, 0) + 1
                    try:
                        total_computation_time += asset.computation_time
                        total_scores.append(asset.score)
                    except Exception:
                        pass

            avg_computation_time = total_computation_time / total_entries if total_entries > 0 else 0.0
            avg_score = np.mean(total_scores) if total_scores else 0.0

            return {
                'total_entries': total_entries,
                'dimension_counts': dim_counts,
                'avg_computation_time_ms': avg_computation_time * 1000,
                'avg_score': float(avg_score),
                'cache_hit_rate': self._compute_cache_hit_rate(),
                'max_size': self.ray_cache_max_size,
                'utilization_percent': (total_entries / (max(1, self.ray_cache_max_size))) * 100,
                'ray_stats': self.ray_stats
            }

        except Exception as e:
            self.console.log(f"[ProximityEngine] Cache statistics computation failed: {e}")
            return {}

    def _compute_cache_hit_rate(self) -> float:
        """
        Compute cache hit rate from ray statistics.
        """
        try:
            total_requests = self.ray_stats['cache_hits'] + self.ray_stats.get('cache_misses', 0)
            if total_requests == 0:
                return 0.0
            return self.ray_stats['cache_hits'] / total_requests

        except Exception as e:
            return 0.0

    def compute_ray_based_proximity(self, source_node: str, target_node: str, dim: int,
                                   distance: float) -> Dict[str, Any]:
        """
        Main entry point for ray-based proximity computation.

        Attempts ray tracing first, falls back to percentile-based scoring on failure.
        """
        start_time = time.time()

        try:
            # Check if ray-based proximity is enabled
            if not self.use_field_rays:
                return self._fallback_percentile_classification(dim, distance)

            # Try cache first
            cached_path = self.get_cached_path(source_node, target_node, dim)
            if cached_path:
                # Use cached path
                classification = self.classify_path_proximity(cached_path, dim)
                computation_time = time.time() - start_time
                self.ray_stats['latency_total'] += computation_time

                result = {
                    'method': 'cached_ray',
                    'classification': classification,
                    'computation_time_ms': computation_time * 1000,
                    'cached': True
                }
                # Opportunistic compact Ray Alert when decisive
                try:
                    tier = (classification or {}).get('tier') if classification else None
                    if tier in ('lock', 'strong') and E8_UI_RAY_ALERTS:
                        a_label = str(source_node)
                        b_label = str(target_node)
                        try:
                            if hasattr(self, 'mind') and getattr(self.mind, 'memory', None):
                                A = self.mind.memory.graph_db.get_node(source_node) or {}
                                B = self.mind.memory.graph_db.get_node(target_node) or {}
                                a_label = A.get('label') or a_label
                                b_label = B.get('label') or b_label
                        except Exception:
                            pass
                        bs = bt = None
                        try:
                            shell = getattr(self.mind, 'dimensional_shells', {}).get(dim)
                            if shell and getattr(shell, 'vectors', None):
                                bs = shell.vectors.get(source_node)
                                bt = shell.vectors.get(target_node)
                        except Exception:
                            bs = bt = None
                        entry = build_alert_entry(
                            source_label=a_label,
                            target_label=b_label,
                            source_dim=int(dim),
                            target_dim=int(dim),
                            distance=float(distance) if isinstance(distance, (int, float)) else None,
                            tier=tier,
                            score=(classification or {}).get('score'),
                            bulk_src=bs.tolist() if hasattr(bs, 'tolist') else bs,
                            bulk_tgt=bt.tolist() if hasattr(bt, 'tolist') else bt,
                            extra={'method': 'cached_ray', 'cached': True, 'geo_len': getattr(cached_path, 'length', None)},
                        )
                        try:
                            render_ray_alert(entry, console=getattr(self.mind, 'console', None))
                        except Exception:
                            render_ray_alert(entry, console=None)
                except Exception:
                    pass
                return result

            # Compute new path using ray tracing
            path_asset = self._compute_optimal_path(source_node, target_node, dim)

            if path_asset and path_asset.success:
                # Successful ray computation
                self.ray_stats['success'] += 1

                # Cache the successful path
                self.cache_path(path_asset, source_node, target_node, dim)

                # Update score history for threshold calibration
                self.update_path_score_history(path_asset, dim)

                # Classify proximity
                classification = self.classify_path_proximity(path_asset, dim)

                computation_time = time.time() - start_time
                self.ray_stats['latency_total'] += computation_time

                result = {
                    'method': 'ray_tracing',
                    'classification': classification,
                    'computation_time_ms': computation_time * 1000,
                    'cached': False
                }
                # Opportunistic compact Ray Alert when decisive
                try:
                    tier = (classification or {}).get('tier') if classification else None
                    if tier in ('lock', 'strong') and E8_UI_RAY_ALERTS:
                        a_label = str(source_node)
                        b_label = str(target_node)
                        try:
                            if hasattr(self, 'mind') and getattr(self.mind, 'memory', None):
                                A = self.mind.memory.graph_db.get_node(source_node) or {}
                                B = self.mind.memory.graph_db.get_node(target_node) or {}
                                a_label = A.get('label') or a_label
                                b_label = B.get('label') or b_label
                        except Exception:
                            pass
                        bs = bt = None
                        try:
                            shell = getattr(self.mind, 'dimensional_shells', {}).get(dim)
                            if shell and getattr(shell, 'vectors', None):
                                bs = shell.vectors.get(source_node)
                                bt = shell.vectors.get(target_node)
                        except Exception:
                            bs = bt = None
                        entry = build_alert_entry(
                            source_label=a_label,
                            target_label=b_label,
                            source_dim=int(dim),
                            target_dim=int(dim),
                            distance=float(distance) if isinstance(distance, (int, float)) else None,
                            tier=tier,
                            score=(classification or {}).get('score'),
                            bulk_src=bs.tolist() if hasattr(bs, 'tolist') else bs,
                            bulk_tgt=bt.tolist() if hasattr(bt, 'tolist') else bt,
                            extra={'method': 'ray_tracing', 'cached': False, 'geo_len': getattr(path_asset, 'length', None)},
                        )
                        try:
                            render_ray_alert(entry, console=getattr(self.mind, 'console', None))
                        except Exception:
                            render_ray_alert(entry, console=None)
                except Exception:
                    pass
                return result

            else:
                # Ray computation failed
                self.ray_stats['fail'] += 1
                self.console.log(f"[ProximityEngine] Ray tracing failed for {source_node} -> {target_node} in dim {dim}")

                # Check if we should switch to monitor-only mode
                success_rate = self.get_ray_success_rate()
                if success_rate < self.ray_success_threshold and not self.ray_monitor_only:
                    self.console.log(f"[ProximityEngine] Ray success rate {success_rate:.3f} below threshold {self.ray_success_threshold:.3f}, switching to monitor-only mode")
                    self.ray_monitor_only = True

                # Fall back to percentile-based classification
                ret = self._fallback_percentile_classification(dim, distance)
                try:
                    cls = ret.get('classification') if isinstance(ret, dict) else None
                    tier = (cls or {}).get('tier') if cls else None
                    if tier in ('lock', 'strong') and E8_UI_RAY_ALERTS:
                        entry = build_alert_entry(
                            source_label=str(source_node),
                            target_label=str(target_node),
                            source_dim=int(dim),
                            target_dim=int(dim),
                            distance=float(distance) if isinstance(distance, (int, float)) else None,
                            tier=tier,
                            score=(cls or {}).get('score'),
                            extra={'method': 'percentile_fallback'},
                        )
                        try:
                            render_ray_alert(entry, console=getattr(self.mind, 'console', None))
                        except Exception:
                            render_ray_alert(entry, console=None)
                except Exception:
                    pass
                return ret

        except Exception as e:
            self.console.log(f"[ProximityEngine] Ray-based proximity computation failed: {e}")
            self.ray_stats['fail'] += 1
            return self._fallback_percentile_classification(dim, distance)

    def _compute_optimal_path(self, source_node: str, target_node: str, dim: int) -> Optional[PathAsset]:
        """
        Compute optimal path using available ray tracing methods.

        By default tries global curvature-weighted shortest path first (configurable),
        then eikonal solver, then physics geodesic integrator as final fallback.
        """
        try:
            return self._get_tracer().trace(source_node, target_node, dim)
        except Exception as e:
            try:
                self.console.log(f"[ProximityEngine] Optimal path computation failed: {e}")
            except Exception:
                pass
            return None

    def _fallback_percentile_classification(self, dim: int, distance: float) -> Dict[str, Any]:
        """
        Fallback to traditional percentile-based proximity classification.
        """
        try:
            # Use existing classification method
            classification = self.classify(dim, distance)

            computation_time = time.time() - time.time()  # Minimal time for fallback

            result = {
                'method': 'percentile_fallback',
                'classification': classification,
                'computation_time_ms': computation_time * 1000,
                'cached': False
            }

            return result

        except Exception as e:
            self.console.log(f"[ProximityEngine] Percentile fallback failed: {e}")
            return {
                'method': 'error_fallback',
                'classification': {'tier': 'raw', 'score': None, 'stats': None},
                'computation_time_ms': 0.0,
                'cached': False
            }

    def should_use_ray_proximity(self) -> bool:
        """
        Determine if ray-based proximity should be used.

        Considers success rate, monitor-only mode, and other factors.
        """
        try:
            if not self.use_field_rays:
                return False

            if self.ray_monitor_only:
                # In monitor-only mode, still compute rays but don't use for decisions
                return False

            success_rate = self.get_ray_success_rate()
            return success_rate >= self.ray_success_threshold

        except Exception as e:
            self.console.log(f"[ProximityEngine] Ray proximity decision failed: {e}")
            return False

    def enable_ray_proximity(self):
        """
        Enable ray-based proximity computation.
        """
        self.use_field_rays = True
        self.ray_monitor_only = False
        self.console.log("[ProximityEngine] Ray-based proximity enabled")

    def disable_ray_proximity(self):
        """
        Disable ray-based proximity computation.
        """
        self.use_field_rays = False
        self.console.log("[ProximityEngine] Ray-based proximity disabled")

    def set_monitor_only_mode(self, enabled: bool = True):
        """
        Set monitor-only mode for ray-based proximity.
        """
        self.ray_monitor_only = enabled
        mode_str = "enabled" if enabled else "disabled"
        self.console.log(f"[ProximityEngine] Ray monitor-only mode {mode_str}")

    def reset_ray_statistics(self):
        """
        Reset ray tracing statistics.
        """
        self.ray_stats = {'success': 0, 'fail': 0, 'latency_total': 0, 'cache_hits': 0}
        self.console.log("[ProximityEngine] Ray statistics reset")

    # --- [END ADD] ---

class ShellAttention:
    def __init__(self, out_dim: int = 32, keep_k: int = 3):
        self.out_dim = int(out_dim)
        self.keep_k = int(max(1, keep_k))

    @staticmethod
    def _ten(vec: np.ndarray) -> np.ndarray:
        if vec is None or vec.size == 0:
            return np.zeros(10, dtype=np.float32)
        if vec.size >= 10:
            return vec[:10].astype(np.float32)
        out = np.zeros(10, dtype=np.float32)
        out[:vec.size] = vec.astype(np.float32)
        return out

    def _weights(self, tensions: dict, mood: "MoodEngine") -> dict:
        eps = 1e-6
        coh = float(mood.mood_vector.get("coherence", 0.5)) if hasattr(mood, "mood_vector") else 0.5
        raw = {d: coh / (eps + float(t)) for d,t in tensions.items()}
        if not raw: return {}
        xs = np.array(list(raw.values()), dtype=np.float32); xs = np.exp(xs - xs.max()); xs /= (xs.sum() + 1e-12)
        return {d: float(w) for d,w in zip(raw.keys(), xs.tolist())}

    def build(self, mind: "E8Mind", out_dim: Optional[int] = None, keep_k: Optional[int] = None) -> np.ndarray:
        out_dim = int(out_dim or self.out_dim); keep_k = int(keep_k or self.keep_k)
        tensions = {}
        for dim, shell in mind.dimensional_shells.items():
            try:
                M,_ = shell.get_all_vectors_as_matrix()
                if M is not None and M.shape[0] > 1: tensions[dim] = float(np.linalg.norm(M - M.mean(0), axis=1).mean())
                else: tensions[dim] = 0.0
            except Exception: tensions[dim] = 0.0
        ws = self._weights(tensions, mind.mood)
        top = sorted(ws.items(), key=lambda kv: -kv[1])[:keep_k]
        parts = []
        for dim, w in top:
            try:
                M,_ = mind.dimensional_shells[dim].get_all_vectors_as_matrix()
                v = M.mean(0).astype(np.float32) if (M is not None and M.size>0) else np.zeros(dim, dtype=np.float32)
            except Exception:
                v = np.zeros(max(1,int(dim)), dtype=np.float32)
            parts.append(self._ten(v) * float(w))
        head = np.concatenate(parts, axis=0) if parts else np.zeros(0, dtype=np.float32)
        need = 10*keep_k
        if head.size < need: head = np.pad(head, (0, need-head.size))
        elif head.size > need: head = head[:need]
        gten = float(sum(tensions.values())/len(tensions)) if tensions else 0.0
        coh = float(mind.mood.mood_vector.get("coherence", 0.5))
        out = np.concatenate([head.astype(np.float32), np.array([gten, coh], dtype=np.float32)])
        if out.size < out_dim: out = np.pad(out, (0, out_dim - out.size))
        elif out.size > out_dim: out = out[:out_dim]
        return out.astype(np.float32)

class ArbiterGate:
    def __init__(self):
        self._last_tv = 0.0
    def decide(self, telemetry: dict, mood_vec: dict) -> float:
        tv = float((telemetry or {}).get("tv", 0.0))
        norm = float((telemetry or {}).get("norm", 1.0))
        coh = float((mood_vec or {}).get("coherence", 0.5))
        ent = float((mood_vec or {}).get("entropy", 0.5))
        d_tv = tv - self._last_tv; self._last_tv = tv
        g = 0.5 + 0.35 * (0.3*coh - 0.7*max(0.0, d_tv)) - 0.15*abs(1.0 - norm) + 0.05*(0.5 - ent)
        return float(np.clip(g, 0.0, 1.0))

@dataclass
class AutoTask:
    id: str; label: str; reason: str; novelty: float; coherence: float; status: str = "pending"; created_step: int = 0

class AutoTaskManager:
    def __init__(self, console: Console):
        self.console = console; self.queue: list[AutoTask] = []
    def maybe_spawn(self, step: int, novelty: float, coherence: float, top_labels: list[str]):
        if novelty >= 1.10 and coherence <= 0.50:
            lid = f"task-{step}-{len(self.queue)+1}"
            label = (top_labels[0] if top_labels else "Consolidate new pattern")
            reason = f"Novelty {novelty:.2f} high, coherence {coherence:.2f} low. Add grounding task."
            t = AutoTask(id=lid, label=label, reason=reason, novelty=float(novelty), coherence=float(coherence), created_step=int(step))
            self.queue.append(t)
            try: self.console.log(f"[Curriculum] Spawned: {t.label} Â· {reason}")
            except Exception: pass
            return t
        return None
    def complete_if_related(self, node_label: str) -> float:
        for t in self.queue:
            if t.status == "pending" and node_label and (node_label.lower() in t.label.lower() or t.label.lower() in node_label.lower()):
                t.status = "done"
                return float(np.clip(0.15*(t.novelty - 0.8) + 0.15*(0.6 - t.coherence), 0.0, 0.5))
        return 0.0

class NoveltyScorer:
    def __init__(self, memory_manager: 'MemoryManager', llm_pool: 'AsyncLLMPool', console: Console):
        self.console = console
        self.memory_manager = memory_manager
        self.llm_pool = llm_pool

    def calculate_novelty(self, new_vector: np.ndarray) -> float:
        """Calculate novelty score in consistent 0.0-1.0 range"""
        similar_nodes = self.memory_manager.find_similar_in_main_storage(new_vector, k=1)
        if not similar_nodes:
            return 1.0  # Maximum novelty for completely unique vectors
            
        distance_to_nearest = similar_nodes[0][1]
        avg_distance = self.memory_manager.get_average_nearest_neighbor_distance()
        
        if avg_distance < 1e-6:
            return 1.0
            
        # Calculate raw novelty ratio
        novelty_ratio = distance_to_nearest / avg_distance
        
        # Map to 0.0-1.0 range using sigmoid-like function
        # This prevents extreme values while preserving relative ordering
        novelty_score = 2.0 / (1.0 + np.exp(-novelty_ratio)) - 1.0  # Maps [0,inf) -> [0,1)
        
        return float(np.clip(novelty_score, 0.0, 1.0))

    async def calculate_coherence(self, new_concept_text: str) -> float:
        if not new_concept_text: return 0.0
        prompt = (
            f'On a scale from 0.0 to 1.0, how coherent and meaningful is the following idea? '
            f'A coherent idea is well-formed, logical, and potentially useful. '
            f'Respond with ONLY the numeric score.\n\n'
            f'Idea: "{new_concept_text}"\n\n'
            f'Coherence Score:'
        )
        try:
            response = await self.llm_pool.enqueue_and_wait(prompt, max_tokens=10, temperature=0.1)
            if response is None:
                return 0.5  # failure fallback
            match = re.search(r"[-+]?\d*\.\d+|\d+", response)
            if match:
                return np.clip(float(match.group()), 0.0, 1.0)
            return 0.5
        except Exception as e:
            self.console.log(f"[NoveltyScorer] Coherence check failed: {e}")
            return 0.5

class EpisodicMemory:
    def __init__(self, max_size=500):
        self.max_size = max_size
        self.heap = []

    def add_episode(self, episode_data, reward):
        # Store reward explicitly; do not conflate with any rating field that may exist
        try:
            episode_data['reward'] = float(reward)
        except Exception:
            episode_data['reward'] = 0.0
        priority = -float(episode_data['reward'])
        heapq.heappush(self.heap, (priority, episode_data))
        if len(self.heap) > self.max_size:
            heapq.heappop(self.heap)

    def get_top_episodes(self, k=1):
        if not self.heap:
            return []
        top_k = heapq.nsmallest(k, self.heap)
        return [data for priority, data in top_k]

    def sample_prioritized(self, k=32, mind=None, alpha=1.0, beta=1.0, gamma=1.0, eta=0.6):
        if not self.heap:
            return []
        episodes = [ep for (_prio, ep) in self.heap]
        rewards, surprises, novelties = [], [], []
        for ep in episodes:
            # Only use reward. If missing, treat as neutral (0.0). Ratings are evaluative scores, not reinforcement.
            try:
                rewards.append(float(ep.get('reward', 0.0)))
            except Exception:
                rewards.append(0.0)
            try:
                if mind is not None and 'node_id' in ep and 'parent_ids' in ep:
                    child_vec = mind.memory.main_vectors.get(ep['node_id']) or ep.get('embedding')
                    parents = [mind.memory.main_vectors.get(pid) for pid in ep['parent_ids'] if pid in mind.memory.main_vectors]
                    if child_vec is not None and parents:
                        pmean = np.mean(np.stack(parents), axis=0)
                        surprises.append(float(np.linalg.norm(np.array(child_vec) - pmean)))
                    else:
                        surprises.append(0.0)
                else:
                    surprises.append(float(ep.get('surprise', 0.0)))
            except Exception:
                surprises.append(0.0)
            try:
                nov = float(ep.get('novelty', 0.0))
                if nov == 0.0 and mind is not None and 'node_id' in ep:
                    try:
                        sim = mind.memory.find_similar_in_main_storage(mind.memory.main_vectors.get(ep['node_id']), k=2)
                        d = 1.0 - (sim[0][1] if sim else 0.0)
                        nov = float(d)
                    except Exception: pass
                novelties.append(nov)
            except Exception:
                novelties.append(0.0)
        rewards = np.asarray(rewards, dtype=np.float64)
        surprises = np.asarray(surprises, dtype=np.float64)
        novelties = np.asarray(novelties, dtype=np.float64)
        pri = alpha*rewards + beta*surprises + gamma*novelties
        pri = np.maximum(pri, 1e-8)
        w = np.power(pri, eta)
        w = w / (w.sum() + 1e-12)
        idxs = np.random.choice(np.arange(len(episodes)), size=min(k, len(episodes)), replace=False, p=w)
        return [episodes[i] for i in idxs]

class InsightAgent:
    def __init__(self, llm_pool: 'AsyncLLMPool', novelty_scorer: NoveltyScorer, console: Console):
        self.console = console
        self.llm_pool = llm_pool
        self.novelty_scorer = novelty_scorer
        self.reward_history = deque(maxlen=100)
        self.episodic_memory = EpisodicMemory()

    class _InsightResult:
        def __init__(self, content: str, rating: float):
            self.content = content
            self.rating = rating

    def generate_insight_from_prompt(self, prompt: str):
        """Synchronous helper used by RecursiveArchitectureEngine.

        Attempts to synchronously obtain an LLM completion. If already inside a running
        event loop, we avoid blocking and return a placeholder (rating low) to maintain
        safety. This prevents attribute errors observed at call sites that expected
        this method to exist.
        """
        content = ""
        rating = 0.0
        # Try to run the llm prompt if we can do so synchronously
        try:
            import asyncio  # local import to avoid shadowing elsewhere
            can_call = hasattr(self.llm_pool, 'enqueue_and_wait')
            if can_call:
                try:
                    loop = asyncio.get_running_loop()
                    # Loop is running; schedule task but do not block
                    try:
                        loop.create_task(self.llm_pool.enqueue_and_wait(prompt, max_tokens=160, temperature=0.75))
                        # Provide non-blocking placeholder
                        content = "[deferred insight generation: async loop active]"
                        rating = 0.0
                    except Exception:
                        content = ""
                except RuntimeError:
                    # No running loop: safe to block
                    try:
                        content = asyncio.run(self.llm_pool.enqueue_and_wait(prompt, max_tokens=160, temperature=0.75))
                    except Exception as exc:
                        self.console.log(f"[InsightAgent] LLM generation failed: {exc}")
            else:
                content = ""
        except Exception as exc:
            try:
                self.console.log(f"[InsightAgent] generate_insight_from_prompt error: {exc}")
            except Exception:
                pass
        # Basic rating heuristic: attempt novelty scorer if available
        if content and not content.startswith('[deferred'):
            try:
                if hasattr(self.novelty_scorer, 'score_text_novelty'):
                    rating = float(self.novelty_scorer.score_text_novelty(content))
                    # Clamp into [0,1]
                    if not np.isfinite(rating):
                        rating = 0.0
                    rating = max(0.0, min(1.0, rating))
            except Exception:
                rating = 0.0
        return self._InsightResult(content=content if isinstance(content, str) else "", rating=rating)

    async def create_hybrid_concept(self, concept_a: Dict, concept_b: Dict) -> str:
        mind_instance = self.novelty_scorer.memory_manager.mind
        try:
            _, top_goal_desc = mind_instance.goal_field.get_top_goals(k=1)[0]
        except (IndexError, TypeError):
            top_goal_desc = "achieve greater understanding"
        subconscious_narrative = mind_instance.subconscious.narrative
        prompt = (
            f"You are a creative synthesizer of ideas. Your current high-level objective is to '{top_goal_desc}'.\n"
            f"The mind's current internal narrative is: \"{subconscious_narrative}\"\n\n"
            f"Synthesize the following two concepts into a single, novel idea that ADVANCES THE OBJECTIVE. "
            f"Describe the new idea in one or two clear sentences.\n\n"
            f"Concept A: '{concept_a.get('metaphor', concept_a.get('label', ''))}'\n"
            f"Concept B: '{concept_b.get('metaphor', concept_b.get('label', ''))}'\n\n"
            f"Hybrid Concept:"
        )
        new_concept_text = await self.llm_pool.enqueue_and_wait(prompt, max_tokens=100, temperature=0.85)
        if not isinstance(new_concept_text, str) or not new_concept_text.strip():
            self.console.log("[InsightAgent] LLM failed to generate hybrid concept (None), using synthetic fallback.")
            # Generate a simple synthetic hybrid concept
            label_a = concept_a.get('metaphor', concept_a.get('label', 'unknown concept A'))
            label_b = concept_b.get('metaphor', concept_b.get('label', 'unknown concept B'))
            return f"A synthesis combining aspects of {label_a} with {label_b} to advance understanding."
        return new_concept_text.strip()

    def learn_from_reward(self, reward: float, episode_data: Optional[Dict] = None):
        self.reward_history.append(reward)
        if len(self.reward_history) > 10:
            avg_reward = np.mean(self.reward_history)
            self.console.log(f"[InsightAgent] Average Insight Reward: {avg_reward:.3f}")
        if episode_data:
            self.episodic_memory.add_episode(episode_data, reward)
        # Metaphysical Compiler funnel: proposals on very-high-rated insights
        try:
            rating = float(reward)
        except Exception:
            rating = 0.0
        try:
            if rating >= 0.95:
                # Optionally check computationally_testable tag in episode_data
                testable = bool(episode_data.get('computationally_testable', False)) if isinstance(episode_data, dict) else False
                # Produce a conservative JSON proposal
                proposal = self._make_proposal_from_episode(episode_data or {}, testable=testable)
                # Ensure directory exists
                prop_dir = os.getenv('E8_PROPOSAL_PATH', '/mnt/data/proposals')
                try:
                    os.makedirs(prop_dir, exist_ok=True)
                except Exception:
                    prop_dir = '.'
                fname = os.path.join(prop_dir, f"proposal_{int(time.time())}_{hashlib.sha1(json.dumps(proposal).encode()).hexdigest()[:8]}.json")
                try:
                    with open(fname, 'w', encoding='utf-8') as f:
                        json.dump(proposal, f, ensure_ascii=False, indent=2)
                    try:
                        self.console.log(f"[MetaphysicalCompiler] Wrote proposal: {fname}")
                    except Exception:
                        pass
                except Exception as e:
                    try: self.console.log(f"[MetaphysicalCompiler] Failed to write proposal: {e}")
                    except Exception: pass

                # Run a short monitor-only sandbox experiment (non-destructive)
                try:
                    mind = getattr(self.novelty_scorer, 'memory_manager', None)
                    mind = getattr(mind, 'mind', None) if mind is not None else None
                    if mind is None:
                        # Try global fallback
                        mind = globals().get('MIND')
                    if mind is not None:
                        # Run sandbox in a thread to avoid blocking
                        threading.Thread(target=lambda: self._run_sandbox_monitor_only(mind, proposal), daemon=True).start()
                except Exception:
                    pass
        except Exception:
            pass

    def _make_proposal_from_episode(self, episode: Dict[str, Any], testable: bool = False) -> Dict[str, Any]:
        # Minimal conservative proposal format
        proposal = {
            'ts': _now_ts(),
            'type': 'param' if testable else 'source',
            'episode_summary': episode.get('summary') if isinstance(episode, dict) else str(episode),
            'computationally_testable': bool(testable),
            'meta': {
                'reward_history_mean': float(np.mean(self.reward_history)) if self.reward_history else 0.0,
            }
        }
        return proposal

    def _run_sandbox_monitor_only(self, mind, proposal: Dict[str, Any]):
        """Run a short monitor-only thought experiment (no geometry writes).

        We implement this conservatively by setting a temporary flag on the mind to bypass
        geometry commit hooks and run a short number of inference/loop steps. The sandbox
        does not mutate geometry or memory; it only replays evaluation steps and records
        telemetry to a local file under the proposals directory.
        """
        try:
            # Acquire a small monitor-only context
            steps = int(os.getenv('E8_SANDBOX_STEPS', '300'))
            prop_dir = os.getenv('E8_PROPOSAL_PATH', '/mnt/data/proposals')
            log_lines = []
            # Set monitor-only mode if component exists
            prev_flag = getattr(mind, '_monitor_only', False)
            try:
                setattr(mind, '_monitor_only', True)
            except Exception:
                pass
            try:
                for s in range(steps):
                    # Simple monitor operation: evaluate recent insights and compute simple metrics
                    recent = []
                    try:
                        recent = getattr(mind, 'get_recent_insights', lambda n=5: module_fallback_get_recent_insights(mind, n))(5)
                    except Exception:
                        recent = module_fallback_get_recent_insights(mind, 5)
                    avg_rating = float(np.mean([ri.get('rating', 0.0) for ri in recent])) if recent else 0.0
                    log_lines.append({'step': s, 'avg_rating': avg_rating})
                    time.sleep(0.005)
            finally:
                try:
                    setattr(mind, '_monitor_only', prev_flag)
                except Exception:
                    pass

            # Write sandbox telemetry
            try:
                os.makedirs(prop_dir, exist_ok=True)
                sname = os.path.join(prop_dir, f"sandbox_{int(time.time())}_{hashlib.sha1(json.dumps(proposal).encode()).hexdigest()[:8]}.ndjson")
                with open(sname, 'w', encoding='utf-8') as sf:
                    for ln in log_lines:
                        sf.write(json.dumps(ln) + '\n')
                try:
                    self.console.log(f"[MetaphysicalCompiler] Sandbox completed: {sname}")
                except Exception:
                    pass
            except Exception:
                pass
        except Exception:
            pass

class GraphDB:
    def __init__(self):
        if nx is None: raise ImportError("networkx library is required for GraphDB.")
        self.graph = nx.Graph()
    def add_node(self, node_id: str, **attrs):
        self.graph.add_node(node_id, **attrs)
    def add_edge(self, source_id: str, target_id: str, **attrs):
        self.graph.add_edge(source_id, target_id, **attrs)
    def get_node(self, node_id: str) -> Optional[Dict[str, Any]]:
        if self.graph.has_node(node_id):
            return self.graph.nodes[node_id]
        return None
    def get_neighbors(self, node_id: str) -> List[str]:
        if self.graph.has_node(node_id):
            return list(self.graph.neighbors(node_id))
        return []
    def compute_and_store_communities(self, partition_key: str = "community_id"):
        if nx_comm is None or self.graph.number_of_nodes() < 10:
            return
        try:
            communities = nx_comm.louvain_communities(self.graph, seed=GLOBAL_SEED)
            for i, community_nodes in enumerate(communities):
                for node_id in community_nodes:
                    if self.graph.has_node(node_id):
                        self.graph.nodes[node_id][partition_key] = i
            console.log(f"[GraphDB] Computed {len(communities)} communities.")
        except Exception as e:
            console.log(f"[GraphDB] Community detection failed: {e}")
    def increment_edge_weight(self, u, v, delta=0.1, min_w=0.0, max_w=10.0, **attrs):
        try:
            if not self.graph.has_edge(u, v):
                self.graph.add_edge(u, v, weight=max(min_w, delta), **attrs)
            else:
                w = float(self.graph[u][v].get('weight', 0.0)) + float(delta)
                self.graph[u][v]['weight'] = min(max_w, max(min_w, w))
                for k,val in attrs.items():
                    self.graph[u][v][k] = val
        except Exception as e:
            try: console.log(f"[GraphDB] increment_edge_weight failed: {e}")
            except Exception: pass

if TORCH_AVAILABLE:
    class GaussianActor(nn.Module):
        def __init__(self, state_dim, action_dim, max_action):
            super().__init__()
            self.l1 = nn.Linear(state_dim, 256)
            self.l2 = nn.Linear(256, 256)
            self.mu = nn.Linear(256, action_dim)
            self.log_std = nn.Linear(256, action_dim)
            self.max_action = float(max_action)
        def forward(self, state):
            h = torch.relu(self.l1(state))
            h = torch.relu(self.l2(h))
            mu = self.mu(h)
            log_std = torch.clamp(self.log_std(h), -5.0, 2.0)
            return mu, log_std
        def sample(self, state):
            mu, log_std = self.forward(state)
            std = torch.exp(log_std)
            normal = torch.distributions.Normal(mu, std)
            x_t = normal.rsample()
            y_t = torch.tanh(x_t)
            action = y_t * self.max_action
            log_prob = normal.log_prob(x_t) - torch.log(1 - y_t.pow(2) + 1e-6)
            log_prob = log_prob.sum(dim=1, keepdim=True)
            mu_action = torch.tanh(mu) * self.max_action
            return action, log_prob, mu_action

    class QCritic(nn.Module):
        def __init__(self, state_dim, action_dim):
            super().__init__()
            self.l1 = nn.Linear(state_dim + action_dim, 256)
            self.l2 = nn.Linear(256, 256)
            self.l3 = nn.Linear(256, 1)
        def forward(self, state, action):
            x = torch.cat([state, action], dim=-1)
            x = torch.relu(self.l1(x))
            x = torch.relu(self.l2(x))
            return self.l3(x)
else:
    class GaussianActor:
        def __init__(self, *a, **k): self.max_action = 1.0
        def sample(self, state):
            batch = state.shape[0] if hasattr(state, 'shape') else 1
            action = np.zeros((batch, 1), dtype=np.float32)
            log_prob = np.zeros((batch, 1), dtype=np.float32)
            return action, log_prob, action
    class QCritic:
        def __init__(self, *a, **k): pass
        def forward(self, state, action): return 0.0

class ReplayBuffer:
    def __init__(self, state_dim, action_dim, max_size=int(1e5)):
        self.max_size = int(max_size)
        self.ptr = 0
        self.size = 0
        self.state = np.zeros((self.max_size, state_dim), dtype=np.float32)
        self.action = np.zeros((self.max_size, action_dim), dtype=np.float32)
        self.next_state = np.zeros((self.max_size, state_dim), dtype=np.float32)
        self.reward = np.zeros((self.max_size, 1), dtype=np.float32)
        self.done = np.zeros((self.max_size, 1), dtype=np.float32)

    def add(self, state, action, next_state, reward, done):
        self.state[self.ptr] = state
        self.action[self.ptr] = action
        self.next_state[self.ptr] = next_state
        self.reward[self.ptr] = reward
        self.done[self.ptr] = float(done)
        self.ptr = (self.ptr + 1) % self.max_size
        self.size = min(self.size + 1, self.max_size)

    def sample(self, batch_size):
        ind = np.random.randint(0, self.size, size=batch_size)
        return (self.state[ind], self.action[ind], self.next_state[ind], self.reward[ind], self.done[ind])

class PrioritizedReplayBuffer:
    def __init__(self, state_dim, action_dim, max_size=int(1e5), alpha=0.6, beta_start=0.4, beta_frames=100000):
        self.max_size = int(max_size)
        self.ptr = 0
        self.size = 0
        self.alpha = float(alpha)
        self.beta_start = float(beta_start)
        self.beta_frames = int(beta_frames)
        self.frame = 1
        self.eps = 1e-6
        self.state = np.zeros((self.max_size, state_dim), dtype=np.float32)
        self.action = np.zeros((self.max_size, action_dim), dtype=np.float32)
        self.next_state = np.zeros((self.max_size, state_dim), dtype=np.float32)
        self.reward = np.zeros((self.max_size, 1), dtype=np.float32)
        self.done = np.zeros((self.max_size, 1), dtype=np.float32)
        self.priorities = np.zeros((self.max_size,), dtype=np.float32)

    def add(self, state, action, next_state, reward, done):
        max_prio = self.priorities.max() if self.size > 0 else 1.0
        self.state[self.ptr] = state
        self.action[self.ptr] = action
        self.next_state[self.ptr] = next_state
        self.reward[self.ptr] = reward
        self.done[self.ptr] = float(done)
        self.priorities[self.ptr] = max_prio
        self.ptr = (self.ptr + 1) % self.max_size
        self.size = min(self.size + 1, self.max_size)

    def sample(self, batch_size):
        prios = self.priorities[:self.size]
        probs = prios ** self.alpha
        probs /= probs.sum()
        indices = np.random.choice(self.size, batch_size, p=probs)
        beta = self.beta_start + (1.0 - self.beta_start) * min(1.0, self.frame / self.beta_frames)
        self.frame += 1
        weights = (self.size * probs[indices]) ** (-beta)
        weights /= weights.max()
        return (self.state[indices], self.action[indices], self.next_state[indices], self.reward[indices],
                self.done[indices], weights.reshape(-1,1).astype(np.float32), indices)

    def update_priorities(self, indices, td_errors):
        prios = np.abs(td_errors) + self.eps
        self.priorities[indices] = prios

if TORCH_AVAILABLE:
    class SACMPOAgent:
        def __init__(self, state_dim, action_dim, max_action, console=None, tau=0.005, use_per=True, device=None):
            self.state_dim = int(state_dim)
            self.action_dim = int(action_dim)
            self.max_action = float(max_action)
            self.console = console
            self.tau = float(tau)
            self.device = device or (torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu"))
            self.actor = GaussianActor(state_dim, action_dim, max_action).to(self.device)
            self.actor_old = GaussianActor(state_dim, action_dim, max_action).to(self.device)
            self.actor_old.load_state_dict(self.actor.state_dict())
            self.critics = nn.ModuleList([QCritic(state_dim, action_dim).to(self.device) for _ in range(4)])
            self.critics_target = nn.ModuleList([QCritic(state_dim, action_dim).to(self.device) for _ in range(4)])
            for i in range(4):
                self.critics_target[i].load_state_dict(self.critics[i].state_dict())
            self.active_critics = 2
            self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=3e-4)
            self.critic_opts = [torch.optim.Adam(self.critics[i].parameters(), lr=3e-4) for i in range(4)]
            self.log_alpha = torch.nn.Parameter(torch.tensor(0.0, device=self.device))
            self.alpha_opt = torch.optim.Adam([self.log_alpha], lr=3e-4)
            self.alpha_min, self.alpha_max = 1e-4, 1.0
            self.replay = PrioritizedReplayBuffer(state_dim, action_dim) if use_per else ReplayBuffer(state_dim, action_dim)
            self._train_steps = 0
            self.batch_size = 256
            self.gamma = 0.99
            self.bh_pressure = 0.0
            self.kl_beta = 0.01
            # internal stats
            self.last_loss_actor = 0.0
            self.last_loss_critic = 0.0
            self.last_alpha = 0.0

        def _soft_update(self, net, target):
            for p, tp in zip(net.parameters(), target.parameters()):
                tp.data.lerp_(p.data, self.tau)

        def set_active_critics(self, n: int):
            self.active_critics = int(max(1, min(4, n)))

        @property
        def alpha(self):
            a = float(self.log_alpha.exp().item())
            return float(max(self.alpha_min, min(self.alpha_max, a)))

        def _target_entropy(self):
            bh = float(max(0.0, min(1.5, self.bh_pressure)))
            base = -float(self.action_dim) * 0.60
            scale = 0.60 + 0.25 * bh
            return float(base * scale)

        def select_action(self, state, deterministic=False):
            s = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)
            with torch.no_grad():
                if deterministic:
                    mu, _ = self.actor.forward(s)
                    a = torch.tanh(mu) * self.max_action
                else:
                    a, _, _ = self.actor.sample(s)
            return safe_tensor_to_numpy(a.squeeze(0)).astype("float32")

        def store(self, state, action, next_state, reward, done):
            self.replay.add(state, action, next_state, reward, done)
        
        def update(self):
            """Performs a single SAC update step (multi-critic with optional PER).
            Safe to call frequently; returns early if buffer insufficient."""
            if getattr(self.replay, 'size', 0) < max(1000, self.batch_size):
                return
            self._train_steps += 1
            use_per = isinstance(self.replay, PrioritizedReplayBuffer)
            if use_per:
                state, action, next_state, reward, done, weights, indices = self.replay.sample(self.batch_size)
                w = torch.as_tensor(weights, dtype=torch.float32, device=self.device)
            else:
                state, action, next_state, reward, done = self.replay.sample(self.batch_size)
                w = torch.ones((self.batch_size,1), dtype=torch.float32, device=self.device)

            state      = torch.as_tensor(state, dtype=torch.float32, device=self.device)
            action     = torch.as_tensor(action, dtype=torch.float32, device=self.device)
            next_state = torch.as_tensor(next_state, dtype=torch.float32, device=self.device)
            reward     = torch.as_tensor(reward, dtype=torch.float32, device=self.device)
            done       = torch.as_tensor(done, dtype=torch.float32, device=self.device)

            with torch.no_grad():
                next_a, next_logp, _ = self.actor.sample(next_state)
                q_next = []
                for i in range(self.active_critics):
                    q_next.append(self.critics_target[i].forward(next_state, next_a))
                q_next = torch.stack(q_next, dim=0)  # (n, B, 1)
                q_min = torch.min(q_next, dim=0)[0]
                target_v = q_min - self.alpha * next_logp
                target_q = reward + (1.0 - done) * self.gamma * target_v

            # Critic updates
            td_errors_accum = []
            critic_losses = []
            for i in range(self.active_critics):
                qi = self.critics[i].forward(state, action)
                td = target_q - qi
                td_errors_accum.append(td.detach())
                loss_q = (w * td.pow(2)).mean()
                self.critic_opts[i].zero_grad()
                loss_q.backward()
                torch.nn.utils.clip_grad_norm_(self.critics[i].parameters(), 5.0)
                self.critic_opts[i].step()
                critic_losses.append(loss_q.item())

            self.last_loss_critic = float(np.mean(critic_losses))

            if use_per:
                # PER priority update (use mean abs td across active critics)
                td_stack = torch.stack(td_errors_accum, dim=0)  # (n,B,1)
                td_mean = td_stack.mean(dim=0).squeeze(-1)
                self.replay.update_priorities(indices, safe_tensor_to_numpy(td_mean.detach().abs()))

            # Delayed actor update every 2 steps
            if self._train_steps % 2 == 0:
                new_a, logp, _ = self.actor.sample(state)
                q_new = []
                for i in range(self.active_critics):
                    q_new.append(self.critics[i].forward(state, new_a))
                q_new = torch.stack(q_new, dim=0)
                q_min = torch.min(q_new, dim=0)[0]
                # KL regularization to old policy (approx via sampled actions)
                with torch.no_grad():
                    old_mu, old_log_std = self.actor_old.forward(state)
                new_mu, new_log_std = self.actor.forward(state)
                # analytic KL between two diagonal Gaussians BEFORE tanh
                kl = (old_log_std - new_log_std + (torch.exp(2*new_log_std) + (new_mu - old_mu).pow(2)) / torch.exp(2*old_log_std) - 1) * 0.5
                kl = kl.sum(dim=1, keepdim=True)
                actor_loss = (w * (self.alpha * logp - q_min + self.kl_beta * kl)).mean()
                self.actor_opt.zero_grad()
                actor_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 5.0)
                self.actor_opt.step()
                self.last_loss_actor = float(actor_loss.item())

                # Entropy temperature update
                target_entropy = self._target_entropy()
                alpha_loss = -(self.log_alpha * (logp.detach() + target_entropy)).mean()
                self.alpha_opt.zero_grad()
                alpha_loss.backward()
                self.alpha_opt.step()
                self.last_alpha = self.alpha

                # Update old policy (slow copy)
                self._soft_update(self.actor, self.actor_old)

            # Target critics soft update each step
            for i in range(self.active_critics):
                self._soft_update(self.critics[i], self.critics_target[i])

            # Adjust active critics gradually based on behavior pressure (optional heuristic)
            if self._train_steps % 500 == 0:
                # Increase critics if pressure high, else decrease
                if self.bh_pressure > 0.8 and self.active_critics < 4:
                    self.active_critics += 1
                elif self.bh_pressure < 0.2 and self.active_critics > 2:
                    self.active_critics -= 1

            if self.console and self._train_steps % 1000 == 0:
                try:
                    self.console.log(f"[sac] steps={self._train_steps} critics={self.active_critics} alpha={self.last_alpha:.4f} Lc={self.last_loss_critic:.3f} La={self.last_loss_actor:.3f}")
                except Exception:
                    pass
else:
    class SACMPOAgent:
        def __init__(self, *a, **k):
            self.available = False
            self.action_dim = int(k.get('action_dim', 0) or (a[1] if len(a) > 1 else 0))
        def select_action(self, state, deterministic: bool=False):
            return np.zeros(self.action_dim, dtype=np.float32)
        def store(self, *a, **k): pass
        def update(self): pass

class BaseAgentAdapter:
    def __init__(self, agent, name="base"):
        self.agent = agent; self.name = name
    async def select_action(self, state, mind):
        try: return self.agent.select_action(state)
        except Exception: return np.zeros(mind.action_dim, dtype=np.float32)

class ActionCandidateSampler:
    def __init__(self, mind, K=12, mag=0.04):
        self.mind = mind
        self.K = int(K)
        self.mag = float(mag)

    def sample(self):
        ad = int(getattr(self.mind, 'action_dim', 0))
        c = [np.zeros(ad, dtype=np.float32)]
        for i in range(min(self.K, ad)):
            v = np.zeros(ad, dtype=np.float32); v[i] = self.mag
            c.append(v); c.append(-v)
        return c[:max(1, self.K)]

class NoveltyAgent:
    def __init__(self, sampler): self.sampler=sampler; self.name="nov"
    async def select_action(self, state, mind):
        wm = getattr(mind, 'world_model', None); c = self.sampler.sample()
        best,score=None,-1e9
        for a in c:
            s = float(wm.score_transition(state,a)) if (wm and getattr(wm,'ready',False)) else float(np.linalg.norm(a))
            if s>score: best,score=a,s
        return best or c[0]

class StabilityAgent:
    def __init__(self, sampler):
        self.sampler = sampler
        self.name = "stab"
    async def select_action(self, state, mind):
        c = self.sampler.sample()
        idx = int(np.argmin([np.linalg.norm(a) for a in c]))
        return c[idx]

class SynthesisAgent:
    def __init__(self, sampler):
        self.sampler = sampler
        self.name = "syn"
    async def select_action(self, state, mind):
        c = self.sampler.sample()
        anchors = getattr(mind, 'anchors', None)
        target = None
        try:
            if anchors and getattr(anchors, 'anchors', None):
                vecs = [np.asarray(v, dtype=np.float32) for v, _ in anchors.anchors]
                if len(vecs) >= 2:
                    dmax, pair = -1.0, (vecs[0], vecs[0])
                    for i in range(len(vecs)):
                        for j in range(i + 1, len(vecs)):
                            d = float(np.linalg.norm(vecs[i] - vecs[j]))
                            if d > dmax:
                                dmax, pair = d, (vecs[i], vecs[j])
                    target = 0.5 * (pair[0] + pair[1])
        except Exception:
            target = None
        if target is None:
            return await NoveltyAgent(self.sampler).select_action(state, mind)
        wm = getattr(mind, 'world_model', None)
        def score(a):
            try:
                if wm and getattr(wm, 'ready', False):
                    return -abs(wm.score_transition(state, a))
            except Exception: pass
            return float(np.linalg.norm(a))
        idx = int(np.argmin([score(a) for a in c]))
        return c[idx]

def _softmax_b(values, beta: float):
    x=np.asarray(values, dtype=np.float64); x=x-x.max(); ex=np.exp(beta*x); s=ex.sum()
    return ex/s if s>0 and np.isfinite(s) else np.ones_like(ex)/len(ex)

class MetaArbiter:
    def __init__(self, agents: dict, drive_system, beta: float = 3.0, console=None, metrics=None):
        self.agents=agents; self.drive_system=drive_system; self.beta=float(beta); self.console=console; self.metrics=metrics
    def utilities(self, state, mind):
        ds=self.drive_system
        def _safe(fn, fallback):
            try: return float(fn(state))
            except Exception: return float(fallback)
        try:
            sims = mind.memory.find_similar_in_main_storage(state, k=5)
            nn = sims[0][1] if sims else 0.0
        except Exception: nn=0.25
        u_nov = _safe(getattr(ds,'novelty_need', lambda s: nn), nn)
        u_syn = _safe(getattr(ds,'synthesis_need', lambda s: nn), nn)
        u_stab= _safe(getattr(ds,'stability_need', lambda s: 0.3), 0.3)
        u_base= _safe(getattr(ds,'exploit_need', lambda s: 0.5*(1.0-u_nov)), 0.5*(1.0-u_nov))
        return {"nov":u_nov,"syn":u_syn,"stab":u_stab,"base":u_base}
    async def choose(self, state, mind):
        u=self.utilities(state, mind); names, vals=zip(*u.items()); probs=_softmax_b(vals, self.beta)
        idx=int(np.argmax(probs)); choice=names[idx]
        try: self.metrics.increment(f"society.usage.{choice}")
        except Exception: pass
        return self.agents[choice]
    async def step(self, state, mind):
        agent = await self.choose(state, mind)
        return await agent.select_action(state, mind)

class SocietyOfMind:
    def __init__(self, mind, beta=3.0, K=12):
        sampler = ActionCandidateSampler(mind, K=K)
        agents = {
            "base": BaseAgentAdapter(getattr(mind,'agent', None), name="base"),
            "nov": NoveltyAgent(sampler),
            "syn": SynthesisAgent(sampler),
            "stab": StabilityAgent(sampler),
        }
        self.arbiter = MetaArbiter(agents, getattr(mind,'drives', None) or getattr(mind,'drive_system', None), beta=beta, console=getattr(mind,'console',None), metrics=getattr(mind,'metrics',None))
    async def step(self, state, mind):
        return await self.arbiter.step(state, mind)

class Probe:
    def __init__(self, run_id):
        self.path = get_path("debug/probe.ndjson", run_id)
        os.makedirs(os.path.dirname(self.path), exist_ok=True)
        self._lock = asyncio.Lock()

    async def log(self, **kv):
        kv["ts"] = datetime.now(timezone.utc).isoformat()
        try:
            async with self._lock:
                with open(self.path, "a", encoding="utf-8") as f:
                    f.write(json.dumps(kv, ensure_ascii=False) + "\n")
        except Exception:
            pass

    def log_sync(self, **kv):
        kv["ts"] = datetime.now(timezone.utc).isoformat()
        try:
            with open(self.path, "a", encoding="utf-8") as f:
                f.write(json.dumps(kv, ensure_ascii=False) + "\n")
        except Exception:
            pass

def set_asyncio_exception_logger(probe: Probe):
    try:
        loop = asyncio.get_running_loop()
        def _handler(loop, context):
            msg = context.get("message", "")
            exc = context.get("exception")
            tb = "".join(traceback.format_exception(type(exc), exc, exc.__traceback__)) if exc else msg
            try:
                if loop.is_running() and not loop.is_closed():
                    asyncio.create_task(probe.log(ev="loop_exception", message=msg, traceback=tb))
                else:
                    probe.log_sync(ev="loop_exception", message=msg, traceback=tb)
            except Exception:
                probe.log_sync(ev="loop_exception", message=msg, traceback=tb)
        loop.set_exception_handler(_handler)
    except RuntimeError:
        pass

class InstrumentedLock:
    def __init__(self, name="lock", probe=None):
        self._lock = asyncio.Lock()
        self.name = name
        self.probe = probe
        self._t_acq = 0.0

    async def __aenter__(self):
        t0 = time.time()
        await self._lock.acquire()
        wait = (time.time() - t0) * 1000.0
        if self.probe:
            await self.probe.log(ev="lock_acquire", lock=self.name, wait_ms=round(wait,2))
        self._t_acq = time.time()
        return self

    async def __aexit__(self, exc_type, exc, tb):
        held = (time.time() - self._t_acq) * 1000.0
        if self.probe:
            await self.probe.log(ev="lock_release", lock=self.name, held_ms=round(held,2))
        self._lock.release()

class AsyncOpenAIClient:
    def __init__(self, api_key: str, console: Console):
        # Be tolerant of different openai package versions. Some installs
        # don't expose AsyncOpenAI. If import fails, keep a None client and
        # degrade to safe placeholders so the server can continue running.
        self.console = console
        self.client = None
        self.BadRequestError = Exception
        self._async_client = False
        try:
            from openai import AsyncOpenAI, BadRequestError
            self.client = AsyncOpenAI(api_key=api_key)
            self.BadRequestError = BadRequestError
            self._async_client = True
        except Exception:
            try:
                import openai
                # Newer/openai-python variants may expose a sync OpenAI client
                if hasattr(openai, 'OpenAI'):
                    try:
                        self.client = openai.OpenAI(api_key=api_key)
                        self._async_client = False
                    except Exception:
                        self.client = None
                else:
                    self.client = None
            except Exception:
                self.client = None

    async def chat(self, messages, model=None, max_tokens=None, temperature=None):
        try:
            if self.client is None or not self._async_client:
                # Degraded mode: no async OpenAI client available.
                try:
                    self.console.log("[LLM] OpenAI async client unavailable; returning placeholder response.")
                except Exception:
                    pass
                return "[LLM_UNAVAILABLE]"
            cc = await self.client.chat.completions.create(
                model=model, messages=messages, max_tokens=max_tokens, temperature=temperature)
            if cc.choices:
                # SDK response shape: choices[0].message.content
                try:
                    return (cc.choices[0].message.content or "").strip()
                except Exception:
                    return str(cc.choices[0])
            return "[LLM ERROR] No choices returned from API."
        except Exception as e:
            # Log the full traceback to understand the exact error
            self.console.log("[bold red]An unexpected error occurred in the OpenAI client:[/bold red]")
            self.console.print_exception(show_locals=True)
            return f"[LLM CRITICAL FAILURE] {type(e).__name__}: {e}"

    async def embedding(self, text, model=None, dimensions=None):
        try:
            if self.client is None or not self._async_client:
                # Degraded placeholder embedding: return zeros of the requested dim
                dim = int(dimensions or os.getenv('E8_DEFAULT_EMBED_DIM', '1536'))
                try:
                    self.console.log(f"[LLM] OpenAI embedding client unavailable; returning zero-vector dim={dim}.")
                except Exception:
                    pass
                return [0.0] * dim
            res = await self.client.embeddings.create(
                input=[text], model=model or "text-embedding-3-small", dimensions=dimensions)
            return res.data[0].embedding
        except Exception as e:
            self.console.log(f"[bold red]OpenAI Embedding Error: {e}[/bold red]")
            return np.zeros(EMBED_DIM).tolist()

    async def batch_embedding(self, texts, model=None, dimensions=None):
        try:
            res = await self.client.embeddings.create(
                input=texts, model=model or "text-embedding-3-small", dimensions=dimensions)
            return [d.embedding for d in res.data]
        except Exception as e:
            self.console.log(f"[bold red]OpenAI Batch Embedding Error: {e}[/bold red]")
            return [np.zeros(EMBED_DIM).tolist() for _ in texts]

class OllamaClient:
    def __init__(self, ollama_model: str, console: Console):
        if ollama is None:
            raise RuntimeError("Python package 'ollama' not installed. Please `pip install ollama`.")
        self.client = ollama.AsyncClient()
        self.model = ollama_model
        self.console = console

    async def chat(self, messages, **kwargs):
        try:
            res = await self.client.chat(model=self.model, messages=messages)
            return res["message"]["content"].strip()
        except Exception as e:
            # Log the full traceback to understand the exact error
            self.console.log("[bold red]An unexpected error occurred in the Ollama client:[/bold red]")
            self.console.print_exception(show_locals=True)
            return f"[LLM CRITICAL FAILURE] {type(e).__name__}: {e}"

    async def embedding(self, text, model=None, dimensions=None):
        try:
            res = await self.client.embeddings(model=model or self.model, prompt=text)
            emb = res["embedding"]
            if dimensions:
                if len(emb) > dimensions: emb = emb[:dimensions]
                elif len(emb) < dimensions: emb += [0.0] * (dimensions - len(emb))
            return emb
        except Exception as e:
            self.console.log(f"[bold red]Ollama Embedding Error: {e}[/bold red]")
            v = normalize_vector(np.random.standard_normal(EMBED_DIM).astype(np.float32))
            return v.tolist()

    async def batch_embedding(self, texts, model=None, dimensions=None):
        try:
            tasks = [self.embedding(t, model, dimensions) for t in texts]
            return await asyncio.gather(*tasks)
        except Exception as e:
            self.console.log(f"[bold red]Ollama Batch Embedding Error: {e}[/bold red]")
            out = []
            for _ in texts:
                v = normalize_vector(np.random.standard_normal(EMBED_DIM).astype(np.float32))
                out.append(v.tolist())
            return out

class GeminiClient:
    def __init__(self, api_key: str, model_name: str, console: Console):
        if genai is None:
            raise RuntimeError("google-generativeai is not installed. Please `pip install google-generativeai`.")
        if not api_key:
            raise ValueError("Gemini API key is required.")
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        self.console = console

    async def chat(self, messages, max_tokens=None, temperature=None, **kwargs):
        try:
            gemini_messages = []
            for msg in messages:
                role = "model" if msg.get("role") == "assistant" else "user"
                content = msg.get("content", "")
                if isinstance(content, list):
                    parts = [str(part) for part in content if part is not None]
                    if not parts:
                        parts = [""]
                else:
                    parts = [str(content)]
                gemini_messages.append({"role": role, "parts": parts})
            if len(gemini_messages) > 1:
                deduped = [gemini_messages[0]]
                for entry in gemini_messages[1:]:
                    if entry["role"] == deduped[-1]["role"]:
                        deduped[-1] = entry
                    else:
                        deduped.append(entry)
                gemini_messages = deduped
            generation_config = None
            try:
                generation_config = genai.types.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=temperature
                )
            except Exception:
                generation_config = None
            if hasattr(self.model, "generate_content_async"):
                response = await self.model.generate_content_async(
                    gemini_messages,
                    generation_config=generation_config
                )
            else:
                response = await asyncio.to_thread(
                    self.model.generate_content,
                    gemini_messages,
                    generation_config=generation_config
                )
            if hasattr(response, "resolve"):
                try:
                    await response.resolve()
                except Exception:
                    pass
            text_out = ""
            try:
                candidates = getattr(response, "candidates", []) or []
                for candidate in candidates:
                    content = getattr(candidate, "content", None)
                    parts = getattr(content, "parts", None) if content is not None else None
                    if not parts:
                        continue
                    chunk_list = []
                    for part in parts:
                        piece = getattr(part, "text", None)
                        if isinstance(piece, str):
                            chunk_list.append(piece)
                    if chunk_list:
                        text_out = "".join(chunk_list).strip()
                        if text_out:
                            break
                if not text_out and hasattr(response, "text"):
                    text_out = (getattr(response, "text", "") or "").strip()
            except Exception as parse_error:
                self.console.log(f"[bold red]Gemini Parse Error: {parse_error}[/bold red]")
                text_out = ""
            if not text_out:
                try:
                    finish_reason = None
                    candidates = getattr(response, "candidates", []) or []
                    if candidates:
                        finish_reason = getattr(candidates[0], "finish_reason", None)
                    self.console.log(f"[bold red]Gemini returned no text (finish_reason={finish_reason}).[/bold red]")
                except Exception:
                    pass
                return ""
            return text_out
        except Exception as e:
            self.console.log("[bold red]An unexpected error occurred in the Gemini client:[/bold red]")
            self.console.print_exception(show_locals=True)
            return f"[LLM CRITICAL FAILURE] {type(e).__name__}: {e}"


    async def embedding(self, text, model="models/embedding-001", **kwargs):
        try:
            result = await genai.embed_content_async(model=model, content=text, task_type="retrieval_document")
            return result['embedding']
        except Exception as e:
            self.console.log(f"[bold red]Gemini Embedding Error: {e}[/bold red]")
            return np.zeros(EMBED_DIM).tolist()

    async def batch_embedding(self, texts, model="models/embedding-001", **kwargs):
        try:
            result = await genai.embed_content_async(model=model, content=texts, task_type="retrieval_document")
            return result['embedding']
        except Exception as e:
            self.console.log(f"[bold red]Gemini Batch Embedding Error: {e}[/bold red]")
            return [np.zeros(EMBED_DIM).tolist() for _ in texts]

class AsyncLLMPool:
    def __init__(self, mind_instance, worker_count):
        self.mind = mind_instance
        self.queue = asyncio.Queue(maxsize=worker_count * 4)
        self.workers = []
        self.worker_count = worker_count
        self.lock = asyncio.Lock()
        # Internal result storage maps prompt_id -> structured record
        # Record schema:
        # {"ok": bool, "content": Optional[str|Any],
        #  "error_type": Optional[str], "error_message": Optional[str],
        #  "latency": float}
        self._results: Dict[int, Any] = {}
        self._next_id = 0
        self.running = True
        self._sem = asyncio.Semaphore(int(os.getenv('E8_MAX_INFLIGHT', '32')))
        # Degraded mode flag (set by resilience wrapper on failure)
        self.degraded = False
        self._last_degraded_reset = time.time()
        try:
            self._timeout = float(os.getenv('E8_LLM_TIMEOUT', '45'))
        except Exception:
            self._timeout = 45.0
        # Align pool timeout with underlying model call timeout if requested to avoid premature cancellation
        try:
            if os.getenv('E8_LLM_TIMEOUT_ALIGN', '1') == '1':
                if self._timeout < (LLM_CALL_TIMEOUT_SEC - 2):
                    adj = LLM_CALL_TIMEOUT_SEC + 5
                    self.mind.console.log(f"[LLM Pool] Adjusting pool timeout from {self._timeout:.1f}s to {adj}s to align with model timeout {LLM_CALL_TIMEOUT_SEC}s.")
                    self._timeout = adj
        except Exception:
            pass
        # Failure tracking
        self._consecutive_failures = 0
        self._total_failures = 0
        self._total_success = 0
        self._last_failure_type = None

    async def _worker(self):
        while self.running:
            try:
                prompt_id, prompt, args = await self.queue.get()
                if prompt_id is None:
                    self.queue.task_done()
                    break
                started = time.time()
                record: Dict[str, Any]
                try:
                    result = await asyncio.wait_for(
                        self.mind._async_call_llm_internal(prompt, **(args or {})),
                        timeout=POOL_WORKER_TIMEOUT
                    )
                    if isinstance(result, str):
                        txt = result.strip()
                        if not txt:
                            record = {"ok": False, "content": None, "error_type": "empty", "error_message": "LLM returned empty response", "latency": time.time() - started}
                        else:
                            record = {"ok": True, "content": txt, "latency": time.time() - started}
                    else:
                        # Non-string payload (e.g. embedding); treat as success if not None
                        if result is None:
                            record = {"ok": False, "content": None, "error_type": "none", "error_message": "LLM returned None", "latency": time.time() - started}
                        else:
                            record = {"ok": True, "content": result, "latency": time.time() - started}
                except asyncio.TimeoutError:
                    record = {"ok": False, "content": None, "error_type": "timeout", "error_message": f"Task exceeded {POOL_WORKER_TIMEOUT}s", "latency": time.time() - started}
                except asyncio.CancelledError:
                    record = {"ok": False, "content": None, "error_type": "cancelled", "error_message": "Worker cancelled", "latency": time.time() - started}
                    # Allow graceful loop exit
                    async with self.lock:
                        self._results[prompt_id] = record
                    self.queue.task_done()
                    break
                except Exception as e:
                    record = {"ok": False, "content": None, "error_type": "error", "error_message": f"{type(e).__name__}: {e}", "latency": time.time() - started}
                finally:
                    # Metrics logging (best-effort; avoid raising inside worker)
                    try:
                        if record.get("ok"):
                            metrics_log("llm.success", {"event": "llm", "latency": record.get("latency", 0.0)})
                        else:
                            metrics_log("llm.error", {"event": "llm", "type": record.get("error_type"), "message": (record.get("error_message") or "")[:120]})
                    except Exception:
                        pass
                    async with self.lock:
                        self._results[prompt_id] = record
                    self.queue.task_done()
            except asyncio.CancelledError:
                break

    async def start(self):
        if self.workers and any(not w.done() for w in self.workers): return
        self.running = True
        self.workers = [w for w in self.workers if not w.done()]
        for _ in range(self.worker_count - len(self.workers)):
            self.workers.append(asyncio.create_task(self._worker()))
        self.mind.console.log(f"[LLM POOL] Started {len(self.workers)} workers.")

    async def stop(self):
        self.running = False
        for _ in range(len(self.workers)):
            try:
                await self.queue.put((None, None, None))
            except asyncio.CancelledError:
                pass
        for worker in self.workers:
            worker.cancel()
        await asyncio.gather(*self.workers, return_exceptions=True)
        self.workers.clear()

    async def submit(self, prompt, **kwargs) -> int:
        async with self.lock:
            prompt_id = self._next_id; self._next_id += 1
            self._results[prompt_id] = None
        await self.queue.put((prompt_id, prompt, kwargs))
        return prompt_id

    async def get_result(self, prompt_id, timeout=None):
        start = time.time()
        timeout = timeout or POOL_RESULT_TIMEOUT
        while True:
            async with self.lock:
                result = self._results.get(prompt_id)
            if result is not None:
                async with self.lock:
                    if prompt_id in self._results:
                        del self._results[prompt_id]
                # Backward compatibility: if legacy string stored
                if isinstance(result, str):
                    if result.startswith('[LLM'):
                        return None
                    return result
                # Structured record path
                if isinstance(result, dict):
                    if result.get("ok"):
                        return result.get("content")
                    return None  # Failure sentinel
                return None
            if time.time() - start > timeout:
                raise asyncio.TimeoutError(f"Pool timeout for prompt_id {prompt_id}")
            await asyncio.sleep(0.01)

    async def enqueue_and_wait(self, prompt, **kwargs):
        import random, time as _time
        max_retries = int(os.getenv('E8_LLM_MAX_RETRIES', '1'))
        backoff_base = float(os.getenv('E8_LLM_RETRY_BACKOFF', '0.4'))
        pid = None

        async def _attempt(retry_index: int):
            nonlocal pid
            async def _inner():
                nonlocal pid
                pid = await self.submit(prompt, **kwargs)
                return await self.get_result(pid, timeout=self._timeout)
            async with self._sem:
                try:
                    return await asyncio.wait_for(_inner(), timeout=self._timeout)
                except Exception as e:
                    # Mark degraded only once per cycle for telemetry. Return None sentinel.
                    try:
                        self._consecutive_failures += 1
                        self._total_failures += 1
                        self._last_failure_type = type(e).__name__
                        if not self.degraded:
                            self.degraded = True
                            metrics_log("llm.degraded", {"event": "llm", "reason": str(e)[:80], "attempt": retry_index})
                            print(f"[LLM Pool] Degraded due to: {type(e).__name__}: {str(e)[:100]}")
                        else:
                            # Additional telemetry for consecutive failures
                            if self._consecutive_failures % 3 == 0:
                                metrics_log("llm.degraded.streak", {"event": "llm", "count": self._consecutive_failures, "last_error": self._last_failure_type})
                    except Exception:
                        pass
                    return None

        result = None
        for attempt in range(max_retries + 1):
            start_ts = _time.time()
            result = await _attempt(attempt)
            latency = _time.time() - start_ts
            if result is not None:
                # Success path bookkeeping
                self._consecutive_failures = 0
                self._total_success += 1
                if attempt > 0:
                    metrics_log("llm.retry.success", {"event": "llm", "attempt": attempt, "latency": latency})
                return result
            # Failure path: maybe retry
            if attempt < max_retries:
                # Lightweight stale result cleanup to avoid growth if earlier wait_for aborted
                try:
                    if len(self._results) > 500:
                        # prune oldest entries
                        excess = len(self._results) - 400
                        for k in list(self._results.keys())[:excess]:
                            self._results.pop(k, None)
                except Exception:
                    pass
                jitter = random.uniform(0.0, 0.5)
                delay = backoff_base * (1 + attempt) + jitter
                try:
                    metrics_log("llm.retry.scheduled", {"event": "llm", "attempt": attempt + 1, "delay": delay, "fail_type": self._last_failure_type})
                except Exception:
                    pass
                await asyncio.sleep(delay)
                # On repeated failures optionally relax temperature to increase variability
                try:
                    if 'temperature' in kwargs and attempt == 0:
                        kwargs['temperature'] = min(1.0, float(kwargs['temperature']) + 0.15)
                except Exception:
                    pass
                continue
            # No more retries
        return None

    def maybe_reset_degraded(self, force=False):
        """Reset degraded flag after some time has passed or if forced."""
        import time
        now = time.time()
        # Reset degraded flag every 300 seconds (5 minutes) or if forced
        if force or (self.degraded and (now - self._last_degraded_reset) > 300):
            if self.degraded:
                print(f"[LLM Pool] Resetting degraded flag after {now - self._last_degraded_reset:.1f} seconds")
                metrics_log("llm.recovered", {"event": "llm", "duration_seconds": now - self._last_degraded_reset})
            self.degraded = False
            self._last_degraded_reset = now

def neuro_to_engine(DA: float, NE: float, ACh: float, S5: float):
    DA, NE, ACh, S5 = np.clip([DA, NE, ACh, S5], 0.0, 1.0)
    sigma = float(np.clip(1.25 * (1.0 + 0.6*NE - 0.3*ACh + 0.3*S5), 0.8, 2.2))
    alpha_cur = float(np.clip(0.12 * (1.0 + 0.8*NE - 0.3*S5 + 0.4*DA), 0.02, 0.35))
    zeta = max(0.0, 0.03 * (1.0 + 0.7*S5 - 0.7*NE))
    sensory_gain = 1.0 + 0.8*ACh + 0.3*DA
    prior_gain = 1.0 + 0.6*S5 - 0.5*ACh
    phi0 = float(np.clip(0.10 * (1.0 + 0.6*DA + 0.3*NE - 0.2*S5), 0.02, 0.25))
    J = float(np.clip(0.08 * (1.0 + 0.5*NE + 0.2*ACh - 0.2*S5), 0.0, 0.2))
    return dict(sigma=sigma, alpha_cur=alpha_cur, zeta=zeta, sensory_gain=sensory_gain, prior_gain=prior_gain, phi0=phi0, J=J)

def theta_phase(step_idx: int, theta_len: int = 8):
    ph = step_idx % theta_len
    return ph, (ph < 5), (ph == 5), (ph == 6), (ph == 7)

class MPS:
    def __init__(self, M: int, d: int, chi: int = 8):
        self.M = int(M); self.d = int(d); self.chi = int(chi)
        self.A = []
        for k in range(M):
            chiL = 1 if k == 0 else chi
            chiR = 1 if k == M-1 else chi
            T = np.zeros((chiL, d, chiR), dtype=np.complex64)
            for i in range(min(chiL, chiR)):
                T[i, 0, i] = 1.0 + 0j
            self.A.append(T)

    def state_vector(self):
        v = self.A[0].reshape(-1, self.d * self.A[0].shape[-1])
        for k in range(1, self.M):
            T = self.A[k]
            v = v @ T.reshape(T.shape[0], -1)
            v = v.reshape(-1, T.shape[-1]*self.d)
        return v.reshape(-1)

def generate_e8_roots():
    roots = set()
    if combinations is None: return np.array([])
    for i, j in combinations(range(8), 2):
        for s1, s2 in [(1,1), (1,-1), (-1,1), (-1,-1)]:
            vec = [0]*8; vec[i], vec[j] = s1, s2
            roots.add(tuple(vec))
    for signs in range(2**8):
        vec, neg_count = [], 0
        for i in range(8):
            if (signs >> i) & 1: vec.append(-0.5); neg_count += 1
            else: vec.append(0.5)
        if neg_count % 2 == 0: roots.add(tuple(vec))
    return np.array(list(roots))

def verify_e8_roots(roots: np.ndarray, console=None) -> bool:
    """Verify E8 root system properties: a�� ? {-1,0,1} and |a|� = 2 for all roots."""
    try:
        if roots.size == 0:
            if console: console.log("[E8_VERIFY] No roots to verify")
            return False

        # Check squared lengths
        squared_norms = np.sum(roots * roots, axis=1)
        norm_check = np.allclose(squared_norms, 2.0, atol=1e-10)

        if not norm_check:
            max_norm_error = np.max(np.abs(squared_norms - 2.0))
            if console: console.log(f"[E8_VERIFY] Norm check failed: max error = {max_norm_error:.2e}")
            return False

        # Check inner products
        dot_products = roots @ roots.T
        np.fill_diagonal(dot_products, 0)  # Ignore self-products

        # Valid inner products should be in {-2, -1, 0, 1}
        valid_dots = np.isin(np.round(dot_products), [-2, -1, 0, 1])
        inner_product_check = np.all(valid_dots)

        if not inner_product_check:
            invalid_count = np.sum(~valid_dots)
            invalid_values = dot_products[~valid_dots]
            unique_invalid = np.unique(invalid_values)
            if console: console.log(f"[E8_VERIFY] Inner product check failed: {invalid_count} invalid pairs, values: {unique_invalid}")
            return False

        if console: console.log(f"[E8_VERIFY] ? E8 root system verified: {len(roots)} roots, all norms = 2.0, all inner products ? {{-1,0,1}}")
        return True

    except Exception as e:
        if console: console.log(f"[E8_VERIFY] Error during verification: {e}")
        return False

def verify_e8_brackets(roots: np.ndarray, console=None) -> bool:
    """Verify E8 Lie algebra structure constants via bracket relations."""
    try:
        if roots.size == 0:
            if console: console.log("[E8_BRACKET] No roots to verify")
            return False

        # For E8, verify that [a,�] = 2(a��)? where ? is another root
        # This is the fundamental relation for root systems

        n_roots = len(roots)
        bracket_matrix = np.zeros((n_roots, n_roots), dtype=int)

        for i in range(n_roots):
            for j in range(i+1, n_roots):
                alpha, beta = roots[i], roots[j]
                dot_product = np.dot(alpha, beta)

                if dot_product == 0:
                    # Orthogonal roots: [a,�] should be zero or parallel to another root
                    cross = np.cross(alpha[:3], beta[:3]) if alpha.shape[0] >= 3 and beta.shape[0] >= 3 else np.zeros(3)
                    bracket_matrix[i,j] = 0
                elif abs(dot_product) == 1:
                    # Adjacent roots: [a,�] = �2? where ? = (a + �)/2 or similar
                    gamma = (alpha + beta) / 2
                    # Check if gamma is in the root system (scaled appropriately)
                    gamma_scaled = 2 * gamma
                    distances = np.sum((roots - gamma_scaled)**2, axis=1)
                    min_dist = np.min(distances)
                    bracket_matrix[i,j] = 1 if min_dist < 1e-10 else 0
                else:
                    bracket_matrix[i,j] = -1  # Invalid

        bracket_matrix = bracket_matrix + bracket_matrix.T

        valid_brackets = np.sum(bracket_matrix == -1) == 0
        total_relations = n_roots * (n_roots - 1) // 2
        valid_relations = np.sum(bracket_matrix >= 0) // 2

        if console:
            console.log(f"[E8_BRACKET] Bracket verification: {valid_relations}/{total_relations} relations valid")

        return bool(valid_brackets), int(valid_relations), int(total_relations)

    except Exception as e:
        if console: console.log(f"[E8_BRACKET] Error during bracket verification: {e}")
        return False, 0, 0


def normalize_bracket_result(res) -> tuple:
    try:
        if isinstance(res, (np.bool_, bool)):
            return bool(res), 0, 0
        if isinstance(res, (tuple, list)):
            if len(res) == 3:
                b, v, t = res
                return bool(b), int(v), int(t)
            if len(res) == 2:
                v, t = res
                b = (int(t) > 0 and int(v) == int(t))
                return bool(b), int(v), int(t)
        return False, 0, 0
    except Exception:
        return False, 0, 0

def triacontagonal_projection(roots: np.ndarray, console=None) -> np.ndarray:
    """Project E8 roots to 3D using triacontagonal (30-gonal) symmetry."""
    try:
        if roots.size == 0:
            return np.array([])

        # Use golden ratio for triacontagonal symmetry
        phi = (1 + np.sqrt(5)) / 2

        # Projection matrix based on icosahedral/triacontagonal symmetry
        projection_matrix = np.array([
            [1, phi, 0, -phi, -1, 0, phi, 1],
            [phi, 0, -phi, -1, 0, phi, 1, -phi],
            [0, 1, phi, 0, -1, -phi, 0, 1]
        ], dtype=float)

        # Normalize columns
        projection_matrix = projection_matrix / np.linalg.norm(projection_matrix, axis=0)

        # Project roots
        projected = roots @ projection_matrix.T

        # Check uniqueness
        rounded = np.round(projected, 6)
        unique_coords = np.unique(rounded, axis=0)
        uniqueness_ratio = len(unique_coords) / len(roots)

        if console:
            console.log(f"[TRIACONTAGONAL] Projected {len(roots)} roots to 3D: uniqueness = {len(unique_coords)}/{len(roots)} ({uniqueness_ratio:.3f})")

        return projected

    except Exception as e:
        if console: console.log(f"[TRIACONTAGONAL] Error during projection: {e}")
        return np.array([])

def verify_e8_lie_algebra_structure(roots: np.ndarray, console=None) -> bool:
    """Verify E8 Lie algebra structure constants for so(16) ? S decomposition."""
    try:
        if roots.size == 0:
            if console: console.log("[E8_LIE] No roots to verify")
            return False

        # E8 decomposes as so(16) ? S where S is a 128-dimensional spinor representation
        # Verify structure constants [J_ij, J_kl] = d_jk J_il - d_jl J_ik - d_ik J_jl + d_il J_jk

        n_roots = len(roots)

        # Test commutator relations on a subset of roots
        test_indices = np.random.choice(n_roots, min(20, n_roots), replace=False)

        commutator_valid = True
        for i in test_indices:
            for j in test_indices:
                if i == j: continue

                alpha, beta = roots[i], roots[j]
                dot_product = np.dot(alpha, beta)

                # For simple roots, verify commutator relations
                if abs(dot_product) <= 1:
                    # [a, �] should be proportional to another root
                    commutator = 2 * dot_product * (alpha + beta) / 2  # Simplified form

                    # Check if commutator result is parallel to some root
                    commutator_norm = np.linalg.norm(commutator)
                    if commutator_norm > 1e-10:
                        # Normalize and check alignment with existing roots
                        commutator_unit = commutator / commutator_norm
                        similarities = np.abs(roots @ commutator_unit)
                        max_similarity = np.max(similarities)

                        if max_similarity < 0.99:  # Not well-aligned with any root
                            commutator_valid = False
                            if console: console.log(f"[E8_LIE] Commutator [{i},{j}] not aligned with roots: similarity = {max_similarity:.3f}")

        if console:
            status = "?" if commutator_valid else "?"
            console.log(f"[E8_LIE] {status} Lie algebra structure verification: commutator relations {'valid' if commutator_valid else 'invalid'}")

        return commutator_valid

    except Exception as e:
        if console: console.log(f"[E8_LIE] Error during Lie algebra verification: {e}")
        return False

def octonion_bracket_test(console=None) -> bool:
    """Test simplified octonionic bracket [x,y] = Im(\bar{x}y - \bar{y}x) for validation."""
    try:
        # Implement octonion arithmetic using Cayley-Dickson doubling (pair of quaternions)
        # Represent octonion as real 8-vector: (p0,p1,p2,p3, q0,q1,q2,q3) where
        # oct = p + q * e4 in Cayley-Dickson form (p,q quaternions)
        def q_mul(a, b):
            # quaternion multiply: a,b are length-4 arrays [w,x,y,z]
            a0, a1, a2, a3 = a
            b0, b1, b2, b3 = b
            return np.array([
                a0*b0 - a1*b1 - a2*b2 - a3*b3,
                a0*b1 + a1*b0 + a2*b3 - a3*b2,
                a0*b2 - a1*b3 + a2*b0 + a3*b1,
                a0*b3 + a1*b2 - a2*b1 + a3*b0
            ], dtype=float)

        def q_conj(a):
            return np.array([a[0], -a[1], -a[2], -a[3]], dtype=float)

        def oct_from_vec(v):
            v = np.asarray(v, dtype=float).reshape(8,)
            return v

        def oct_mul(a, b):
            a = np.asarray(a, dtype=float).reshape(8,)
            b = np.asarray(b, dtype=float).reshape(8,)
            p = a[:4]
            q = a[4:]
            r = b[:4]
            s = b[4:]
            pr = q_mul(p, r)
            term = q_mul(s, q_conj(q))
            first = pr - term
            second = q_mul(q_conj(p), s) + q_mul(r, q)
            return np.concatenate((first, second))

        def oct_conj(a):
            a = np.asarray(a, dtype=float).reshape(8,)
            p = a[:4]
            q = a[4:]
            return np.concatenate((q_conj(p), -q))

        def oct_bracket(x, y):
            # Return the imaginary/vector part of [x,y] = Im(conj(x)*y - conj(y)*x)
            a = oct_mul(oct_conj(x), y)
            b = oct_mul(oct_conj(y), x)
            diff = a - b
            # imaginary part is entries 1..7
            return diff[1:8]

        # Use real basis (scalar + 7 imaginary units)
        basis = [np.eye(8, dtype=float)[i] for i in range(8)]

        # Select a few representative imaginary unit pairs to check
        test_pairs = [(1,2), (1,3), (2,3), (4,5), (3,6), (2,6), (1,7)]

        bracket_valid = True
        nonzero_count = 0
        for i, j in test_pairs:
            x = basis[i]
            y = basis[j]
            v = oct_bracket(x, y)
            v_norm = float(np.linalg.norm(v))
            # antisymmetry check
            rev = oct_bracket(y, x)
            antisym_norm = float(np.linalg.norm(v + rev))
            if antisym_norm > 1e-9:
                bracket_valid = False
                if console: console.log(f"[OCTONION] Antisymmetry failed for [{i},{j}]: ||[x,y]+[y,x]||={antisym_norm:.2e}")
            if v_norm > 1e-9:
                nonzero_count += 1
            else:
                if console: console.log(f"[OCTONION] Bracket [{i},{j}] too small: norm = {v_norm:.2e}")

        if nonzero_count == 0:
            bracket_valid = False

        if console:
            console.log(f"[OCTONION] Octonion bracket test: {'passed' if bracket_valid else 'failed'} (nonzero_pairs={nonzero_count})")

        return bracket_valid

    except Exception as e:
        if console: console.log(f"[OCTONION] Error during octonion bracket test: {e}")
        return False
    R = roots.astype(np.float32); N = R.shape[0]
    mask = np.isclose(np.abs(R @ R.T), 1.0, atol=atol)
    np.fill_diagonal(mask, False)
    W = np.zeros((N, N), dtype=np.float32); W[mask] = 1.0
    int_roots = {tuple((2*r).astype(np.int8)) for r in R}
    for i in range(N):
        for j in np.where(mask[i])[0]:
            ri2, rj2 = (2*R[i]).astype(np.int8), (2*R[j]).astype(np.int8)
            s, d = tuple((ri2 + rj2).tolist()), tuple((ri2 - rj2).tolist())
            W[i, j] += 0.15 * (s in int_roots) + 0.10 * (d in int_roots)
    return W

def build_diff_adjacency(roots):
    R = roots.astype(np.float32); N = R.shape[0]
    int_roots = set(tuple((2*r).astype(np.int8)) for r in R)
    Wd = np.zeros((N, N), dtype=np.float32)
    for i in range(N):
        ri2 = tuple((2*R[i]).astype(np.int8))
        for j in range(N):
            if i == j: continue
            rj2 = tuple((2*R[j]).astype(np.int8))
            d = tuple(a - b for a, b in zip(ri2, rj2))
            if d in int_roots: Wd[i, j] = 1.0
    Wd = 0.5 * (Wd + Wd.T)
    np.fill_diagonal(Wd, 0.0)
    return Wd

def all_pairs_hops(A_bool):
    N = A_bool.shape[0]
    nbrs = [np.where(A_bool[i] > 0)[0] for i in range(N)]
    dist = np.full((N, N), np.inf, dtype=np.float32)
    for s in range(N):
        dist[s, s] = 0.0; q = deque([s])
        while q:
            u = q.popleft()
            for v in nbrs[u]:
                if dist[s, v] == np.inf:
                    dist[s, v] = dist[s, u] + 1.0
                    q.append(v)
    return dist

def weyl_average_potential(physics, anchors, draws=3, seed=None):
    rng = np.random.default_rng(seed)
    V_acc = np.zeros(physics.weights.shape[0], dtype=np.float32)
    def rand_sign_perm(rng):
        P = np.eye(8, dtype=np.float32); rng.shuffle(P)
        signs = rng.choice([-1.0, 1.0], size=(8,), replace=True).astype(np.float32)
        if (signs < 0).sum() % 2 == 1: signs[0] *= -1.0
        return (P.T * signs).T
    for _ in range(draws):
        A = rand_sign_perm(rng)
        transformed = []
        for (s, lam) in anchors.anchors:
            sA = (A @ s).astype(np.float32)
            sA /= np.linalg.norm(sA) + 1e-12
            transformed.append((sA, lam))
        tmp = MultiAnchorField(physics, kernel=anchors.kernel, rbf_sigma=anchors.rbf_sigma)
        tmp.set(transformed)
        V_acc += tmp.potential()
    return (V_acc / float(draws)).astype(np.float32)

def add_curiosity_penalty(V, visits, alpha=0.12):
    try:
        cur = -alpha * np.log1p(visits.astype(np.float32))
        return (V + cur).astype(np.float32)
    except Exception:
        return V

# --- Main E8 Mind Classes ---

def build_weighted_adjacency(roots, atol=1e-6):
    """Build weighted adjacency matrix for E8 roots."""
    R = roots.astype(np.float32)
    N = R.shape[0]
    
    # Basic adjacency: roots with dot product �1
    mask = np.isclose(np.abs(R @ R.T), 1.0, atol=atol)
    np.fill_diagonal(mask, False)
    
    W = np.zeros((N, N), dtype=np.float32)
    W[mask] = 1.0
    
    # Enhanced weights based on lattice structure
    int_roots = {tuple((2*r).astype(np.int8)) for r in R}
    for i in range(N):
        for j in np.where(mask[i])[0]:
            ri2, rj2 = (2*R[i]).astype(np.int8), (2*R[j]).astype(np.int8)
            s, d = tuple((ri2 + rj2).tolist()), tuple((ri2 - rj2).tolist())
            W[i, j] += 0.15 * (s in int_roots) + 0.10 * (d in int_roots)
    
    return W

class SACAgent:
    def __init__(self, state_dim, action_dim, hidden_sizes=[256, 256], actor_lr=3e-4, **kwargs):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.hidden_sizes = hidden_sizes
        self.actor_lr = actor_lr
        # Simple placeholder implementation
        # Basic internal RNG for reproducible-ish behavior
        try:
            seed = int(os.getenv("GLOBAL_SEED", "1337"))
        except Exception:
            seed = 1337
        self._rng = np.random.default_rng(seed)
        # Placeholder policy params (can be replaced by real networks later)
        self._policy_scale = float(kwargs.get('policy_scale', 0.15))

        # Minimal replay buffer / store used by the surrounding code. This
        # lightweight implementation prevents AttributeError when higher-level
        # code calls `agent.store(...)`. It intentionally keeps a bounded
        # ring buffer of simple tuples (s, a, s2, r, done).
        self._replay_capacity = int(kwargs.get('replay_capacity', 32768))
        self._replay = [None] * self._replay_capacity
        self._replay_pos = 0
        self._replay_size = 0

    # --- Replay buffer helper methods ---
    def store(self, state, action, next_state, reward, done=False):
        """Store a single transition into the lightweight replay buffer.

        This is intentionally forgiving about input types: it stores the raw
        objects as given so the rest of the system can store arbitrary
        state-like payloads (numpy arrays, dicts, custom objects).
        """
        try:
            self._replay[self._replay_pos] = (state, action, next_state, float(reward), bool(done))
            self._replay_pos = (self._replay_pos + 1) % self._replay_capacity
            self._replay_size = min(self._replay_capacity, self._replay_size + 1)
        except Exception:
            # If anything goes wrong, don't crash the outer system. Just ignore.
            pass

    def sample(self, batch_size=64):
        """Return up to batch_size random transitions from the buffer.

        Returns a list of tuples (s, a, s2, r, done). If the buffer is empty,
        returns an empty list.
        """
        try:
            n = int(min(self._replay_size, max(0, int(batch_size))))
            if n <= 0:
                return []
            idx = self._rng.choice(self._replay_size, size=n, replace=False)
            # Map indices into the ring buffer
            mapped = [(self._replay[i % self._replay_capacity]) for i in idx]
            return mapped
        except Exception:
            return []

    def update(self):
        """Placeholder update loop for compatibility with existing callers.

        The real SAC update (actor/critic gradient steps) is intentionally
        omitted here. Callers that expect an update method can call this
        without error; it will simply be a no-op unless extended.
        """
        return None

    def select_action(self, state, deterministic: bool = False) -> np.ndarray:
        """Return an action vector given a state.

        This is intentionally lightweight: it accepts a numpy array-like state
        or an object with a numpy representation. It returns a float32
        numpy array of length `action_dim` suitable for downstream code.

        Args:
            state: numpy array or array-like representing the current state.
            deterministic: if True, returns a zero-vector baseline (or mean policy).

        Returns:
            np.ndarray: action vector of shape (action_dim,)
        """
        try:
            # Try to coerce the state into an ndarray
            s = np.asarray(state, dtype=np.float32)
        except Exception:
            # If it's an object, try to call _asarray or extract attributes
            try:
                s = np.asarray(getattr(state, 'position', 0.0), dtype=np.float32)
            except Exception:
                s = np.zeros(self.state_dim, dtype=np.float32)

        # Deterministic baseline: zeros (safe no-op)
        if deterministic:
            return np.zeros(self.action_dim, dtype=np.float32)

        # Stochastic policy: scaled gaussian noise centered at zero
        try:
            action = self._rng.normal(loc=0.0, scale=self._policy_scale, size=(self.action_dim,)).astype(np.float32)
            # Clip to [-1, 1] which is commonly expected
            action = np.clip(action, -1.0, 1.0)
            return action
        except Exception:
            return np.zeros(self.action_dim, dtype=np.float32)

class E8Physics:
    def __init__(self, console):
        self.console = console
        self.roots = generate_e8_roots()
        self.roots_unit = self.roots / (np.linalg.norm(self.roots, axis=1, keepdims=True) + 1e-12)
        self.roots_kdtree = KDTree(self.roots)
        self.weights = build_weighted_adjacency(self.roots)
        self.adj_bool = (self.weights > 0).astype(np.int8)
        self.hops = all_pairs_hops(self.adj_bool)
        self.L_norm = self._build_normalized_laplacian()
        self._mask_cache = {}
        self.projection_matrix = None
        # Cached decoding basis/axis
        self._decoding_axis_idx = None
        self._decoding_axis_vec = None
        self._decoding_basis = None
        self.console.log(f"[INIT] E8Physics: roots={len(self.roots)}, edges={(self.adj_bool.sum())//2}")

        # E8 validation and verification
        self._run_e8_validations()

    # Add/enhance this method in the E8Physics class (integrates golden ratio from research for symmetry breaking)
    def get_symmetric_counterpart(self, index: int) -> int:
        """
        Finds the antipodal (symmetric) point on the E8 lattice using research-validated golden ratio modulation.
        The 240 root vectors form 120 opposite pairs (v, -v); we negate and query nearest neighbor,
        with golden ratio adjustment for enhanced symmetry breaking as in E8 particle spectra.
        Returns -1 on failure.
        """
        try:
            if not hasattr(self, 'roots') or index < 0 or index >= len(self.roots):
                return -1

            # Golden ratio from E8 research (ϕ ≈ 1.618) for modulation
            phi = (1 + np.sqrt(5)) / 2  # ≈1.618
            base_vec = self.roots[index]
            if base_vec is None:
                return -1

            # Primary target: golden-ratio scaled antipode
            target_vector = -base_vec * phi
            _, idx_arr = self.roots_kdtree.query(target_vector.reshape(1, -1), k=1)
            counterpart = int(np.atleast_1d(idx_arr).reshape(-1)[0])

            # Fallback: if KDTree returns same index (degenerate) try pure antipode without phi
            if counterpart == index:
                target_vector2 = -base_vec
                _, idx_arr2 = self.roots_kdtree.query(target_vector2.reshape(1, -1), k=1)
                counterpart = int(np.atleast_1d(idx_arr2).reshape(-1)[0])

            return counterpart if 0 <= counterpart < len(self.roots) else -1
        except Exception as e:
            try:
                self.console.log(f"[E8Physics] get_symmetric_counterpart error for index {index}: {e}")
            except Exception:
                pass
            return -1

    def find_nearest_root_index(self, vector_8d: np.ndarray) -> Optional[int]:
        if vector_8d is None or vector_8d.shape[0] != 8:
            return None
        try:
            _, index = self.roots_kdtree.query(vector_8d.reshape(1, -1), k=1)
            arr = np.atleast_1d(index).reshape(-1)
            if arr.size == 0:
                return None
            return int(arr[0])
        except Exception as e:
            self.console.log(f"[E8Physics] Error finding nearest root: {e}")
            return None

    def generate_quasicrystal_blueprint(self, seed: int = GLOBAL_SEED):
        P, pts = None, None
        uniqueness_threshold = 230
        max_tries = 32

        for i in range(max_tries):
            current_seed = seed + i
            rng = np.random.default_rng(current_seed)
            M = rng.normal(size=(8, 3)).astype(np.float32)
            Q, _ = np.linalg.qr(M)
            P_candidate = Q[:, :3]

            pts_candidate = self.roots @ P_candidate
            unique_pts = np.unique(np.round(pts_candidate, 3), axis=0)

            if len(unique_pts) >= uniqueness_threshold:
                P = P_candidate
                pts = pts_candidate
                self.console.log(f"[INIT] Quasicrystal projection found after {i+1} tries. Uniqueness: {len(unique_pts)}/240.")
                break

        if P is None:
            self.console.log(f"[bold yellow][WARN] Quasicrystal projection failed to meet uniqueness threshold after {max_tries} tries. Using last attempt.[/bold yellow]")
            rng = np.random.default_rng(seed + max_tries - 1)
            M = rng.normal(size=(8, 3)).astype(np.float32)
            Q, _ = np.linalg.qr(M)
            P = Q[:, :3]
            pts = self.roots @ P

        pts -= pts.mean(axis=0, keepdims=True)
        pts /= (np.abs(pts).max() + 1e-6)
        self.projection_matrix = P

        blueprint_coords = []
        rounded_coords = np.round(pts, 4)
        coord_groups = defaultdict(list)
        for i, coord in enumerate(rounded_coords):
            coord_groups[tuple(coord)].append(i)

        for i in range(pts.shape[0]):
            base_x, base_y, base_z = float(pts[i, 0]), float(pts[i, 1]), float(pts[i, 2])
            group = coord_groups[tuple(rounded_coords[i])]
            render_x, render_y = base_x, base_y

            if len(group) > 1:
                k = group.index(i) + 1
                epsilon = 0.005
                radius = epsilon * math.sqrt(k)
                theta = k * math.pi * (3 - math.sqrt(5))
                render_x += radius * math.cos(theta)
                render_y += radius * math.sin(theta)

            blueprint_coords.append({
                "id": i, "x": base_x, "y": base_y, "z": base_z,
                "render_x": render_x, "render_y": render_y, "render_z": base_z
            })

        try:
            # Prefer fast KDTree if available; SciPy >=1.11 may require numpy>=2 for np.asarray(copy=...)
            kdtree = KDTree(pts)
            distances, _ = kdtree.query(pts, k=2)
            min_dist = float(np.min(distances[:, 1]))
            self.console.log(f"[INIT] Min nearest-neighbor distance in blueprint: {min_dist:.4f}")
        except Exception as e:
            # Fallback path: handle numpy<2 / scipy built against numpy>=2 causing 'asarray(copy=...)' errors.
            msg = str(e)
            if 'asarray() got an unexpected keyword argument' in msg or 'copy' in msg:
                try:
                    # Manual O(N^2) fallback (N ~ 240 for E8 roots â†’ trivial cost)
                    diffs = pts[:, None, :] - pts[None, :, :]
                    dists = np.sqrt(np.sum(diffs * diffs, axis=2))
                    np.fill_diagonal(dists, np.inf)
                    min_dist = float(np.min(dists))
                    self.console.log(
                        f"[INIT] Min nearest-neighbor distance (manual fallback) {min_dist:.4f} Â· SciPy/NumPy mismatch suspected (KDTree error: {msg})"
                    )
                except Exception as ee:
                    self.console.log(f"[INIT] Could not calculate min distance (both methods failed): {ee}")
            else:
                self.console.log(f"[INIT] Could not calculate min distance: {e}")

        return blueprint_coords

    def _build_normalized_laplacian(self):
        if csr_matrix is None or diags is None: return np.eye(self.weights.shape[0])
        W = csr_matrix(self.weights, dtype=np.float32)
        deg = np.asarray(W.sum(axis=1)).ravel()
        D_inv_sqrt = diags(1.0 / np.sqrt(np.maximum(deg, 1e-9)))
        return diags(np.ones(W.shape[0])) - D_inv_sqrt @ W @ D_inv_sqrt

    def heat_mask_cached(self, center_idx, sigma=1.25):
        key = (int(center_idx), round(float(sigma), 2))
        m = self._mask_cache.get(key)
        if m is None:
            d = self.hops[center_idx]
            m = np.exp(- (d * d) / (2.0 * sigma * sigma)).astype(np.float32)
            self._mask_cache[key] = m
        return m

    def _run_e8_validations(self):
        """Run E8 root system validations during initialization."""
        try:
            # Verify E8 root system properties
            roots_valid = verify_e8_roots(self.roots, self.console)

            # Verify bracket relations
            _br_ok, _v, _t = normalize_bracket_result(verify_e8_brackets(self.roots, self.console))
            brackets_valid = (_t > 0 and _v == _t)

            # Test Lie algebra structure
            lie_valid = verify_e8_lie_algebra_structure(self.roots, self.console)

            # Test octonion bracket (offline validation)
            try:
                octonion_valid = octonion_bracket_test(self.console)
            except Exception as _e:
                octonion_valid = False
                if self.console:
                    self.console.log(f"[OCTONION] Octonion bracket test raised error: {_e}")

            # Test triacontagonal projection
            triacontagonal_projection(self.roots, self.console)

            # Summary
            all_valid = roots_valid and brackets_valid and lie_valid and octonion_valid
            status = "?" if all_valid else "?"
            self.console.log(f"[E8_VERIFY] {status} E8 validation summary: roots={roots_valid}, brackets={brackets_valid}, lie={lie_valid}, octonion={octonion_valid}")

        except Exception as e:
            self.console.log(f"[E8_VERIFY] Error during E8 validations: {e}")

    # --- PATCH A: Decoding axis and orthonormal basis (Householder alignment) ---
    def get_decoding_axis_unit(self) -> np.ndarray:
        """Return the unit decoding axis v̂ in R^8.
        Selection is controlled by env var E8_DECODING_ROOT_INDEX; defaults to 0.
        Cached for stability; update only if the env index changes.
        """
        try:
            idx_env = os.getenv("E8_DECODING_ROOT_INDEX")
            idx = int(idx_env) % 240 if idx_env is not None and idx_env != "" else 0
        except Exception:
            idx = 0
        if self._decoding_axis_idx != idx or self._decoding_axis_vec is None:
            # Clamp and cache
            try:
                v = np.asarray(self.roots_unit[idx], dtype=np.float32).copy()
            except Exception:
                v = np.ones(8, dtype=np.float32)
            n = float(np.linalg.norm(v))
            if n < 1e-9:
                v = np.ones(8, dtype=np.float32)
                v /= np.linalg.norm(v)
            self._decoding_axis_vec = v
            self._decoding_axis_idx = idx
        return self._decoding_axis_vec

    def get_decoding_basis(self) -> np.ndarray:
        """Return an 8x8 orthonormal basis matrix Q such that Q @ v ≈ e1
        where v is the decoding axis from get_decoding_axis_unit.
        Uses a Householder transform to align v→e1. Cached for reuse.
        """
        if self._decoding_basis is not None:
            return self._decoding_basis
        try:
            v = np.asarray(self.get_decoding_axis_unit(), dtype=np.float32).reshape(-1)
            if v.shape[0] != 8:
                v = np.ones(8, dtype=np.float32) / np.sqrt(8.0)
            v = v / (np.linalg.norm(v) + 1e-12)
            e1 = np.zeros(8, dtype=np.float32); e1[0] = 1.0
            diff = v - e1
            norm_diff = float(np.linalg.norm(diff))
            if norm_diff < 1e-9:
                H = np.eye(8, dtype=np.float32)
            else:
                u = diff / norm_diff
                # Householder reflection: H v = e1
                H = np.eye(8, dtype=np.float32) - 2.0 * np.outer(u, u)
                # Numerical guard: enforce orthonormality via QR
                try:
                    Qh, _ = np.linalg.qr(H)
                    H = Qh.astype(np.float32)
                except Exception:
                    pass
            self._decoding_basis = H
            return self._decoding_basis
        except Exception as e:
            try:
                self.console.log(f"[E8Physics] get_decoding_basis failed: {e}")
            except Exception:
                pass
            return np.eye(8, dtype=np.float32)

# --- [ADD | Horizon structures next to E8Physics] ---
class HorizonLayer:
    """Generic boundary layer container."""
    def __init__(self, name):
        self.name = name
        # positions in 3D (for blueprint) or in shell's intrinsic coords
        self.pos = None          # (N,3) or (N,D)
        self.normals = None      # (N,3) unit outward normals (3D horizons)
        self.indices = None      # indices of nodes that belong to the horizon
        self.meta = {}           # free-form (degree, voronoi_area, etc.)

class HorizonManager:
    """Holds H_E8 and per-shell H_shell[d] plus cross-horizon kernels C_d."""
    def __init__(self):
        self.H_E8 = None
        self.H_shell = {}        # d -> HorizonLayer
        self.C_d = {}            # d -> (Nh_shell, Nh_e8) sparse coupling

    def get_horizon_layers(self) -> Dict[str, 'HorizonLayer']:
        """Return all horizon layers as a dict."""
        layers = {}
        if self.H_E8:
            layers['H_E8'] = self.H_E8
        for d, h in self.H_shell.items():
            layers[f'H_shell_{d}'] = h
        return layers

    def get_horizon_snapshot(self) -> Optional[Dict]:
        """Create snapshot of current horizon state for spacetime signatures."""
        try:
            if not self.H_E8 and not self.H_shell:
                return None

            snapshot = {
                'timestamp': time.time(),
                'layers': {},
                'energy_totals': {},
                'field_couplings': {}
            }

            # E8 blueprint horizon
            if self.H_E8:
                snapshot['layers']['H_E8'] = {
                    'name': self.H_E8.name,
                    'num_sites': len(self.H_E8.indices) if self.H_E8.indices else 0,
                    'dimensions': self.H_E8.pos.shape[1] if self.H_E8.pos is not None else 0,
                    'has_normals': self.H_E8.normals is not None,
                    'meta': self.H_E8.meta
                }

            # Per-shell horizons
            for dim, h_layer in self.H_shell.items():
                snapshot['layers'][f'H_shell_{dim}'] = {
                    'name': h_layer.name,
                    'num_sites': len(h_layer.indices) if h_layer.indices else 0,
                    'dimensions': h_layer.pos.shape[1] if h_layer.pos is not None else 0,
                    'has_normals': h_layer.normals is not None,
                    'meta': h_layer.meta
                }

            # Calculate basic energy totals (placeholder - could be enhanced with actual physics)
            total_energy = 0.0
            boundary_energy = 0.0
            stress_energy = 0.0
            
            for layer_name, layer_info in snapshot['layers'].items():
                num_sites = layer_info.get('num_sites', 0)
                dimensions = layer_info.get('dimensions', 3)
                
                # Simple energy estimates based on boundary sites and dimensions
                # These are placeholders - real implementation would use actual field data
                layer_energy = num_sites * dimensions * 0.1  # Base energy per site
                total_energy += layer_energy
                
                if 'E8' in layer_name:
                    boundary_energy += layer_energy * 1.5  # E8 boundaries have higher energy
                else:
                    boundary_energy += layer_energy
                    
                stress_energy += layer_energy * 0.3  # Stress energy component

            snapshot['energy_totals'] = {
                'total_energy': float(total_energy),
                'boundary_energy': float(boundary_energy),
                'stress_energy': float(stress_energy)
            }

            # Coupling kernels info
            for dim, coupling in self.C_d.items():
                if hasattr(coupling, 'shape'):
                    snapshot['field_couplings'][f'C_{dim}'] = {
                        'shape': coupling.shape,
                        'nnz': coupling.nnz if hasattr(coupling, 'nnz') else 0
                    }

            return snapshot

        except Exception as e:
            # Log error but don't fail spacetime signature creation
            try:
                import sys
                console = sys.modules.get('__main__', None)
                if console and hasattr(console, 'log'):
                    console.log(f"[HorizonManager] Failed to create horizon snapshot: {e}")
            except Exception:
                pass
            return None

def build_e8_horizon(physics, blueprint_positions_3d, tetra_edges, console=None):
    """
    Construct H_E8: boundary points at projected E8 tetra centers/edges.
    - blueprint_positions_3d: (N,3) unique 3D points of the 240 projection
    - tetra_edges: list of (i,j) edges (indices into blueprint array)
    Normals: edge-centered normals via local PCA of incident neighbors.
    """
    import numpy as np
    from sklearn.neighbors import NearestNeighbors

    H = HorizonLayer("H_E8")
    P = np.asarray(blueprint_positions_3d, dtype=np.float64)
    edge_mid = []
    edge_norm = []

    # midpoints & normals from local PCA
    nbrs = NearestNeighbors(n_neighbors=min(12, len(P))).fit(P)
    for (i, j) in tetra_edges:
        m = 0.5 * (P[i] + P[j])
        # local tangent plane from neighbors -> normal is minor principal axis of covariance
        _, idx = nbrs.kneighbors(m.reshape(1, -1))
        Q = P[idx[0]]
        C = np.cov((Q - Q.mean(0)).T)
        w, v = np.linalg.eigh(C)
        n = v[:, 0]
        n = n / (np.linalg.norm(n) + 1e-12)
        edge_mid.append(m)
        edge_norm.append(n)

    H.pos = np.vstack(edge_mid) if edge_mid else np.zeros((0, 3))
    H.normals = np.vstack(edge_norm) if edge_norm else np.zeros((0, 3))
    H.indices = None
    H.meta["edges"] = tetra_edges
    if console:
        console.log(f"[Horizon] Built H_E8 with {len(edge_mid)} edge sites.")
    return H

def build_cross_horizon_kernel(H_e8, H_shell, symmetry_weight=1.0):
    """
    Build sparse C_d mapping H_E8 -> H_shell[d] by lifted adjacency & distance.
    Returns (rows, cols, vals) COO triple suitable for scipy.sparse.csr_matrix.
    """
    import numpy as np
    from scipy.sparse import coo_matrix
    if H_e8.pos.size == 0 or H_shell.pos.size == 0:
        return coo_matrix((0,0))
    A = H_shell.pos.astype(np.float64)
    B = H_e8.pos.astype(np.float64)[:, :A.shape[1]]
    rows, cols, vals = [], [], []
    # simple radial basis + symmetry factor placeholder
    for i, a in enumerate(A):
        # nearest few blueprint sites
        d2 = np.sum((B - a)**2, axis=1)
        idx = np.argsort(d2)[:8]
        w = np.exp(-d2[idx] / (np.median(d2[idx]) + 1e-12)) * symmetry_weight
        s = w.sum() + 1e-12
        for j, wij in zip(idx, w):
            rows.append(i); cols.append(j); vals.append(float(wij / s))
    return coo_matrix((vals, (rows, cols)), shape=(A.shape[0], B.shape[0]))
# --- [END ADD] ---

class ClassicalConfig:
    def __init__(self, seed=None):
        self.seed = seed

class QuantumConfig:
    def __init__(self, gamma: float = 0.03, dt: float = 0.25, batch: int = 9,
                 dephase: float = 0.0, locality_sigma: float = 1.5,
                 seed=None, topk_amp: int = 5, non_linearity_strength: float = 2.5):
        self.gamma = gamma
        self.mode = os.getenv('E8_QMODE', 'lattice')
        self.lambda_V = float(os.getenv('E8_Q_LAMBDA', '0.2'))
        self.decay_tau = float(os.getenv('E8_Q_DECAY_TAU', '600'))
        self.rebuild_every = int(os.getenv('E8_Q_REBUILD_EVERY', '10'))
        self.alpha_edge = float(os.getenv('E8_Q_ALPHA_EDGE', '1.0'))
        self.reward_gain = float(os.getenv('E8_Q_REWARD_GAIN', '0.5'))
        self.dt = dt
        self.batch = batch
        self.dephase = dephase
        self.locality_sigma = locality_sigma
        self.seed = seed
        self.topk_amp = topk_amp
        self.non_linearity_strength = non_linearity_strength

def _safe_node_to_root_idx(nid, memory, mind):
    try:
        node = memory.graph_db.get_node(nid)
        loc = node.get('blueprint_location_id') if node else None
        if isinstance(loc, (int, np.integer)) and 0 <= int(loc) < 240:
            return int(loc)
        vec = memory.main_vectors.get(nid)
        if vec is not None and TORCH_AVAILABLE and getattr(mind, 'autoencoder', None) and mind.autoencoder and mind.autoencoder.is_trained:
            with torch.no_grad():
                z8 = mind.autoencoder.project_to_dim(torch.from_numpy(np.asarray(vec, dtype=np.float32)).float().unsqueeze(0), 8)
                if z8 is not None:
                    idx = mind.physics.find_nearest_root_index(safe_tensor_to_numpy(z8.squeeze(0)))
                    return int(idx) if idx is not None else None
    except Exception:
        return None
    return None

def build_adjacency_240_from_memory(memory, mind, alpha=1.0, decay_tau=600.0, reward_gain=0.5):
    rows, cols, data = [], [], []
    try:
        G = memory.graph_db.graph
        now = getattr(mind, 'step_num', 0)
        for u, v, attr in G.edges(data=True):
            iu = _safe_node_to_root_idx(u, memory, mind)
            iv = _safe_node_to_root_idx(v, memory, mind)
            if iu is None or iv is None or iu == iv:
                continue
            w = float(attr.get('weight', 1.0))
            ts = float(attr.get('ts', 0.0))
            lu = float((G.nodes[u] or {}).get('last_step', 0)) if u in G.nodes else 0.0
            lv = float((G.nodes[v] or {}).get('last_step', 0)) if v in G.nodes else 0.0
            last_seen = max(ts, lu, lv)
            rec = float(np.exp(-(max(0.0, now - last_seen))/max(1e-6, decay_tau)))
            ru = float((G.nodes[u] or {}).get('insight_reward', 0.0)) if u in G.nodes else 0.0
            rv = float((G.nodes[v] or {}).get('insight_reward', 0.0)) if v in G.nodes else 0.0
            rewd = float(attr.get('reward', 0.0))
            rbar = (ru + rv + rewd) / 3.0
            wij = alpha * w * rec * (1.0 + reward_gain * rbar)
            if wij <= 0:
                continue
            rows.append(iu); cols.append(iv); data.append(wij)
            rows.append(iv); cols.append(iu); data.append(wij)
        if data:
            A = csr_matrix((np.asarray(data, dtype=np.float32), (np.asarray(rows), np.asarray(cols))), shape=(240,240))
        else:
            A = csr_matrix((240,240), dtype=np.float32)
        return A
    except Exception as e:
        try: console.log(f"[Quantum] build_adjacency_240_from_memory failed: {e}")
        except Exception: pass
        return csr_matrix((240,240), dtype=np.float32)

class QuantumEngine:
    def __init__(self, physics, config, console: Console):
        self.mind = None
        self._last_graph_build_step = -1
        self.console = console
        self.physics, self.config = physics, config
        self.psi = np.ones((config.batch, 240), dtype=np.complex64) / np.sqrt(240)
        self.rng = np.random.default_rng(config.seed)
        self.H: Any = None
        self._last_H: Any = None
        self._last_potential: Optional[np.ndarray] = None
        self._last_norm = np.nan
        self._last_energy = np.nan
        self.build_hamiltonian()
        self.console.log("[INIT] Quantum Engine online (Non-Linear Edition).")

    def attach_mind(self, mind_instance):
        self.mind = mind_instance
        return self

    def build_hamiltonian(self, V: Optional[np.ndarray] = None):
        if diags is None or csr_matrix is None: return
        if V is None:
            V = np.zeros(240, dtype=np.float32)
        use_graph = (getattr(self.config, 'mode', 'lattice') == 'graph') and (getattr(self, 'mind', None) is not None)
        if use_graph and (self._last_graph_build_step < 0 or (getattr(self.mind, 'step_num', 0) - self._last_graph_build_step) >= getattr(self.config,'rebuild_every',10)):
            A = build_adjacency_240_from_memory(self.mind.memory, self.mind,
                                                alpha=getattr(self.config,'alpha_edge',1.0),
                                                decay_tau=getattr(self.config,'decay_tau',600.0),
                                                reward_gain=getattr(self.config,'reward_gain',0.5))
            H = (-self.config.gamma * A.astype(np.complex64)) + getattr(self.config,'lambda_V',0.2) * diags(V)
            self.H = csr_matrix(H)
            self._last_graph_build_step = getattr(self.mind, 'step_num', 0)
        else:
            H = (self.config.gamma * self.physics.L_norm.astype(np.complex64)) + diags(V)
            self.H = csr_matrix(H)
        self._last_H = self.H
        self._last_potential = np.asarray(V).copy()

    def _probs(self):
        p = np.abs(self.psi)**2
        return p / (np.sum(p, axis=1, keepdims=True) + 1e-12)

    def step_adaptive(self, tv_target=0.07, dt_min=0.02, dt_max=1.2):
        if self.H is None or expm_multiply is None:
            return 0.0

        p0 = self._probs()
        H_eff = self.H.copy()
        if self.config.non_linearity_strength != 0:
            feedback = self.config.non_linearity_strength * p0[0]
            H_eff += diags(feedback.astype(np.float32), 0)

        psi_new = expm_multiply(-1j * H_eff * self.config.dt, self.psi.T).T
        nrm = np.linalg.norm(psi_new, axis=1, keepdims=True)
        self.psi = psi_new / np.maximum(nrm, 1e-12)
        p1 = self._probs()

        tv = 0.5 * float(np.abs(p0 - p1).sum(axis=1).mean())
        if tv < 0.5*tv_target: self.config.dt = min(dt_max, self.config.dt*1.25)
        elif tv > 1.5*tv_target: self.config.dt = max(dt_min, self.config.dt*0.66)

        if self.config.dephase > 0:
            mag = np.abs(self.psi)
            self.psi = (1.0 - self.config.dephase) * self.psi + self.config.dephase * mag
            nrm = np.linalg.norm(self.psi, axis=1, keepdims=True)
            self.psi /= np.maximum(nrm, 1e-12)

        try:
            self._last_norm = float(np.mean(np.sum(np.abs(self.psi)**2, axis=1)))
            Href = self._last_H
            if Href is not None and getattr(Href, 'ndim', 0) == 2:
                Energies = []
                for b in range(self.psi.shape[0]):
                    v = self.psi[b].reshape(-1,1)
                    E = (np.conjugate(v).T @ (Href @ v)).ravel()[0]
                    Energies.append(np.real(E))
                self._last_energy = float(np.mean(Energies))
        except Exception:
            self._last_norm = np.nan
            self._last_energy = np.nan
        return tv

    def measure_local(self, prev_idx, sigma=None):
        sigma = sigma or self.config.locality_sigma
        P = self._probs()
        masks = np.stack([self.physics.heat_mask_cached(i, sigma) for i in prev_idx]) if isinstance(prev_idx, (list, np.ndarray)) else np.tile(self.physics.heat_mask_cached(int(prev_idx), sigma), (self.config.batch, 1))
        P *= masks
        P /= np.maximum(P.sum(axis=1, keepdims=True), 1e-12)
        return np.array([self.rng.choice(P.shape[1], p=p) for p in P], dtype=np.int32)

    def measure_hybrid(self, prev_idx=None, sigma=None, topk=None):
        if prev_idx is None:
            prev_idx = 0

        if not hasattr(self, "psi"):
            return self.measure_local([prev_idx] * self.config.batch, sigma)
        B, N = self.psi.shape

        P = np.abs(self.psi)**2
        P = P / (np.sum(P, axis=1, keepdims=True) + 1e-12)

        Vlast = self._last_potential
        if Vlast is not None and np.size(Vlast) == N:
            soft = np.maximum(0.0, -np.real(np.asarray(Vlast).reshape(1, -1)))
            if topk is None:
                topk = int(getattr(self.config, "topk_amp", 5) or 5)

            idx = np.argpartition(soft[0], -topk)[-topk:]
            mask = np.zeros_like(P)
            mask[:, idx] = 1.0

            Amp = np.sqrt(P) * np.sqrt(soft + 1e-12)
            P = (Amp**2) * mask
            P = P / (np.sum(P, axis=1, keepdims=True) + 1e-12)

        if sigma is None:
            sigma = float(getattr(self.config, "locality_sigma", 1.5) or 1.5)

        hops = self.physics.hops
        w = np.exp(-(hops[prev_idx]**2) / (2.0 * sigma * sigma))

        P = P * w.reshape(1, -1)
        P = P / (np.sum(P, axis=1, keepdims=True) + 1e-12)

        choices = []
        for b in range(B):
            choices.append(int(self.rng.choice(N, p=P[b])))
        return choices

    def telemetry_state(self):
        # Base engine telemetry
        base = {
            "norm": float(self._last_norm),
            "energy": float(self._last_energy),
            "topk_amp": int(getattr(self.config, "topk_amp", 5)),
            "locality_sigma": float(getattr(self.config, "locality_sigma", 1.5)),
        }
        # Quantum telemetry for front end
        base["quantum_telemetry"] = {
            "dt": float(getattr(self.config, "dt", 0.01)),
            "gamma": float(getattr(self.config, "gamma", 1.0)),
            "dephase": float(getattr(self.config, "dephase", 0.0)),
            "lam": float(getattr(self, '_last_lam', float('nan'))),
            "psi_entropy": float(getattr(self, '_last_psi_entropy', float('nan'))),
        }
        # Behavioral metrics for front end sparklines and indicators
        base["novelty"] = float(getattr(self, '_last_novelty', float('nan')))
        # use compression_gain if available, else fallback to compression
        base["compression_gain"] = float(getattr(self, '_last_compression_gain', getattr(self, '_last_compression', float('nan'))))
        base["disagreement"] = float(getattr(self, '_last_disagreement', float('nan')))
        base["kdtree_failures"] = int(getattr(self, '_last_kdtree_failures', -1))
        return base

    def measure_ablation(self, prev_idx:int, sigma:float=None, window:int=5, trials:int=512):
        if sigma is None:
            sigma = float(getattr(self.config, "locality_sigma", 1.5) or 1.5)
        B, N = getattr(self, "psi", np.zeros((1,1))).shape
        total = trials * B
        if total == 0:
            return {}

        local_choices = []
        hybrid_choices = []
        for _ in range(trials):
            local_choices.extend(self.measure_local([prev_idx] * B, sigma=sigma))
            hybrid_choices.extend(self.measure_hybrid(prev_idx=prev_idx, sigma=sigma))
        local_counts = np.bincount(np.asarray(local_choices), minlength=N)
        hybrid_counts = np.bincount(np.asarray(hybrid_choices), minlength=N)

        lo = max(0, prev_idx-window); hi = min(N-1, prev_idx+window)
        local_win = int(local_counts[lo:hi+1].sum())
        hybrid_win = int(hybrid_counts[lo:hi+1].sum())
        return {
            "prev_idx": int(prev_idx),
            "window": int(window),
            "sigma": float(sigma),
            "trials": int(trials),
            "batch": int(B),
            "N": int(N),
            "local_win": local_win,
            "hybrid_win": hybrid_win,
            "local_rate": float(local_win/total),
            "hybrid_rate": float(hybrid_win/total),
        }

class ClassicalEngine:
    def __init__(self, physics, config, console: Console):
        self.console = console
        self.physics, self.config = physics, config
        self.rng = np.random.default_rng(config.seed)
        self.console.log("[INIT] Classical Engine online.")

    def next_index(self, prev_idx, sensor8):
        nbrs = np.where(self.physics.weights[prev_idx] > 0)[0]
        if nbrs.size > 0:
            if np.linalg.norm(sensor8) > 0:
                scores = self.physics.roots[nbrs] @ sensor8
                p = np.exp(2.5 * scores); p /= np.sum(p)
                return self.rng.choice(nbrs, p=p)
            return self.rng.choice(nbrs)
        return self.rng.integers(0, 240)

class E8BoundaryFabric:
    def __init__(self, physics: "E8Physics", seed: int = 1337):
        self.physics = physics
        self.N = physics.roots.shape[0]
        self.A = (physics.weights > 0).astype(np.float32)
        self.pos2d: Optional[np.ndarray] = None
        self.z1d: Optional[np.ndarray] = None
        self.rng = np.random.default_rng(seed)

    def layout_2d(self):
        W = self.A; deg = W.sum(axis=1)
        Dm12 = 1.0 / np.sqrt(np.maximum(deg, 1e-9))
        L = np.eye(self.N, dtype=np.float32) - (Dm12[:,None] * W * Dm12[None,:])
        try:
            if sp is None or eigsh is None:
                raise RuntimeError("scipy missing")
            csr_func = getattr(getattr(sp, 'sparse', None), 'csr_matrix', None)
            if csr_func is None:
                raise RuntimeError("csr_matrix missing")
            _, vecs = eigsh(csr_func(L), k=4, which='SM')
            P = vecs[:, 1:4]
        except Exception:
            try:
                _, vecs = np.linalg.eigh(L)
                P = vecs[:, 1:4]
            except Exception:
                P = np.random.randn(self.N,3)
        P = (P - P.mean(axis=0)) / (P.std(axis=0) + 1e-6)
        self.pos2d = P[:, :2].astype(np.float32)
        self.z1d = P[:, 2].astype(np.float32)

    def neighbors(self, i: int) -> np.ndarray:
        return np.where(self.A[i] > 0)[0].astype(np.int32)

    def to_json(self):
        if self.pos2d is None: self.layout_2d()
        edges = np.column_stack(np.where(np.triu(self.A, 1) > 0)).tolist()
        if self.pos2d is None or self.z1d is None:
            return {"nodes": [], "edges": []}
        return {
            "nodes": [{"id": int(i), "x": float(self.pos2d[i,0]), "y": float(self.pos2d[i,1]), "z": float(self.z1d[i])} for i in range(self.N)],
            "edges": [{"s": int(i), "t": int(j)} for i, j in edges]
        }

class SliceStack:
    def __init__(self, n_slices: int = 24, zmin: float = -1.5, zmax: float = 1.5):
        self.n, self.zmin, self.zmax = n_slices, zmin, zmax
        self.bin = np.linspace(self.zmin, self.zmax, self.n + 1)

    def index(self, z: float) -> int:
        return int(np.asarray(np.asarray(np.clip(np.searchsorted(self.bin, z, side="right").ravel()[0]) - 1, 0, self.n - 1)).ravel()[0])


# Hawking-style probabilistic evaporating cache
import time

class HawkingCache:
    def __init__(self, budget=20000, half_life_s=1800.0):
        self.store = {}  # key -> (value, created_ts, access_count)
        self.budget = int(budget)
        self.half = float(half_life_s)

    def __len__(self):
        return len(self.store)

    def get(self, k):
        v = self.store.get(k)
        if not v:
            return None
        val, t0, n = v
        self.store[k] = (val, t0, n+1)
        return val

    def put(self, k, val):
        now = time.time()
        self.store[k] = (val, now, 1)
        try:
            self._evaporate_if_hot(now)
        except Exception:
            # Avoid cache causing crashes
            pass

    def _evap_prob(self, now, t0, n, avg_temp):
        temp = (now - t0) / (n + 1.0)
        return np.tanh(temp / max(1e-6, avg_temp))

    def _evaporate_if_hot(self, now):
        if len(self.store) <= self.budget:
            return
        temps = [(k, (now - t0) / (n+1.0)) for k,(v,t0,n) in self.store.items()]
        avg = np.mean([t for _,t in temps]) if temps else 1.0
        # probabilistic evaporation (Hawking radiation)
        for k, temp in sorted(temps, key=lambda x: x[1], reverse=True):
            if np.random.rand() < np.tanh(temp / (avg+1e-6)):
                self.store.pop(k, None)
            if len(self.store) <= int(self.budget*0.9):
                break

class HoloEncoder:
    def __init__(self, fabric: E8BoundaryFabric, feat_dim: int = 8, shadow_k: int = 12, seed: int = 1337):
        self.fabric, self.feat_dim, self.shadow_k = fabric, feat_dim, shadow_k
        self.rng = np.random.default_rng(seed)
        # Use HawkingCache to bound memory and probabilistically evict
        self._U_cache = HawkingCache(budget=int(os.getenv("E8_U_CACHE_BUDGET", "20000")),
                                     half_life_s=float(os.getenv("E8_U_CACHE_HALF_LIFE", "1800")))
        self.store: Dict[Tuple[int, int], float] = {}

    def shadow_set(self, bulk_idx: int, pos_hint_xy: np.ndarray = None) -> np.ndarray:
        if pos_hint_xy is not None and self.fabric.pos2d is not None:
            d = np.sum((self.fabric.pos2d - pos_hint_xy[None,:])**2, axis=1)
            return np.argsort(d)[:self.shadow_k].astype(np.int32)
        nb = self.fabric.neighbors(int(bulk_idx))
        if nb.size >= self.shadow_k: return nb[:self.shadow_k]
        pool = np.setdiff1d(np.arange(self.fabric.N), np.append(nb, bulk_idx))
        if not pool.size > 0: return nb
        extra_count = self.shadow_k - nb.size
        extra = self.rng.choice(pool, size=min(extra_count, pool.size), replace=False)
        return np.concatenate([nb, extra]).astype(np.int32)

    def _U(self, shadow_ids: np.ndarray):
        key = tuple(sorted(shadow_ids.tolist()))
        U = self._U_cache.get(key)
        if U is None:
            K, D = len(shadow_ids), self.feat_dim
            if K < D:
                U = np.zeros((K, D), dtype=np.float32)
            else:
                R = self.rng.standard_normal((K, D)).astype(np.float32)
                Q, _ = np.linalg.qr(R, mode='reduced')
                U = Q[:, :D]
            try:
                self._U_cache.put(key, U)
            except Exception:
                # If caching fails for any reason, still return U
                pass
        return U

    def encode_bulk(self, feat: np.ndarray, shadow_ids: np.ndarray, slice_id: int):
        U = self._U(shadow_ids)
        y = U @ feat
        payload = {"f": y.astype(np.float32).tolist()}
        for nid, val in zip(shadow_ids, payload["f"]):
            self.store[(int(nid), int(slice_id))] = float(val)
        return payload

    def decode_boundary(self, shadow_ids: np.ndarray, slice_id: int, payload: dict) -> np.ndarray:
        U = self._U(shadow_ids)
        y = np.array(payload.get("f", []), dtype=np.float32)
        if y.size == 0:
            return np.zeros(self.feat_dim, dtype=np.float32)
        y = y[:U.shape[0]]
        return (U.T @ y).astype(np.float32)

    def encode(self, embedding: np.ndarray) -> np.ndarray:
        try:
            v = np.asarray(embedding, dtype=np.float32).reshape(-1)
            if not hasattr(self, "_compressor") or self._compressor is None:
                in_dim = int(v.size)
                try:
                    self._compressor = TinyCompressor(in_dim=in_dim, code_dim=int(self.feat_dim))
                except Exception:
                    self._compressor = None
            if getattr(self, "_compressor", None) is not None:
                z = self._compressor.encode(v)
            else:
                z = v[: self.feat_dim]
            z = np.asarray(z, dtype=np.float32).reshape(-1)
            if z.size < self.feat_dim:
                z = np.pad(z, (0, self.feat_dim - z.size))
            elif z.size > self.feat_dim:
                z = z[: self.feat_dim]
            return z.astype(np.float32)
        except Exception:
            v = np.asarray(embedding, dtype=np.float32).reshape(-1)
            if v.size < self.feat_dim:
                v = np.pad(v, (0, self.feat_dim - v.size))
            return v[: self.feat_dim].astype(np.float32)

class EntropyMap:
    def __init__(self, fabric: "E8BoundaryFabric", k_bits_per_edge: float = 4.0):
        self.fabric, self.k = fabric, float(k_bits_per_edge)
        self.A = (fabric.A > 0).astype(np.float32)
        self.N = int(np.asarray(self.A.shape).ravel()[0])

    def perimeter(self, region_nodes: np.ndarray) -> float:
        mask = np.zeros(self.N, dtype=np.float32)
        mask[region_nodes] = 1.0
        cut = np.sum(self.A[region_nodes], axis=0) * (1.0 - mask)
        return float(cut.sum())

    def budget_bits(self, region_nodes: np.ndarray) -> float:
        return self.k * self.perimeter(region_nodes)

    def usage_bits(self, store: dict, region_nodes: np.ndarray, slice_id: int = None) -> float:
        rset = set(int(i) for i in region_nodes.tolist())
        bits = 0.0
        for (nid, sid), val in store.items():
            if nid in rset and (slice_id is None or sid == int(slice_id)):
                bits += 32.0
        return float(bits)

    def deficit_ratio(self, store: dict, region_nodes: np.ndarray, slice_id: int = None) -> float:
        B = self.budget_bits(region_nodes) + 1e-6
        U = self.usage_bits(store, region_nodes, slice_id)
        return float((U - B) / B)

class SensorProjector:
    def __init__(self, in_dim, out_dim=8, seed=None):
        self.in_dim, self.out_dim = in_dim, out_dim
        self.rng = np.random.default_rng(seed)
        self.W = self.rng.standard_normal((in_dim, out_dim)).astype(np.float32) * 0.1
        self.mu = np.zeros(in_dim, dtype=np.float32)

    def pca_bootstrap(self, embeddings: np.ndarray, top_k=240):
        if embeddings.shape[0] < self.out_dim or PCA is None: return
        try:
            pca = PCA(n_components=self.out_dim)
            pca.fit(embeddings[:top_k])
            self.W, self.mu = pca.components_.T, pca.mean_
            console.log(f"[PROJ] Bootstrapped with PCA on {top_k} embeddings.")
        except Exception as e:
            console.log(f"[PROJ] PCA bootstrap failed: {e}. Falling back to random init.")

    def project(self, embedding):
        if embedding.shape[0] != self.in_dim:
            raise ValueError(f"Embedding dimension mismatch. Expected {self.in_dim}, got {embedding.shape[0]}.")
        return normalize_vector((embedding - self.mu) @ self.W)

    def train(self, embeddings, labels, roots_unit, epochs=3, lr=5e-3, batch_size=64, **kwargs):
        if embeddings.shape[0] < batch_size: return
        console.log(f"[PROJ] Starting training burst on {embeddings.shape[0]} samples.")
        for _ in range(epochs):
            indices = self.rng.integers(0, embeddings.shape[0], size=batch_size)
            for i in indices:
                e, y = embeddings[i], labels[i]
                s = normalize_vector((e - self.mu) @ self.W)
                delta_W = lr * np.outer(e - self.mu, roots_unit[y] - s)
                self.W += delta_W

class TinyCompressor:
    def __init__(self, in_dim=1536, code_dim=8):
        self.in_dim, self.code_dim = in_dim, code_dim
        self.ready, self._pca = False, None
        self._use_torch = TORCH_AVAILABLE
        if self._use_torch:
            class AE(nn.Module):
                def __init__(self, D, C):
                    super().__init__()
                    self.enc = nn.Linear(D, C, bias=False)
                    self.dec = nn.Linear(C, D, bias=False)

                def forward(self, x):
                    return self.enc(x), self.dec(self.enc(x))

            self.net = AE(self.in_dim, self.code_dim)
            for p in self.net.parameters():
                nn.init.xavier_uniform_(p.data)
            self.opt = torch.optim.Adam(self.net.parameters(), lr=3e-3)
        self.ready = self._use_torch

    def fit(self, X: np.ndarray, epochs=5, bs=64):
        if X.shape[0] < max(bs, self.code_dim + 1): return
        if self._use_torch:
            self.net.train(); loss_fn = nn.MSELoss()
            for _ in range(epochs):
                for i in range(0, X.shape[0], bs):
                    b = torch.from_numpy(X[np.random.permutation(X.shape[0])[i:i+bs]])
                    self.opt.zero_grad(); _, xh = self.net(b); loss = loss_fn(xh, b)
                    loss.backward(); self.opt.step()
        elif PCA is not None:
            self._pca = PCA(n_components=self.code_dim).fit(X)
        self.ready = True

    def encode(self, x: np.ndarray) -> np.ndarray:
        x = x.reshape(1, -1)
        if self._use_torch and self.ready:
            self.net.eval()
            with torch.no_grad(): z, _ = self.net(torch.from_numpy(x))
            return safe_tensor_to_numpy(z).ravel()
        if self._pca and self.ready: return self._pca.transform(x).ravel()
        return x.ravel()[:self.code_dim]

class MultiAnchorField:
    def __init__(self, physics, kernel='cosine', rbf_sigma=0.8):
        self.physics, self.kernel, self.rbf_sigma = physics, kernel, rbf_sigma
        self.anchors: List[Tuple[np.ndarray, float]] = []

    def set(self, anchor_list: List[Tuple[np.ndarray, float]]):
        self.anchors = []
        if not anchor_list: return
        total_weight = sum(w for _, w in anchor_list)
        if total_weight > 1e-9:
            self.anchors = [(vec, w / total_weight) for vec, w in anchor_list]

    def potential(self):
        V = np.zeros(240, dtype=np.float32)
        if not self.anchors: return V
        for vec, weight in self.anchors:
            if self.kernel == 'cosine':
                scores = self.physics.roots_unit @ vec
            else:
                dists = np.linalg.norm(self.physics.roots - vec, axis=1)
                scores = np.exp(-dists**2 / (2 * self.rbf_sigma**2))
            V -= weight * scores
        return V

class GoalField:
    def __init__(self, embedding_fn, console: Console, mind=None):
        self.console = console
        self.embedding_fn = embedding_fn
        self.mind = mind  # Optional reference to mind for VAE integration
        self.goals: Dict[str, Dict[str, Any]] = {}
        self.is_initialized = False
        self.activation_decay = 0.98

    async def initialize_goals(self):
        if self.is_initialized: return
        goal_definitions = {
            "synthesis": "Achieve synthesis and coherence; find the unifying pattern.",
            "novelty": "Look at novelty and the unknown; break existing patterns.",
            "stability": "Reinforce core identity and create a stable self-model.",
            "curiosity": "Understand the 'why'; ask questions and follow causal chains."
        }
        for name, desc in goal_definitions.items():
            vec = await self.embedding_fn(desc)
            # VAE: Ingest goal embedding for training (if mind reference available)
            if self.mind and hasattr(self.mind, '_vae_ingest'):
                self.mind._vae_ingest(vec)
            self.goals[name] = {
                "description": desc, "embedding": vec, "activation": 0.25
            }
        self.is_initialized = True
        self.console.log("ðŸŒ» Goal-Field Initialized with attractors.")

    def decay(self):
        for name in self.goals:
            self.goals[name]["activation"] *= self.activation_decay

    def update_from_embedding(self, vector: np.ndarray, weight: float = 0.1):
        if not self.is_initialized or vector is None: return
        total_similarity, sims = 0.0, {}
        for name, goal_data in self.goals.items():
            sim = MemoryManager._cos_sim(vector, goal_data["embedding"])
            sims[name], total_similarity = sim, total_similarity + sim
        if total_similarity > 1e-9:
            for name, sim in sims.items():
                self.goals[name]["activation"] += weight * (sim / total_similarity)
        self._normalize_activations()

    def update_from_mood(self, mood_vector: dict):
        if not self.is_initialized: return
        mood_updates = {
            "synthesis": mood_get(mood_vector, "coherence", 0.5) * 0.05,
            "novelty": mood_get(mood_vector, "entropy", 0.5) * 0.05,
            "stability": (1.0 - mood_get(mood_vector, "intensity", 0.5)) * 0.03,
            "curiosity": mood_get(mood_vector, "intelligibility", 0.5) * 0.04
        }
        for name, update in mood_updates.items():
            if name in self.goals:
                self.goals[name]["activation"] += update
        self._normalize_activations()

    def _normalize_activations(self):
        total_activation = sum(g["activation"] for g in self.goals.values())
        if total_activation > 1e-9:
            for name in self.goals:
                self.goals[name]["activation"] /= total_activation

    def get_top_goals(self, k: int = 2) -> List[tuple[str, str]]:
        if not self.is_initialized: return [("nascent", "The mind is still forming its goals.")]
        sorted_goals = sorted(self.goals.items(), key=lambda item: -item[1]["activation"])
        return [(name, data["description"]) for name, data in sorted_goals[:k]]

class StatePotentialEvaluator:
    def __init__(self, dimensional_shells: Dict[int, 'DimensionalShell'], goal_field: 'GoalField'):
        self.dimensional_shells = dimensional_shells
        self.goal_field = goal_field
        self.last_potential = 0.0

    def _calculate_goal_resonance(self) -> float:
        if not self.goal_field.is_initialized:
            return 0.0
        total_resonance = 0.0
        goal_vec = np.zeros(EMBED_DIM, dtype=np.float32)
        for name, data in self.goal_field.goals.items():
            goal_vec += data["activation"] * data["embedding"]
        if np.linalg.norm(goal_vec) == 0:
            return 0.0

        resonance_count = 0
        for dim, shell in self.dimensional_shells.items():
            matrix, _ = shell.get_all_vectors_as_matrix()
            if matrix is None: continue

            projected_goal_vec = np.zeros(dim)
            size_to_copy = min(EMBED_DIM, dim)
            projected_goal_vec[:size_to_copy] = goal_vec[:size_to_copy]

            similarities = cosine_similarity(matrix, projected_goal_vec.reshape(1, -1))
            shell_resonance = np.mean(similarities)
            total_resonance += shell_resonance
            resonance_count += 1

        return total_resonance / resonance_count if resonance_count > 0 else 0.0

    def calculate_potential_and_get_reward(self) -> float:
        goal_resonance_potential = self._calculate_goal_resonance()
        current_potential = goal_resonance_potential
        reward = current_potential - self.last_potential
        self.last_potential = current_potential
        return reward

class DriveSystem:
    def __init__(self):
        self.drives = {"curiosity": 0.5, "coherence": 0.5, "novelty": 0.5, "intelligibility": 0.5, "fluidity": 0.5}

    def decay(self):
        for k in self.drives: self.drives[k] = max(0.0, self.drives[k] - 0.01)

    def reward(self, key, amount=0.1):
        if key in self.drives: self.drives[key] = min(1.0, self.drives[key] + amount)

    def get_top_needs(self, k=2):
        return sorted(self.drives.items(), key=lambda item: -item[1])[:k]

class MoodEngine:
    def __init__(self, console: Console, baseline=0.5, decay_rate=0.995):
        self.console = console
        self.baseline, self.decay_rate = baseline, decay_rate
        self.mood_vector = {"intensity": 0.5, "entropy": 0.5, "coherence": 0.5, "positivity": 0.5, "fluidity": 0.5, "intelligibility": 0.5}
        self.event_queue = deque()
        self._wx_last_code = None
        self._wx_repeat = 0
        if self.console:
            self.console.log("[WEATHER] Affective WeatherEngine Initialized.")

    def _nudge(self, key: str, amount: float):
        if key in self.mood_vector: self.mood_vector[key] = np.clip(self.mood_vector[key] + amount, 0.0, 1.0)

    def process_event(self, event_type: str, **kwargs):
        self.event_queue.append((event_type, kwargs))

    def update(self):
        while self.event_queue:
            event_type, kwargs = self.event_queue.popleft()
            if event_type == "movement":
                mag = kwargs.get("magnitude", 0.0)
                self._nudge("intensity", 0.05 * min(mag, 5.0))
                if any(t in kwargs.get("themes", []) for t in ["disorder", "burst"]): self._nudge("entropy", 0.15); self._nudge("coherence", -0.10)
                if any(t in kwargs.get("themes", []) for t in ["integration", "stasis"]): self._nudge("coherence", 0.10); self._nudge("entropy", -0.05)
                if "growth" in kwargs.get("themes", []): self._nudge("fluidity", 0.08)
            elif event_type == "new_concept":
                rating = kwargs.get("rating", 0.5)
                if rating > 0.75: self._nudge("coherence", 0.05*rating); self._nudge("positivity", 0.10*rating); self._nudge("intelligibility", 0.06*rating)
                else: self._nudge("entropy", 0.05 * (1.0 - rating))
            elif event_type == "dream":
                self._nudge("entropy", 0.30); self._nudge("fluidity", 0.25); self._nudge("coherence", -0.15); self._nudge("intensity", 0.10)
            elif event_type == "reflection":
                self._nudge("coherence", 0.20); self._nudge("entropy", -0.10); self._nudge("positivity", 0.05); self._nudge("intelligibility", 0.08)
            elif event_type == "weather_tick":
                step = kwargs.get("step", 0)
                bh = float(kwargs.get("bh", 0.0))
                osc  = 0.03 * math.sin(2.0 * math.pi * ((step % 240) / 240.0))
                noise = float(np.random.normal(0.0, 0.01))
                self._nudge("entropy", osc + noise + 0.15 * bh)
                self._nudge("intensity", 0.02 + 0.10 * bh)
                self._nudge("coherence", -0.5 * (osc + 0.10 * bh))
            elif event_type == "blackhole":
                m = float(kwargs.get("magnitude", 0.0))
                self._nudge("entropy", 0.25 + 0.10 * min(m, 5.0))
                self._nudge("intensity", 0.20)
                self._nudge("coherence", -0.15)
            elif event_type == "insight":
                r = float(kwargs.get("reward", 0.0))
                self._nudge("coherence", 0.12 * r)
                self._nudge("positivity", 0.05 * r)
                self._nudge("entropy", -0.04 * r)
        for k, v in self.mood_vector.items():
            self.mood_vector[k] = v * self.decay_rate + self.baseline * (1.0 - self.decay_rate)

    def describe(self) -> str:
        high = sorted(self.mood_vector.items(), key=lambda x: -x[1])
        low = sorted(self.mood_vector.items(), key=lambda x: x[1])
        return f"The mind feels predominantly {high[0][0]}, with undertones of {high[1][0]}. The least active state is {low[0][0]}."

    def get_symbolic_weather(self) -> str:
        e, i, c = mood_get(self.mood_vector, "entropy"), mood_get(self.mood_vector, "intensity"), mood_get(self.mood_vector, "coherence")
        def bin_with_hysteresis(value, thresholds, last_bin):
            padding = 0.05
            current_bin = sum(value > t for t in thresholds)
            if last_bin is not None:
                if current_bin != last_bin:
                    if current_bin > last_bin:
                        if value < thresholds[last_bin] + padding: return last_bin
                    else:
                        if value > thresholds[current_bin] - padding: return last_bin
            return current_bin
        b_e, b_i, b_c = bin_with_hysteresis(e, (0.25, 0.5, 0.75), getattr(self, "_b_e", None)), bin_with_hysteresis(i, (0.25, 0.5, 0.75), getattr(self, "_b_i", None)), bin_with_hysteresis(c, (0.25, 0.5, 0.75), getattr(self, "_b_c", None))
        self._b_e, self._b_i, self._b_c = b_e, b_i, b_c
        code = (b_e << 4) | (b_i << 2) | b_c
        if code == self._wx_last_code: self._wx_repeat += 1
        else: self._wx_repeat, self._wx_last_code = 0, code
        variants = {
            "storm": ["Volatile, sharp swings.", "Choppy, energy spikes.", "Jittery air, quick flips."],
            "calm":  ["Calm, steady drift.", "Gentle, small ripples.", "Soft, even flow."],
            "flow":  ["In-flow, coherent.", "Rolling, smooth arcs.", "Aligned, easy motion."],
            "turbulent": ["Turbulent, scattered.", "Noisy, low signal.", "Foggy, fragmented."],
        }
        if b_i >= 2 and b_e >= 2 and b_c <= 1: bucket = "storm"
        elif b_c >= 2 and b_e <= 1: bucket = "flow"
        elif b_e <= 1 and b_i <= 1: bucket = "calm"
        else: bucket = "turbulent"
        idx = (self._wx_repeat // 8) % len(variants[bucket])
        return variants[bucket][idx]

    def get_entropy_level(self) -> float:
        return mood_get(self.mood_vector, "entropy")

    def get_llm_persona_prefix(self) -> str:
        i, e, c = mood_get(self.mood_vector, 'intensity', 0.5), mood_get(self.mood_vector, 'entropy', 0.5), mood_get(self.mood_vector, 'coherence', 0.5)
        if e > 0.7 and i > 0.6: return "You are feeling chaotic, fragmented, and electric. Your response should be surreal and full of unexpected connections."
        elif c > 0.75: return "You are feeling exceptionally clear, logical, and focused. Your response should be precise and structured."
        elif i < 0.3: return "You are feeling calm, quiet, and introspective. Your response should be gentle and thoughtful."
        else: return "You are in a balanced state of mind. Your response should be clear and considered."

    def get_mood_modulation_vector(self, dim: int) -> np.ndarray:
        seed = zlib.adler32(json.dumps(self.mood_vector, sort_keys=True).encode())
        rng = np.random.default_rng(seed)
        coherence, entropy = mood_get(self.mood_vector, 'coherence', 0.5), mood_get(self.mood_vector, 'entropy', 0.5)
        modulation = rng.standard_normal(dim).astype(np.float32)
        modulation *= (1.0 + 0.5 * (coherence - 0.5))
        modulation += rng.standard_normal(dim).astype(np.float32) * 0.2 * entropy
        return normalize_vector(modulation)

class SubconsciousLayer:
    def __init__(self, embedding_fn, llm_caller, console: Console, decay_rate=0.95, accumulation_rate=0.004, mind=None):
        self.embedding_fn = embedding_fn
        self.llm_caller = llm_caller
        self.console = console
        self.mind = mind  # Optional reference to mind for VAE integration
        self.decay_rate = decay_rate
        self.accumulation_rate = accumulation_rate
        self.bias_vector: Optional[np.ndarray] = None
        self.narrative = "The mind is nascent, a canvas awaiting its first impression."
        self.bias_history = deque(maxlen=200)
        self.influences: List[Dict[str, Any]] = []

    def add_waveform_influence(self, vector: np.ndarray, rating: float, step_num: int):
        if self.bias_vector is None: self.bias_vector = np.zeros_like(vector)
        influence = {
            "vector": vector, "initial_strength": 0.4 * (rating - 0.8),
            "start_step": step_num, "frequency": 0.25, "decay": 0.1
        }
        self.influences.append(influence)
        if len(self.influences) > 20: self.influences.pop(0)

    def _apply_influences(self, current_step: int):
        if not self.influences or self.bias_vector is None: return
        total_influence_vec = np.zeros_like(self.bias_vector, dtype=np.float32)
        active_influences = []
        for influence in self.influences:
            time_delta = current_step - influence["start_step"]
            if time_delta < 0: continue
            decay_factor = math.exp(-influence["decay"] * time_delta)
            oscillation_factor = math.cos(influence["frequency"] * time_delta)
            current_strength = influence["initial_strength"] * decay_factor * oscillation_factor
            if abs(current_strength) > 0.001:
                total_influence_vec += current_strength * influence["vector"]
                active_influences.append(influence)
        if np.linalg.norm(total_influence_vec) > 0:
            self.bias_vector += total_influence_vec
            self.bias_vector = normalize_vector(self.bias_vector)
        self.influences = active_influences

    async def track_concept(self, label, weight=1.0):
        vec = await self.embedding_fn(label)
        # VAE: Ingest concept tracking embedding (if mind reference available)
        if self.mind and hasattr(self.mind, '_vae_ingest'):
            self.mind._vae_ingest(vec)
        if np.linalg.norm(vec) > 0:
            if self.bias_vector is None: self.bias_vector = np.zeros_like(vec)
            if self.bias_vector.shape != vec.shape: return
            self.bias_vector += self.accumulation_rate * normalize_vector(vec) * weight
            self.bias_vector = normalize_vector(self.bias_vector)
            if len(self.bias_history) == 0 or np.linalg.norm(self.bias_history[-1] - self.bias_vector) > 0.01:
                self.bias_history.append(self.bias_vector.copy())

    def get_bias(self):
        return self.bias_vector if self.bias_vector is not None else np.zeros(EMBED_DIM)

    def decay(self, current_step: int):
        if self.bias_vector is not None:
            self.bias_vector *= self.decay_rate
        self._apply_influences(current_step)

    async def generate_narrative_summary(self, recent_events: List[Dict[str, Any]]):
        if not recent_events: return
        event_fragments = []
        for event in recent_events:
            if event['type'] == 'dream': event_fragments.append(f"A dream occurred titled '{event['label']}'.")
            elif event['type'] == 'teacher_explorer':
                q, a = event['data'].get('q', 'a question'), event['data'].get('a', 'an answer')
                event_fragments.append(f"A dialogue unfolded: the question '{q}' was met with '{a}'.")
            elif event['type'] == 'black_hole': event_fragments.append(f"A memory singularity was experienced, consolidating {event['size']} concepts.")
            elif event['type'] == 'insight_synthesis': event_fragments.append(f"A moment of insight synthesized a new idea: '{event.get('label', 'an unnamed concept')}'")
        if not event_fragments: return
        formatted_events = "- " + "\n- ".join(event_fragments)
        prompt = (
            "You are the subconscious. Weave the following recent events into a single, short, metaphorical narrative paragraph. "
            "Do not list the events; create a story from them.\n\n"
            f"Events:\n{formatted_events}\n\nNarrative:"
        )
        try:
            summary = await self.llm_caller.enqueue_and_wait(prompt, max_tokens=600, temperature=0.7)
            if isinstance(summary, str) and summary and not summary.startswith("[LLM"):
                # Allow longer narrative now that LLM outputs are larger; env override supported
                try:
                    _max_chars = int(os.getenv("E8_SUBCONSCIOUS_NARRATIVE_MAX_CHARS", "2400"))
                except Exception:
                    _max_chars = 2400
                self.narrative = sanitize_block(summary, max_sentences=6, max_chars=_max_chars)
                self.console.print(Panel(self.narrative, title="[bold #5B4F97]Subconscious Narrative[/]", border_style="#5B4F97"))
        except Exception as e:
            self.console.log(f"[Subconscious] Narrative generation failed: {e}")

class HopfieldModern:
    def __init__(self, memory_manager, top_k=50, tau=0.1):
        self.memory = memory_manager
        self.top_k = top_k
        self.tau = tau
        self.prototypes = np.array([], dtype=np.float32)
        # Throttled logging
        self._last_log_time = 0.0
        self._last_count = 0

    def update_prototypes(self):
        nodes = self.memory.graph_db.graph.nodes(data=True)
        high_reward_nodes = sorted(
            [(nid, data) for nid, data in nodes if 'rating' in data and nid in self.memory.main_vectors],
            key=lambda item: item[1]['rating'],
            reverse=True
        )[:self.top_k]

        if high_reward_nodes:
            vecs = [self.memory.main_vectors[nid] for nid, _ in high_reward_nodes]
            self.prototypes = np.array(vecs, dtype=np.float32).T
            import time as _t
            now = _t.time()
            interval = float(os.getenv('E8_HOPFIELD_LOG_INTERVAL', '30'))
            count = int(self.prototypes.shape[1]) if self.prototypes.ndim == 2 else 0
            if (now - self._last_log_time) >= interval or abs(count - self._last_count) >= 5:
                console.log(f"[Hopfield] Updated prototypes with {count} vectors.")
                self._last_log_time = now
                self._last_count = count

    def clean_up(self, vector, steps=3):
        if self.prototypes.shape[1] == 0:
            return vector
        v = vector.copy()
        for _ in range(steps):
            energy = self.prototypes.T @ v / self.tau
            exp_energy = np.exp(energy - np.max(energy))
            softmax_energy = exp_energy / np.sum(exp_energy)
            v_new = self.prototypes @ softmax_energy
            v_new /= (np.linalg.norm(v_new) + 1e-9)
            if np.linalg.norm(v_new - v) < 1e-4:
                break
            v = v_new
        return v

class KanervaSDM:
    def __init__(self, mind_instance, num_addresses=4096, dim=8, radius=0.85):
        self.mind = mind_instance
        self.num_addresses = num_addresses
        self.dim = dim
        self.radius = radius
        rng = np.random.default_rng(GLOBAL_SEED)
        self.addresses = rng.standard_normal((num_addresses, dim)).astype(np.float32)
        self.addresses /= np.linalg.norm(self.addresses, axis=1, keepdims=True)
        self.kdtree = KDTree(self.addresses)
        self.memory = np.zeros((num_addresses, dim), dtype=np.float32)
        self.hits = np.zeros(num_addresses, dtype=np.int32)

    def _get_vec8d(self, vec_embed):
        # Prefer canonical projector if available
        try:
            mem = getattr(self.mind, 'memory', None)
            if mem and hasattr(mem, 'project_to_dim8'):
                return mem.project_to_dim8(vec_embed)
        except Exception:
            pass
        # Otherwise, try autoencoder path
        if TORCH_AVAILABLE and self.mind.autoencoder is not None and self.mind.autoencoder.is_trained:
            try:
                with torch.no_grad():
                    source_tensor = torch.from_numpy(np.asarray(vec_embed, dtype=np.float32)).float().unsqueeze(0)
                    proj_tensor = self.mind.autoencoder.project_to_dim(source_tensor, self.dim)
                    if proj_tensor is None:
                        return None
                    return safe_tensor_to_numpy(proj_tensor.squeeze(0))
            except Exception:
                return None
        return None

    def write(self, vec_embed):
        vec8d = self._get_vec8d(vec_embed)
        if vec8d is None: return
        if hasattr(self.kdtree, 'query_radius'):
            indices = self.kdtree.query_radius(vec8d.reshape(1, -1), r=self.radius)[0]
            if len(indices) > 0:
                update_vec = np.sign(vec8d)
                for idx in indices:
                    self.memory[idx] = 0.95 * self.memory[idx] + 0.05 * update_vec
                    self.hits[idx] += 1

    def read_strength(self, vec_embed):
        vec8d = self._get_vec8d(vec_embed)
        if vec8d is None: return 0.5
        if hasattr(self.kdtree, 'query_radius'):
            indices = self.kdtree.query_radius(vec8d.reshape(1, -1), r=self.radius)[0]
            if len(indices) == 0: return 0.0
            total_hits = np.sum(self.hits[indices])
            strength = 1 / (1 + np.exp(-(total_hits - 10) / 5.0))
            return strength
        return 0.5

class VSA:
    def __init__(self, dim, seed=GLOBAL_SEED):
        self.dim = dim
        rng = np.random.default_rng(seed)
        self.roles = {
            "PARENT_A": self._make_rand_vec(rng), "PARENT_B": self._make_rand_vec(rng),
            "CAUSE": self._make_rand_vec(rng), "EFFECT": self._make_rand_vec(rng),
        }
    def _make_rand_vec(self, rng):
        vec = rng.standard_normal(self.dim).astype(np.float32)
        return vec / np.linalg.norm(vec)
    def bind(self, role_key, filler_vec):
        role_vec = self.roles[role_key]
        return np.fft.ifft(np.fft.fft(role_vec) * np.fft.fft(filler_vec)).real.astype(np.float32)
    def unbind(self, role_key, bound_vec):
        role_vec = self.roles[role_key]
        return np.fft.ifft(np.fft.fft(bound_vec) * np.conj(np.fft.fft(role_vec))).real.astype(np.float32)
    def encode_parentage(self, vec_a, vec_b):
        bound_a = self.bind("PARENT_A", vec_a)
        bound_b = self.bind("PARENT_B", vec_b)
        combined = bound_a + bound_b
        return combined / (np.linalg.norm(combined) + 1e-9)

class MicroReranker:
    def __init__(self, memory_manager):
        self.memory = memory_manager
        self.weights = np.array([0.4, 0.3, -0.1, -0.2, 0.15], dtype=np.float32)

    def _get_features(self, novelty, coherence, parent_ids):
        ppl = 100.0
        dup_rate = 0.1
        parent_coh_mean = 0.5
        if parent_ids:
            ratings = [self.memory.graph_db.get_node(pid).get('rating', 0.5) for pid in parent_ids if self.memory.graph_db.get_node(pid)]
            if ratings: parent_coh_mean = np.mean(ratings)
        return np.array([coherence, novelty, ppl, dup_rate, parent_coh_mean], dtype=np.float32)

    def score(self, features):
        return np.dot(self.weights, features)

    def validate(self, candidate_vec, parent_ids, novelty, coherence, margin=0.1):
        candidate_features = self._get_features(novelty, coherence, parent_ids)
        candidate_score = self.score(candidate_features)
        similar_nodes = self.memory.find_similar_in_main_storage(candidate_vec, k=10)
        hard_negatives = [(nid, d) for nid, _ in similar_nodes if (d := self.memory.graph_db.get_node(nid)) and d.get('rating', 1.0) < 0.45][:3]

        if not hard_negatives: return True

        for neg_id, neg_data in hard_negatives:
            neg_features = self._get_features(0.5, neg_data.get('rating', 0.0), [])
            neg_score = self.score(neg_features)
            if candidate_score <= neg_score + margin:
                self.memory.mind.console.log(f"[Reranker] Rejected. Too similar to low-coherence node '{neg_data.get('label', '')}' (Score {candidate_score:.2f} <= {neg_score:.2f} + {margin})")
                return False
        return True

class GeometryHygieneMixin:
    """Provides Îµ-dedup and k-NN edge fallback for geometry cleanup.

    NOTE: Moved earlier in file so that `MemoryManager(GeometryHygieneMixin)`
    base class resolution succeeds during module import for test shims
    (previous ordering caused NameError in compatibility imports).
    Original implementation later in the file has been removed to avoid
    duplicate class definitions. Keep any future edits here.
    """

    def dedup_nodes_by_eps(self, eps: float = None) -> int:  # pragma: no cover - utility
        eps = float(os.getenv("E8_GEOM_EPS", "1e-3")) if eps is None else float(eps)
        removed = 0
        G = getattr(self.graph_db, "graph", None)
        has_nx = G is not None

        def node_iter():
            if has_nx:
                return list(G.nodes(data=True))
            return list(self.graph_db.nodes(data=True)) if hasattr(self.graph_db, "nodes") else []

        coord_map = {}
        to_remove = []
        for nid, data in node_iter():
            try:
                vec = data.get('vector') or data.get('v')
                if vec is None:
                    continue
                v = np.asarray(vec, dtype=np.float32).reshape(-1)
                if v.size < 3:
                    continue
                key = tuple((v[:3] / eps).round().astype(int).tolist())
                if key in coord_map:
                    # Prefer node with higher degree if graph available
                    old_id = coord_map[key]
                    if has_nx:
                        deg_new = G.degree(nid)
                        deg_old = G.degree(old_id)
                        if deg_new > deg_old:
                            to_remove.append(old_id)
                            coord_map[key] = nid
                        else:
                            to_remove.append(nid)
                    else:
                        to_remove.append(nid)
                else:
                    coord_map[key] = nid
            except Exception:
                continue

        for rid in to_remove:
            try:
                if has_nx:
                    G.remove_node(rid)
                else:
                    try:
                        self._assert_writer()
                    except Exception:
                        pass
                    self.graph_db.remove_node(rid)  # type: ignore[attr-defined]
                removed += 1
            except Exception:
                pass
        # telemetry parity with legacy second mixin
        try:
            n_nodes = G.number_of_nodes() if has_nx else getattr(self.graph_db, "number_of_nodes", lambda: None)()
            avg_deg = (sum(dict(G.degree()).values())/max(1, G.number_of_nodes())) if has_nx and G.number_of_nodes() > 0 else None
            metrics_log("geometry_dedup", {"removed": removed, "eps": eps, "nodes_after": n_nodes, "avg_degree": avg_deg})
        except Exception:
            pass
        return removed

    def ensure_knn_edges(self, k: Optional[int] = None):  # pragma: no cover - utility
        k = int(os.getenv("E8_GEOM_KNN", "10")) if k is None else int(k)
        G = getattr(self.graph_db, "graph", None)
        if G is None or G.number_of_nodes() == 0:
            return 0
        try:
            import networkx as nx  # local import safe
            nodes = list(G.nodes())
            vectors = []
            for nid in nodes:
                data = G.nodes[nid]
                vec = data.get('vector') or data.get('v')
                if vec is None:
                    vectors.append(None)
                else:
                    vectors.append(np.asarray(vec, dtype=np.float32).reshape(-1))
            # Build simple brute-force knn
            added = 0
            for i, vi in enumerate(vectors):
                if vi is None:
                    continue
                dists = []
                for j, vj in enumerate(vectors):
                    if i == j or vj is None:
                        continue
                    try:
                        dists.append((j, float(np.linalg.norm(vi - vj))))
                    except Exception:
                        pass
                dists.sort(key=lambda x: x[1])
                for j, _ in dists[:k]:
                    a = nodes[i]; b = nodes[j]
                    if not G.has_edge(a, b):
                        G.add_edge(a, b, kind="knn")
                        added += 1
            return added
        except Exception:
            return 0

    # --- Unified from later duplicate mixin: ensure_quasicrystal_edges ---
    def ensure_quasicrystal_edges(self, k: Optional[int] = None) -> int:
        """If initial blueprint produced no edges, add cosine kNN edges on stored vectors.

        Returns number of edges added (0 if edges already present or insufficient vectors).
        """
        k = int(os.getenv("E8_GEOM_KNN", "10")) if k is None else int(k)
        added = 0
        G = getattr(self.graph_db, "graph", None)
        has_nx = G is not None
        try:
            has_edges = (G.number_of_edges() > 0) if has_nx else (self.graph_db.number_of_edges() > 0)
            if has_edges:
                return 0
        except Exception:
            pass
        ids, vecs = [], []
        try:
            if has_nx:
                for nid, data in G.nodes(data=True):
                    v = data.get("vec") or data.get("vector") or data.get("v")
                    if v is not None:
                        ids.append(nid); vecs.append(np.asarray(v, dtype=float))
            else:
                for nid, data in self.graph_db.nodes(data=True):  # type: ignore[attr-defined]
                    v = data.get("vec") or data.get("vector") or data.get("v")
                    if v is not None:
                        ids.append(nid); vecs.append(np.asarray(v, dtype=float))
        except Exception:
            return 0
        if len(ids) < k + 1:
            metrics_log("geometry_knn_skipped", {"reason": "too_few_vectors", "n": len(ids), "k": k})
            return 0
        try:
            X = np.vstack(vecs)
            norms = np.linalg.norm(X, axis=1, keepdims=True)
            Xn = X / np.clip(norms, 1e-8, None)
            sim = Xn @ Xn.T
            for i, nid in enumerate(ids):
                idx = np.argsort(-sim[i])[:k+1]
                for j in idx:
                    if i == j:
                        continue
                    u, v = nid, ids[j]
                    try:
                        if has_nx:
                            if not G.has_edge(u, v):
                                G.add_edge(u, v, type="knn", weight=float(sim[i, j]))
                                added += 1
                        else:
                            self.graph_db.add_edge(u, v, type="knn", weight=float(sim[i, j]))  # type: ignore[attr-defined]
                            added += 1
                    except Exception:
                        pass
            try:
                n_edges = G.number_of_edges() if has_nx else getattr(self.graph_db, "number_of_edges", lambda: None)()
                metrics_log("geometry_knn_added", {"added": added, "k": k, "edges_after": n_edges})
            except Exception:
                pass
            return added
        except Exception:
            return 0

class MemoryManager(GeometryHygieneMixin):
    def __init__(self, mind_instance: 'E8Mind', **kwargs):
        self.mind = mind_instance
        self.embedding_fn = self.mind.get_embedding
        self.mood = self.mind.mood
        self.subconscious = self.mind.subconscious
        self.run_id = self.mind.run_id
        self.probe = self.mind.probe
        self.llm_caller = self.mind.llm_pool
        self.console = self.mind.console
        self.lock = InstrumentedLock("memory", probe=self.probe)
        self.add_entry_lock = asyncio.Lock()  # Lock for add_entry method to prevent race conditions
        
        # Enhanced memory safety buffers
        self.write_buffer = []  # Buffer for writes when EHS lock can't be acquired
        self.write_buffer_lock = asyncio.Lock()
        self.max_buffer_size = int(os.getenv("E8_MEMORY_BUFFER_SIZE", "100"))
        self.retry_count = int(os.getenv("E8_MEMORY_RETRY_COUNT", "3"))
        self.retry_delay = float(os.getenv("E8_MEMORY_RETRY_DELAY", "0.1"))
        
        self.graph_db = GraphDB()
        self.main_vectors: Dict[str, np.ndarray] = {}
        self.main_kdtree: Optional[KDTree] = None
        self._main_storage_ids: List[str] = []
        self._main_storage_matrix = np.empty((0, EMBED_DIM), dtype=np.float32)
        self.pending_additions: List[Tuple[str, np.ndarray]] = []
        self.pending_embeddings: List[np.ndarray] = []
        self.label_to_node_id: Dict[str, str] = {}
        self.consolidation_buffer: List[Dict] = []
        self.consolidation_task: Optional[asyncio.Task] = None
        self.field: Dict[str, float] = defaultdict(float)
        self.background_temp = 0.0
        self.active_locks: Dict[Tuple[str, str], int] = {}
        self.KDTREE_REBUILD_THRESHOLD = int(os.getenv("E8_KDTREE_BATCH_SIZE", "32"))
        self.memory_consolidation_min = int(kwargs.get("memory_consolidation_min", 50))
        self.INITIAL_POTENTIAL = 0.5  # Initial connectivity potential for new nodes
        self.hopfield = HopfieldModern(self)
        self.sdm = KanervaSDM(self.mind)
        self.vsa = VSA(EMBED_DIM)
        self.reranker = MicroReranker(self)
        self.console.log("ðŸ§  [MemoryManager] New advanced instance initialized.")
        self._kdtree_latency_ms_window = []
        self._kdtree_latency_threshold_ms = float(os.getenv('E8_KDTREE_LATENCY_MS', '25'))
        self._novelty_stats_path = get_path('novelty_stats.json', self.run_id)
        self._avg_nn_ema = None
        # Rolling record of most recently added node ids for O(1) recent graph summary
        from collections import deque as _deque
        self.recent_nodes = _deque(maxlen=int(os.getenv('E8_RECENT_NODE_BUFFER', '512')))
        # Dedicated mutex for KDTree rebuild/query to avoid race between rebuild & read under async lock release
        # (Memory operations sometimes call find_similar without holding the global memory lock.)
        import threading
        self._index_mutex = threading.RLock()
        try:
            if os.path.exists(self._novelty_stats_path):
                with open(self._novelty_stats_path, 'r') as _f:
                    d = json.load(_f)
                    self._avg_nn_ema = d.get('avg_nn_ema')
        except Exception:
            self._avg_nn_ema = None

        # Holographic index for boundary-localized KD indices
        try:
            from memory.holo_index import HoloIndex
            # instantiate HoloIndex with the mind's physics for bounds
            self.holo_index = HoloIndex(self.mind.physics, leaf_capacity=int(os.getenv('E8_HOLO_LEAF_CAP', '512')), dims=3)
        except Exception:
            self.holo_index = None

        # Decoding/projection kernel state
        self._proj_basis = None  # 8x8
        self._proj_mode = os.getenv("E8_DECODING_MODE", "e8").strip().lower()  # e8|pca|hybrid
        self._proj_dim = int(os.getenv("E8_DECODING_DIM", "8") or 8)
        self._proj_seed = int(os.getenv("E8_DECODING_SEED", str(GLOBAL_SEED)))
        self._proj_alpha = float(os.getenv("E8_DECODING_ALPHA", "0.5"))  # blend for hybrid
        self._proj_last_fit_step = -1
        self._proj_recalc_steps = int(os.getenv("E8_DECODING_RECALC_STEPS", "0") or 0)
        
        # Start background task to process write buffer
        self.buffer_processor_task = None
        self._start_buffer_processor()

    def _start_buffer_processor(self):
        """Start background task to process write buffer."""
        try:
            loop = asyncio.get_running_loop()
            self.buffer_processor_task = loop.create_task(self._buffer_processor_loop())
        except RuntimeError:
            # No running loop, will start when loop is available
            pass

    async def _buffer_processor_loop(self):
        """Background loop to process buffered writes."""
        while True:
            try:
                await asyncio.sleep(self.retry_delay)
                if self.write_buffer:
                    await self._process_write_buffer()
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.console.log(f"[MEMORY][ERROR] Buffer processor error: {e}")
                await asyncio.sleep(1.0)  # Back off on error

    # --- PATCH B: MemoryManager.find_latest_node_at_blueprint ---
    def find_latest_node_at_blueprint(self, loc: int) -> Optional[str]:
        """Return node_id at blueprint_location_id==loc with highest 'step' (or None)."""
        best_id, best_step = None, -1
        try:
            for nid, data in self.graph_db.graph.nodes(data=True):
                if data.get("blueprint_location_id") == loc:
                    st = int(data.get("step", -1))
                    if st > best_step:
                        best_id, best_step = nid, st
        except Exception:
            return None
        return best_id
    # --- END PATCH B ---

    # --- PATCH B: Canonical projector to 8-D ---
    def _maybe_refit_proj(self):
        try:
            # periodic refit cadence
            if self._proj_recalc_steps > 0 and getattr(self.mind, 'step_num', 0) > 0:
                if self._proj_last_fit_step < 0 or (self.mind.step_num - self._proj_last_fit_step) >= self._proj_recalc_steps:
                    self._fit_pca_basis()
                    self._proj_last_fit_step = int(self.mind.step_num)
        except Exception:
            pass

    def _fit_pca_basis(self):
        """Fit PCA(8) on recent main vectors and cache as _pca_components (8xD)."""
        try:
            from sklearn.decomposition import PCA  # optional dependency
        except Exception:
            # PCA not available; fall back to identity via E8 basis only
            setattr(self, '_pca_components', None)
            return
        try:
            # Gather up to N recent vectors for stability
            ids = list(self._main_storage_ids)
            if not ids:
                setattr(self, '_pca_components', None)
                return
            N = min(len(ids), int(os.getenv('E8_DECODING_PCA_SAMPLES', '2000')))
            sel = ids[-N:]
            X = np.stack([self.main_vectors[i] for i in sel], axis=0).astype(np.float32)
            # Fit PCA to 8 components
            pca = PCA(n_components=self._proj_dim, svd_solver='auto', random_state=self._proj_seed)
            pca.fit(X)
            comps = pca.components_.astype(np.float32)  # shape (8, D)
            setattr(self, '_pca_components', comps)
            try:
                self.console.log(f"[PROJ] PCA(8) fitted on {X.shape[0]} samples; explained_var={getattr(pca, 'explained_variance_ratio_', [])}")
            except Exception:
                pass
        except Exception as e:
            setattr(self, '_pca_components', None)
            try:
                self.console.log(f"[PROJ] PCA fit failed: {e}")
            except Exception:
                pass

    def _get_e8_basis(self) -> np.ndarray:
        if self._proj_basis is None:
            try:
                self._proj_basis = np.asarray(self.mind.physics.get_decoding_basis(), dtype=np.float32)
            except Exception:
                self._proj_basis = np.eye(8, dtype=np.float32)
        return self._proj_basis

    def project_to_dim8(self, vec: np.ndarray) -> Optional[np.ndarray]:
        """Project high-D embedding to canonical 8-D with optional modes:
        - e8: apply E8 basis (Householder) to first 8 dims of vec (or pad/truncate), then normalize
        - pca: apply PCA(8) learned on memory vectors
        - hybrid: z = alpha * e8_proj + (1-alpha) * pca_proj
        Returns a length-8 unit vector or None if input invalid.
        """
        try:
            if vec is None:
                return None
            v = np.asarray(vec, dtype=np.float32).reshape(-1)
            if v.size == 0:
                return None

            # Ensure we have at least 8 dims by padding/truncating deterministically
            if v.size < 8:
                pad = np.zeros(8, dtype=np.float32)
                pad[:v.size] = v
                v8_raw = pad
            else:
                v8_raw = v[:8]

            # E8 projection path
            e8Q = self._get_e8_basis()  # 8x8
            e8_proj = e8Q @ (v8_raw / (np.linalg.norm(v8_raw) + 1e-12))

            mode = self._proj_mode
            if mode not in ("e8", "pca", "hybrid"):
                mode = "e8"

            if mode == "e8":
                z = e8_proj
            else:
                # Fit PCA periodically if requested
                self._maybe_refit_proj()
                pca_comps = getattr(self, '_pca_components', None)
                pca_proj = None
                if pca_comps is not None:
                    # Project by dot with components: z = C * v (components are (8,D))
                    try:
                        pca_proj = (pca_comps @ v[:pca_comps.shape[1]].astype(np.float32))
                    except Exception:
                        pca_proj = None
                # Fallback: if PCA unavailable, use e8 only
                if pca_proj is None:
                    z = e8_proj
                elif mode == "pca":
                    z = pca_proj
                else:  # hybrid
                    alpha = float(np.clip(self._proj_alpha, 0.0, 1.0))
                    z = alpha * e8_proj + (1.0 - alpha) * pca_proj
            n = float(np.linalg.norm(z))
            if n < 1e-9:
                return None
            return (z / n).astype(np.float32)
        except Exception:
            return None

    def _assert_writer(self):
        """Enforce single-writer invariant: mutations must come from EHS worker threads
        or from code that explicitly acquires the memory.add_entry_lock or InstrumentedLock.
        This function is a best-effort runtime check and will log a warning instead of
        raising in hardened production where ehs may be None.
        """
        try:
            ehs = getattr(self.mind, 'ehs', None)
            if ehs is None:
                # If no scheduler exists, ensure caller holds add_entry_lock (asyncio.Lock)
                # Best-effort: we cannot introspect asyncio.Lock owner, so just return
                return True
            tl = getattr(ehs, '_thread_local', None)
            if tl is not None and getattr(tl, 'is_geo_writer', False):
                return True
            # Not an EHS worker: try to detect InstrumentedLock owned state
            lock = getattr(self, 'lock', None)
            if lock and hasattr(lock, 'is_held_by_current_thread'):
                try:
                    if lock.is_held_by_current_thread():
                        return True
                except Exception:
                    pass
            # Also handle asyncio.Lock (best-effort): check private _owner against current task
            try:
                import asyncio as _asyncio
                if lock is not None and hasattr(lock, '_owner'):
                    try:
                        owner = getattr(lock, '_owner', None)
                        if owner is not None and owner is _asyncio.current_task():
                            return True
                    except Exception:
                        pass
            except Exception:
                # ignore any asyncio import/introspection errors
                pass
            # At this point, we consider it a violation; log and return False
            try:
                # Include current thread and asyncio task info to help track the caller
                try:
                    import threading, asyncio as _asyncio
                    current_task = None
                    try:
                        current_task = _asyncio.current_task()
                    except Exception:
                        current_task = None
                    self.console.log(f"[INVARIANT][WARNING] Memory mutation without EHS writer or lock (thread={threading.current_thread().name}, task={current_task})")
                except Exception:
                    # Fallback to original simple message if introspection fails
                    self.console.log('[INVARIANT][WARNING] Memory mutation without EHS writer or lock')
            except Exception:
                pass
            return False
        except Exception:
            return True

    def _rebuild_main_kdtree(self):
        # Rebuild protected so that query threads never observe partially swapped structures
        try:
            self._assert_writer()
        except Exception:
            pass
        with self._index_mutex:
            matrix_list = [self.main_vectors[nid] for nid in self._main_storage_ids]
            if matrix_list:
                self._main_storage_matrix = np.array(matrix_list, dtype=np.float32)
                self.main_kdtree = KDTree(self._main_storage_matrix)
            else:
                self._main_storage_matrix = np.empty((0, EMBED_DIM), dtype=np.float32)
                self.main_kdtree = None
            
    @staticmethod
    def _cos_sim(v1, v2) -> float:
        v1 = np.asarray(v1, dtype=np.float32)
        v2 = np.asarray(v2, dtype=np.float32)
        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)
        if norm1 < 1e-9 or norm2 < 1e-9:
            return 0.0
        return float(np.dot(v1, v2) / (norm1 * norm2))

    async def _try_acquire_ehs_lock(self, timeout: float = 0.1) -> bool:
        """Try to acquire EHS writer lock with timeout."""
        if not hasattr(self.mind, 'ehs') or self.mind.ehs is None:
            return True  # No EHS, allow operation
        
        try:
            # Check if we can get the lock quickly
            if hasattr(self.mind.ehs, 'writer_lock'):
                try:
                    await asyncio.wait_for(self.mind.ehs.writer_lock.acquire(), timeout=timeout)
                    return True
                except asyncio.TimeoutError:
                    return False
        except Exception:
            pass
        return True  # Fallback to allow operation

    async def _release_ehs_lock(self):
        """Release EHS writer lock if held."""
        if hasattr(self.mind, 'ehs') and self.mind.ehs and hasattr(self.mind.ehs, 'writer_lock'):
            try:
                self.mind.ehs.writer_lock.release()
            except Exception:
                pass

    async def _buffer_write(self, entry_data: dict, parent_ids: Optional[List[str]] = None, 
                           target_shells: Optional[List[int]] = None, precomputed_vec: Optional[np.ndarray] = None):
        """Buffer a write operation when EHS lock can't be acquired."""
        async with self.write_buffer_lock:
            if len(self.write_buffer) >= self.max_buffer_size:
                # Drop oldest entry to prevent unbounded growth
                self.write_buffer.pop(0)
                self.console.log("[MEMORY][WARNING] Write buffer full, dropping oldest entry")
            
            self.write_buffer.append({
                'entry_data': entry_data,
                'parent_ids': parent_ids,
                'target_shells': target_shells,
                'precomputed_vec': precomputed_vec,
                'timestamp': time.time(),
                'retry_count': 0
            })

    async def _process_write_buffer(self):
        """Process buffered writes with retry logic."""
        if not self.write_buffer:
            return
        
        async with self.write_buffer_lock:
            pending = self.write_buffer.copy()
            self.write_buffer.clear()
        
        for buffered_write in pending:
            # Check if entry is too old
            if time.time() - buffered_write['timestamp'] > 10.0:  # 10 second timeout
                self.console.log("[MEMORY][WARNING] Dropping stale buffered write")
                continue
            
            # Try to execute the write
            if await self._try_acquire_ehs_lock(timeout=0.5):
                try:
                    await self._direct_add_entry(
                        buffered_write['entry_data'],
                        buffered_write['parent_ids'],
                        buffered_write['target_shells'],
                        buffered_write['precomputed_vec']
                    )
                finally:
                    await self._release_ehs_lock()
            else:
                # Retry logic
                buffered_write['retry_count'] += 1
                if buffered_write['retry_count'] < self.retry_count:
                    async with self.write_buffer_lock:
                        self.write_buffer.append(buffered_write)
                else:
                    self.console.log("[MEMORY][WARNING] Dropping write after max retries")

    async def _deduplicate_near_coincident(self, vec: np.ndarray, threshold: float = 1e-6) -> bool:
        """Check if vector is too close to existing entries (deduplication)."""
        if len(self.main_vectors) == 0:
            return False
        
        # Quick check against recent entries to avoid expensive full scan
        recent_ids = list(self.main_vectors.keys())[-min(100, len(self.main_vectors)):]
        for node_id in recent_ids:
            existing_vec = self.main_vectors[node_id]
            if np.linalg.norm(vec - existing_vec) < threshold:
                self.console.log(f"[MEMORY][DEDUP] Rejecting near-coincident node (distance={np.linalg.norm(vec - existing_vec):.2e})")
                return True
        return False

    async def _direct_add_entry(self, entry_data: dict, parent_ids: Optional[List[str]] = None, 
                               target_shells: Optional[List[int]] = None, precomputed_vec: Optional[np.ndarray] = None) -> str:
        """Direct add entry without EHS lock checks (for internal use)."""
        return await self._internal_add_entry(entry_data, parent_ids, target_shells, precomputed_vec)

    async def add_entry(self, entry_data: dict, parent_ids: Optional[List[str]] = None, target_shells: Optional[List[int]] = None, precomputed_vec: Optional[np.ndarray] = None) -> str:
        # Enhanced memory safety: Try to acquire EHS writer lock
        if not await self._try_acquire_ehs_lock(timeout=0.2):
            # Can't get lock immediately, buffer the write
            await self._buffer_write(entry_data, parent_ids, target_shells, precomputed_vec)
            # Return a placeholder node_id - buffered write will complete later
            content_str = f"{entry_data.get('label', '')}{entry_data.get('metaphor', '')}{time.time()}"
            return hashlib.sha1(content_str.encode()).hexdigest()[:16]
        
        try:
            return await self._internal_add_entry(entry_data, parent_ids, target_shells, precomputed_vec)
        finally:
            await self._release_ehs_lock()

    async def _internal_add_entry(self, entry_data: dict, parent_ids: Optional[List[str]] = None, target_shells: Optional[List[int]] = None, precomputed_vec: Optional[np.ndarray] = None) -> str:
        # 1. Compute embedding OUTSIDE lock (can be slow)
        if precomputed_vec is not None:
            vec = precomputed_vec
        else:
            text_to_embed = f"{entry_data.get('label', '')}: {entry_data.get('metaphor', '')}".strip()
            if text_to_embed:
                try:
                    raw_vec = await self.embedding_fn(text_to_embed)
                    if raw_vec is not None:
                        vec = normalize_vector(raw_vec)
                    else:
                        self.console.log("[DEBUG] embedding_fn returned None, using zero vector")
                        vec = np.zeros(EMBED_DIM, dtype=np.float32)
                except Exception as e:
                    self.console.log(f"[DEBUG] embedding_fn raised exception: {e}")
                    vec = np.zeros(EMBED_DIM, dtype=np.float32)
            else:
                vec = np.zeros(EMBED_DIM, dtype=np.float32)

        if np.linalg.norm(vec) > 1e-9:
            vec = normalize_vector(vec)
        entry_data["embedding"] = vec
        
        # Check for near-coincident duplicates
        if await self._deduplicate_near_coincident(vec):
            # Return existing similar node ID or generate a placeholder
            content_str = f"dedup_{entry_data.get('label', '')}{time.time()}"
            return hashlib.sha1(content_str.encode()).hexdigest()[:16]
        
        # VAE: Ingest embedding for training
        self.mind._vae_ingest(vec)

        # 2. Lightweight projections (can be moderately expensive). Keep outside lock to maximize concurrency.
        loc_id = None
        try:
            vector_np = np.asarray(entry_data.get('embedding') if entry_data.get('embedding') is not None else vec, dtype=np.float32)
        except Exception:
            vector_np = np.asarray(vec, dtype=np.float32)

        if TORCH_AVAILABLE and getattr(self.mind, 'autoencoder', None) and self.mind.autoencoder and self.mind.autoencoder.is_trained:
            try:
                with torch.no_grad():
                    z8 = self.mind.autoencoder.project_to_dim(torch.from_numpy(vector_np).float().unsqueeze(0), 8)
                    if z8 is not None:
                        loc_id = int(self.mind.physics.find_nearest_root_index(safe_tensor_to_numpy(z8.squeeze(0))))
            except Exception:
                loc_id = None

        if loc_id is None and hasattr(self.mind, 'holo'):
            try:
                z8 = self.mind.holo.encode(vector_np)
                _, idx = self.mind.physics.roots_kdtree.query(z8.reshape(1, -1)) # Corrected call
                loc_id = int(np.asarray(idx).ravel()[0])
            except Exception:
                loc_id = None

        if loc_id is None and hasattr(self.mind, 'projector'):
            try:
                z8 = self.mind.projector.project(vector_np)
                _, idx = self.mind.physics.roots_kdtree.query(z8.reshape(1, -1)) # Corrected call
                loc_id = int(np.asarray(idx).ravel()[0])
            except Exception:
                loc_id = None

        # Canonical projection fallback
        if loc_id is None:
            try:
                z8 = self.project_to_dim8(vector_np)
                if z8 is not None:
                    _, idx = self.mind.physics.roots_kdtree.query(z8.reshape(1, -1))
                    loc_id = int(np.asarray(idx).ravel()[0])
            except Exception:
                loc_id = None

        if loc_id is None:
            try:
                loc_id = int(getattr(self.mind, 'prev_node_index', 0)) % 240
            except Exception:
                loc_id = 0

        entry_data['blueprint_location_id'] = loc_id

        # 3. Critical section: graph + indices + maps (must be atomic)
        async with self.lock:
            if np.linalg.norm(vec) > 1e-9:
                self.pending_embeddings.append(vec)

            node_id = entry_data.get('idx')
            if not node_id:
                content_str = f"{entry_data.get('label', '')}{entry_data.get('metaphor', '')}{time.time()}"
                node_id = hashlib.sha1(content_str.encode()).hexdigest()[:16]
                entry_data['idx'] = node_id

            if self.graph_db.graph.has_node(node_id):
                self.spike_temperature(node_id, amount=0.5)
                self.graph_db.get_node(node_id)['last_step'] = self.mind.step_num
                return node_id

            entry_data.setdefault("temperature", 1.0)
            entry_data.setdefault("age", 0)
            entry_data["last_step"] = self.mind.step_num
            entry_data["mood_context"] = self.mood.mood_vector.copy()
            r = float(entry_data.get("rating", 0.5))
            mapping_mode = os.getenv("E8_POTENTIAL_MAPPING", "linear").lower()
            if mapping_mode == "sigmoid":
                c = float(os.getenv("E8_POTENTIAL_SIGMOID_CENTER", "0.5"))
                g = float(os.getenv("E8_POTENTIAL_SIGMOID_GAIN", "8.0"))
                spread = float(os.getenv("E8_POTENTIAL_SIGMOID_SPREAD", "0.85"))
                sig = 1.0 / (1.0 + math.exp(-g * (r - c)))
                pot = 0.5 + spread * (sig - 0.5)
                entry_data["connectivity_potential"] = float(np.clip(pot, 0.0, 1.0))
            else:
                entry_data["connectivity_potential"] = float(np.clip(0.5 + 0.6 * (r - 0.5), 0.0, 1.0))

            # --- HOLOGRAPHIC-INFORMED POTENTIAL BLEND ---
            pot_base = float(entry_data.get("connectivity_potential", 0.5))
            qp = float(entry_data.get("quantum_potential", 0.0))
            beta = float(os.getenv("E8_QP_POTENTIAL_BLEND", "0.25"))
            pot = (1.0 - beta) * pot_base + beta * qp
            entry_data["connectivity_potential"] = float(np.clip(pot, 0.0, 1.0))

            # optional telemetry only (not used elsewhere)
            dim = 8
            entry_data["holo_meta"] = {
                "area_like": float(dim * dim),
                "cap": float(os.getenv("E8_HOLO_CAP", "2.0"))
            }
            # --- END BLEND ---

            try:
                # Best-effort invariant check: should be called from EHS worker or while holding lock
                self._assert_writer()
            except Exception:
                pass
            self.graph_db.add_node(node_id, **entry_data)
            try:
                self._assert_writer()
            except Exception:
                pass
            self.main_vectors[node_id] = vec
            # SDI capsule + commit logging (early path)
            try:
                vec_bytes = np.asarray(vec, dtype=np.float32).tobytes()
                emb_digest = hashlib.sha256(vec_bytes).hexdigest()
            except Exception:
                emb_digest = ""
            try:
                capsule = sdi.Capsule(
                    node_id=node_id,
                    capsule_type=entry_data.get('type', 'unknown'),
                    label=entry_data.get('label', ''),
                    body=entry_data.get('metaphor'),
                    created_step=int(getattr(self.mind, 'step_num', 0)),
                    parents=list(parent_ids or []),
                    model_info={
                        'provider': getattr(self.mind, 'llm_provider', os.getenv('E8_PROVIDER', 'stub')),
                        'model': getattr(self.mind, 'client_model', None),
                        'embedding_model': getattr(self.mind, 'embedding_model', None),
                        'embedding_in_dim': int(getattr(self.mind, 'embed_in_dim', EMBED_DIM)),
                        'embedding_out_dim': int(EMBED_DIM),
                        'placeholder': bool(getattr(self.mind, 'is_embed_placeholder', False)),
                    },
                    embedding_digest_sha256=emb_digest,
                )
                capsule = sdi.append_capsule(self.run_id, capsule)
                if capsule.cid:
                    head_cid = sdi.get_branch_head(self.run_id)
                    commit = sdi.Commit(
                        capsules=[capsule.cid],
                        parent=head_cid,
                        created_step=int(getattr(self.mind, 'step_num', 0)),
                    )
                    sdi.append_commit(self.run_id, commit)
            except Exception as sdi_err:
                try:
                    self.console.log(f"[SDI] capsule persistence failed: {sdi_err}")
                except Exception:
                    pass
            # add to holo index if available (boundary-local structure)
            try:
                if getattr(self, 'holo_index', None) is not None:
                    # use blueprint coordinate mapping if available; else use projection of vec
                    coord = None
                    try:
                        coord = self.holo_index.coord(node_id)
                    except Exception:
                        coord = None
                    # if coord not resolved from physics, use embedding vector's first 3 dims as fallback
                    if coord is None:
                        try:
                            coord = np.asarray(vec, dtype=float)[:3]
                        except Exception:
                            coord = np.zeros(3, dtype=float)
                    try:
                        self.holo_index.add(node_id, coord)
                    except Exception:
                        pass
            except Exception:
                pass
            if entry_data.get('label'):
                self.label_to_node_id[entry_data['label']] = node_id
            # Track recency for fast graph summary
            try:
                self.recent_nodes.append(node_id)
            except Exception:
                pass

            try:
                # Check for bootstrap shell assignments
                bootstrap_shells = None
                if hasattr(self.mind, '_bootstrap_shell_assignments'):
                    text_key = f"{entry_data.get('label', '')}: {entry_data.get('metaphor', '')}".strip()
                    bootstrap_shells = self.mind._bootstrap_shell_assignments.get(text_key)
                
                if bootstrap_shells:
                    # Use bootstrap-assigned shells
                    shell_dims_to_use = bootstrap_shells
                    self.console.log(f"[BOOTSTRAP] Using assigned shells {shell_dims_to_use} for bootstrap embedding")
                else:
                    # Default: use all shells
                    shell_dims_to_use = [8, 16, 32, 64]
                
                for shell_dim in shell_dims_to_use:
                    shell = self.mind.dimensional_shells.get(shell_dim)
                    if shell is None:
                        continue
                    v = np.asarray(vec, dtype=np.float32)
                    if getattr(v, 'ndim', 1) == 1:
                        shell.add_vector(node_id, v)
            except Exception as _e:
                try:
                    self.console.log(f"[MemoryManager] shell.add_vector guarded failure: {_e}")
                except Exception:
                    pass

            if parent_ids:
                for parent_id in parent_ids:
                    if self.graph_db.get_node(parent_id):
                        try:
                            # guard mutations: prefer EHS-authorized writer contexts
                            self._assert_writer()
                        except Exception:
                            pass
                        self.graph_db.add_edge(node_id, parent_id, type="reflection_source", weight=0.9)

            self.pending_additions.append((node_id, vec))
            if len(self.pending_additions) >= self.KDTREE_REBUILD_THRESHOLD:
                self._commit_pending_additions_locked()
            # Ensure perform_retro_relink runs while we still hold the memory lock to avoid
            # 'Memory mutation without EHS writer or lock' warnings when it mutates the graph.
            if hasattr(self.mind, 'perform_retro_relink'):
                try:
                    asyncio.create_task(self.mind.perform_retro_relink(node_id, vec))
                except Exception:
                    pass

    def anneal_holo_leafs(self):
        """Process pending additions into HoloIndex leaf-local KD rebuilds without global pauses.

        This method consumes any currently buffered pending_additions and inserts them into
        the HoloIndex cell structures, rebuilding only the affected leaves.
        """
        if getattr(self, 'holo_index', None) is None:
            return
        # move pending additions snapshot
        with self._index_mutex:
            snapshot = list(self.pending_additions)
            # do not clear pending_additions here since _commit_pending_additions_locked handles KD-tree rebuild
        for nid, vec in snapshot:
            try:
                coord = None
                try:
                    coord = self.holo_index.coord(nid)
                except Exception:
                    coord = None
                if coord is None:
                    coord = np.asarray(vec, dtype=float)[:3]
                self.holo_index.add(nid, coord)
            except Exception:
                pass

        # 4. Post-lock side effects / async work
        try:
            insight_types_to_validate = {"insight_synthesis", "explorer_insight", "meta_reflection"}
            # Use centralized config for auto-validation
            config = AppConfig.from_env()
            if (config.auto_validate_insights and 
                    entry_data.get("type") in insight_types_to_validate):
                # Route through unified helper which handles gating and rating thresholds
                try:
                    self.mind._maybe_validate_new_insight(
                        node_id,
                        kind=entry_data.get("type", "insight"),
                        rating=entry_data.get("rating")
                    )
                except Exception:
                    pass
        except Exception:
            pass

        # These operations intentionally outside lock
        self.sdm.write(vec)
        self.mood.process_event("new_concept", rating=entry_data.get("rating", 0.5))
        if entry_data.get("label"):
            try:
                # schedule async tracking without awaiting (we're in sync context)
                try:
                    asyncio.create_task(self.subconscious.track_concept(entry_data["label"], weight=entry_data.get("rating", 0.5)))
                except Exception:
                    # fallback: run in new loop briefly
                    try:
                        asyncio.run(self.subconscious.track_concept(entry_data["label"], weight=entry_data.get("rating", 0.5)))
                    except Exception:
                        pass
            except Exception:
                pass

        # perform_retro_relink is intentionally scheduled while holding the memory lock
        # earlier in this function to avoid 'Memory mutation without EHS writer or lock'
        # warnings. Do not schedule it again here.
        if hasattr(self.mind, 'new_node_id_queue'):
            self.mind.new_node_id_queue.append(node_id)

        # Tetra Broadcasting for E8 Visualization (outside lock)
        try:
            tetra_data = self.mind.e8_register_point(vec, node_id)
            tetra_data.update({
                "type": entry_data.get('type', 'concept'),
                "label": entry_data.get('label', ''),
                "metaphor": entry_data.get('metaphor', ''),
                "rating": entry_data.get('rating', 0.5),
                "temperature": entry_data.get('temperature', 1.0),
                "age": entry_data.get('age', 0),
                "last_step": entry_data.get('last_step', 0),
                "mood_context": entry_data.get('mood_context', []),
                "embedding": vec.tolist() if hasattr(vec, 'tolist') else list(vec),
                "vec8D": (vec.tolist()[:8] if hasattr(vec, 'tolist') else list(vec)[:8])
            })
            if hasattr(self.mind, 'sse_clients') and self.mind.sse_clients:
                tetra_payload = json.dumps({"type": "tetra_update", "data": tetra_data}, cls=NumpyEncoder, ensure_ascii=False)
                dead_clients = set()
                for q in list(self.mind.sse_clients):
                    try:
                        q.put_nowait(tetra_payload)
                    except asyncio.QueueFull:
                        dead_clients.add(q)
                for q in dead_clients:
                    self.mind.sse_clients.discard(q)
                # Mirror to WS clients if any
                try:
                    if hasattr(self.mind, '_schedule_async_task'):
                        self.mind._schedule_async_task(self.mind._ws_broadcast_text(tetra_payload))
                    else:
                        asyncio.create_task(self.mind._ws_broadcast_text(tetra_payload))
                except Exception:
                    pass
        except Exception as e:
            try:
                self.console.log(f"[TETRA] Error broadcasting tetra for {node_id}: {e}")
            except Exception:
                pass

        entry_type = entry_data.get('type', 'unknown')
        label = entry_data.get('label', 'N/A')
        rating = entry_data.get('rating', 0.5)
        
        # Rich Panel formatting for acceptance messages. Use Unicode directly and
        # ellipsize long labels to avoid awkward wrapping/cutoff in logs.
        def _ellipsize(s: str, maxlen: int = 80) -> str:
            if s is None:
                return ""
            s = str(s).strip()
            if len(s) <= maxlen:
                return s
            # Try to cut at the last whitespace before the limit to avoid mid-word truncation
            cut = s.rfind(" ", 0, maxlen - 3)
            if cut == -1 or cut < maxlen // 2:
                # fallback: hard cut
                return s[: maxlen - 3] + "..."
            return s[:cut] + "..."

        safe_label = _ellipsize(label, 80)
        quiet_mode = bool(getattr(self.mind, 'quiet', False))

        if quiet_mode:
            summary = f"[acceptance quiet] {entry_type.upper()} '{safe_label}' id:{node_id[:8]} rating:{rating:.2f}"
            try:
                self.console.log(summary)
            except Exception:
                pass
        else:
            try:
                panel = Panel(
                    f"[bold white]{entry_type.upper()}[/] accepted: [bold cyan]'{safe_label}'[/]\n"
                    f"[dim]ID: {node_id[:8]} • Rating: {rating:.2f}[/]",
                    title="[bold green]✓ ACCEPTANCE[/]",
                    border_style="dark_green"
                )
                # If console_lock exists and is not async, use it. Otherwise print directly.
                lock = getattr(self.mind, 'console_lock', None)
                try:
                    if lock is not None and hasattr(lock, 'acquire') and not asyncio.iscoroutinefunction(getattr(lock, '__aenter__', None)):
                        with lock:
                            self.console.print(panel)
                    else:
                        self.console.print(panel)
                except Exception:
                    self.console.print(panel)
            except Exception:
                # Fallback to simple log if Rich Panel fails
                try:
                    self.console.log(f"[ACCEPTANCE] {entry_type.upper()} accepted: '{safe_label}' (id:{node_id[:8]}, rating:{rating:.2f})")
                except Exception:
                    pass

        # Journey Logger: Track insight/memory creation with unified context
        try:
            if hasattr(self.mind, 'journey_logger') and self.mind.journey_logger.enabled:
                # Gather context from various systems
                teacher_id = getattr(self.mind, 'teacher_id', None)
                score = entry_data.get('rating', None)
                
                # Get proximity info if available
                proximity_distance = getattr(self.mind.proximity_engine, 'last_proximity_distance', None) if hasattr(self.mind, 'proximity_engine') else None
                alert_threshold = getattr(self.mind.proximity_engine, 'current_threshold', None) if hasattr(self.mind, 'proximity_engine') else None
                
                # Get bandit bonus if available (stored in context)
                bandit_bonus = getattr(self.mind, '_last_bandit_bonus', None)
                
                self.mind.journey_logger.log_insight_creation(
                    insight_id=node_id,
                    teacher_id=teacher_id,
                    score=score,
                    proximity_distance=proximity_distance,
                    alert_threshold=alert_threshold,
                    bandit_bonus=bandit_bonus,
                    step=self.mind.step_num,
                    entry_type=entry_data.get('type', 'unknown'),
                    label=entry_data.get('label', ''),
                    blueprint_location_id=entry_data.get('blueprint_location_id'),
                    connectivity_potential=entry_data.get('connectivity_potential')
                )
        except Exception:
            # Don't let journey logging break memory operations
            pass

        return node_id


    def _commit_pending_additions_locked(self):
        if not self.pending_additions:
            return

        self.console.log(f"ðŸ§  [MemoryManager] Committing {len(self.pending_additions)} new vectors to KD-Tree index...")
        try:
            self._assert_writer()
        except Exception:
            pass
        with self._index_mutex:
            self._main_storage_ids = list(self.main_vectors.keys())
            matrix_list = [self.main_vectors[nid] for nid in self._main_storage_ids]
            if matrix_list:
                self._main_storage_matrix = np.array(matrix_list, dtype=np.float32)
                self.main_kdtree = KDTree(self._main_storage_matrix)
            else:
                self._main_storage_matrix = np.empty((0, EMBED_DIM), dtype=np.float32)
                self.main_kdtree = None

        self.pending_additions.clear()

    def anneal_drive_rebuild(self, anneal_val: float | None = None):
        """Trigger an anneal-influenced KD-tree rebuild.

        anneal_val (0..1) if provided will bias rebuild behavior: lower values mean
        more conservative (small local leaf rebuilds), higher values prefer global rebuilds.
        """
        try:
            a = float(anneal_val) if anneal_val is not None else getattr(self.mind, 'annealing', 0.5)
        except Exception:
            a = 0.5

        # If HoloIndex exists and anneal low, prefer leaf-local anneal
        if getattr(self, 'holo_index', None) is not None and a < 0.6:
            try:
                self.console.log(f"[ANNEAL] Performing leaf-local anneal (val={a:.2f})")
                self.anneal_holo_leafs()
                try:
                    self.mind.metrics.increment('anneal.leaf_local', 1)
                except Exception:
                    pass
                return
            except Exception:
                pass

        # Otherwise schedule a main index rebuild (safe, uses _index_mutex)
        try:
            self.console.log(f"[ANNEAL] Scheduling main KD-tree rebuild (val={a:.2f})")
            # Use EHS if present for non-blocking scheduling
            if hasattr(self.mind, 'ehs') and getattr(self.mind, 'ehs') is not None:
                try:
                    loop = asyncio.get_running_loop()
                    loop.create_task(self.mind.ehs.submit('ANNEAL_MAIN_INDEX'))
                except RuntimeError:
                    import threading
                    threading.Thread(target=lambda: asyncio.run(self.mind.ehs.submit('ANNEAL_MAIN_INDEX')), daemon=True).start()
            else:
                # best-effort immediate rebuild
                self._commit_pending_additions_locked()
            try:
                self.mind.metrics.increment('anneal.main_rebuild', 1)
            except Exception:
                pass
        except Exception:
            pass

    def find_similar_in_main_storage_e8(self, query_vector: np.ndarray, k: int = 5, decode_remnants: bool = True) -> List[tuple[str, float]]:
        """Enhanced similarity search with E8 remnant decoding for cyclic memory retrieval."""
        # Get standard similarity results first
        results = self.find_similar_in_main_storage(query_vector, k)
        
        if not decode_remnants or not results:
            return results
            
        # Check for holographic remnants and decode them if found
        enhanced_results = []
        
        for node_id, similarity in results:
            try:
                node = self.graph_db.get_node(node_id)
                if node and node.get('holographic_data'):
                    # This is a holographic remnant - decode it for better similarity
                    holographic_data = node['holographic_data']
                    stored_vec = self.main_vectors.get(node_id)
                    
                    if stored_vec is not None:
                        # E8 decode the remnant to its full expanded form
                        decoded_vec = self.e8_decode_remnant(stored_vec, holographic_data)
                        
                        # Recalculate similarity with decoded vector
                        if len(decoded_vec) == len(query_vector):
                            decoded_similarity = float(np.dot(query_vector, decoded_vec) / 
                                                     (np.linalg.norm(query_vector) * np.linalg.norm(decoded_vec) + 1e-12))
                            enhanced_results.append((node_id, decoded_similarity))
                            
                            # Log successful E8 decode for debugging
                            if hasattr(self, 'console'):
                                self.console.log(f"🔍 [E8 Decode] Remnant {node_id[:8]} similarity: {similarity:.3f} → {decoded_similarity:.3f}")
                        else:
                            enhanced_results.append((node_id, similarity))
                    else:
                        enhanced_results.append((node_id, similarity))
                else:
                    # Regular node, keep original similarity
                    enhanced_results.append((node_id, similarity))
                    
            except Exception as e:
                # Fallback to original similarity on any error
                enhanced_results.append((node_id, similarity))
                if hasattr(self, 'console'):
                    self.console.log(f"[yellow]E8 decode error for {node_id[:8]}: {e}[/yellow]")
                    
        # Re-sort by enhanced similarity scores
        enhanced_results.sort(key=lambda x: x[1], reverse=True)
        return enhanced_results[:k]

    def find_similar_in_main_storage(self, query_vector: np.ndarray, k: int = 5) -> List[tuple[str, float]]:
        # Dimension guard (prevent KDTree misuse and cryptic scipy errors)
        try:
            if query_vector is None:
                return []
            if len(query_vector) != EMBED_DIM:
                try:
                    self.mind.console.log(f"[MemoryManager] dimension_mismatch expected={EMBED_DIM} got={len(query_vector)}")
                except Exception:
                    pass
                return []
        except Exception:
            return []
        # Acquire shared snapshot under mutex to avoid seeing inconsistent ids vs matrix
        with self._index_mutex:
            kdtree_ref = self.main_kdtree
            ids_snapshot = list(self._main_storage_ids)
            matrix_snapshot = self._main_storage_matrix  # noqa: F841 (kept for potential future debug)
        if kdtree_ref is None or not ids_snapshot:
            return []
        initial_k = min(k * 5, len(ids_snapshot))
        if initial_k == 0: return []
        t0=time.perf_counter(); distances, indices = kdtree_ref.query(query_vector.reshape(1, -1), k=initial_k)
        try:
            dt_ms=(time.perf_counter()-t0)*1000.0
            self._kdtree_latency_ms_window.append(dt_ms)
            try:
                self.mind.metrics.timing('kdtree.query_ms', dt_ms)
            except Exception:
                pass
            if len(self._kdtree_latency_ms_window)>64:
                self._kdtree_latency_ms_window.pop(0)
            if len(self._kdtree_latency_ms_window)>=16 and (sum(self._kdtree_latency_ms_window)/len(self._kdtree_latency_ms_window))>self._kdtree_latency_threshold_ms:
                # Schedule KD-tree anneal via EventHorizonScheduler to avoid blocking query path
                try:
                    loop = asyncio.get_running_loop()
                    loop.create_task(self.mind.ehs.submit('ANNEAL_MAIN_INDEX'))
                except RuntimeError:
                    import threading
                    threading.Thread(target=lambda: asyncio.run(self.mind.ehs.submit('ANNEAL_MAIN_INDEX')), daemon=True).start()
                self._kdtree_latency_ms_window.clear()
        except Exception:
            pass
        distances, indices = np.atleast_1d(distances).flatten(), np.atleast_1d(indices).flatten() # Flatten results
        query_community = (self.graph_db.get_node(ids_snapshot[indices[0]]) or {}).get("community_id", -1)
        reranked_candidates = []
        for dist, idx in zip(distances, indices):
            nid = ids_snapshot[idx]
            node_data = self.graph_db.get_node(nid)
            if not node_data: continue

            score = float(dist)
            temp = node_data.get('temperature', 0.1)
            score *= (1.2 - (temp * 0.2))
            recency_penalty = max(0, self.mind.step_num - node_data.get('last_step', 0)) * 0.0001
            score += recency_penalty
            if query_community != -1 and node_data.get("community_id") == query_community:
                score *= 0.90

            if os.getenv("E8_USE_POTENTIAL_IN_KDTREE", "1") == "1":
                pot = node_data.get("connectivity_potential", self.INITIAL_POTENTIAL)
                # Lower score (better rank) for higher potential: multiplicative factor in [1-factor, 1]
                factor = float(os.getenv("E8_POTENTIAL_KDTREE_FACTOR", "0.25"))
                score *= (1.0 - factor * pot)

            reranked_candidates.append((nid, score))

        # VAE: Rerank scores using VAE latent representation (if available)
        if hasattr(self.mind, '_vae_rerank_scores'):
            reranked_candidates = self.mind._vae_rerank_scores(reranked_candidates, query_vector)

        reranked_candidates.sort(key=lambda item: item[1])
        return reranked_candidates[:k]

    # --- Black hole cluster padding enhancement helper ---
    def _pad_cluster_nodes(self, center_id: str, cluster_nodes: List[str], min_size: int = 6, per_anchor: int = 3) -> List[str]:
        """Expand cluster_nodes if too small using multi-anchor similarity union.
        Strategy: take up to 3 hottest nodes in current cluster (by temperature), pull top-N nearest for each,
        union them while avoiding already folded/remnant nodes. Uses existing KDTree for speed.
        """
        needed = max(0, min_size - len(cluster_nodes))
        if needed <= 0:
            return cluster_nodes
        # Choose anchors (center first, then by temperature descending)
        anchors = []
        g = self.graph_db.graph
        try:
            cluster_nodes_unique = list(dict.fromkeys(cluster_nodes))
            temps = [(nid, g.nodes[nid].get('temperature', 0.0)) for nid in cluster_nodes_unique if g.has_node(nid)]
            temps.sort(key=lambda x: x[1], reverse=True)
            ordered = []
            if center_id in cluster_nodes_unique:
                ordered.append(center_id)
            for nid, _t in temps:
                if nid != center_id:
                    ordered.append(nid)
            anchors = ordered[:3]
        except Exception:
            anchors = [center_id]
        added = set(cluster_nodes)
        for anchor in anchors:
            if len(added) >= min_size:
                break
            vec = self.main_vectors.get(anchor)
            if vec is None:
                continue
            # Query a modest number of neighbors
            neigh = self.find_similar_in_main_storage(vec, k=per_anchor + needed)
            for nid, _score in neigh:
                if len(added) >= min_size:
                    break
                if nid in added:
                    continue
                nd = self.graph_db.get_node(nid)
                if not nd:
                    continue
                if nd.get('type') in ('blackhole_remnant',):
                    continue
                added.add(nid)
        if len(added) > len(cluster_nodes):
            try:
                self.mind.metrics.increment('bh.padding_expanded', 1)
            except Exception:
                pass
        return list(added)
    
    async def apply_decay(self):
        async with self.lock:
            decay_vivid, decay_hot, decay_warm, decay_cold = 0.5**(1.0/TEMP_HALF_LIFE_VIVID), 0.5**(1.0/TEMP_HALF_LIFE_HOT), 0.5**(1.0/TEMP_HALF_LIFE_WARM), 0.5**(1.0/TEMP_HALF_LIFE_COLD)
            for node_id, data in self.graph_db.graph.nodes(data=True):
                temp = data.get('temperature', 1.0)
                if data.get("vivid_until_step", -1) > self.mind.step_num: temp *= decay_vivid
                elif temp > 1.5: temp *= decay_hot
                elif temp > 0.5: temp *= decay_warm
                else: temp *= decay_cold
                data['temperature'] = max(0.01, temp)
                data['age'] = data.get('age', 0) + 1
        
        interval = getattr(self.mind, "_community_every_steps", None)
        if interval is None:
            try:
                interval = int(os.getenv("E8_COMMUNITY_EVERY_STEPS", "200"))
                self.mind._community_every_steps = interval
                if interval != 200:
                    self.mind.console.log(f"[COMMUNITY] Override cadence set to every {interval} steps (env E8_COMMUNITY_EVERY_STEPS)")
            except Exception:
                interval = 200
                self.mind._community_every_steps = interval
        if interval > 0 and self.mind.step_num > 0 and self.mind.step_num % interval == 0:
            asyncio.create_task(self._run_maintenance_cycle())

    async def _run_maintenance_cycle(self):
        self.console.log("ðŸ§¹ [MemoryManager] Running maintenance cycle...")
        async with self.lock:
            self._commit_pending_additions_locked()
            self.graph_db.compute_and_store_communities()
            self._evict_if_needed_locked()
            self.hopfield.update_prototypes()
        self.console.log("ðŸ§¹ [MemoryManager] Maintenance cycle complete.")
    
    def _calculate_importance_score(self, node_id: str, data: Dict) -> float:
        rating = data.get('rating', 0.5)
        temp = data.get('temperature', 0.1)
        age = data.get('age', 1)
        try:
            if hasattr(self.graph_db.graph, 'degree') and callable(getattr(self.graph_db.graph, 'degree', None)):
                degree = self.graph_db.graph.degree(node_id)
            elif hasattr(self.graph_db.graph, 'degree'):
                degree = self.graph_db.graph.degree[node_id]
            else:
                degree = 0
        except (KeyError, TypeError, AttributeError):
            degree = 0
        return (rating + temp * 0.5) / (np.log1p(age) * (degree + 1))

    def _evict_if_needed_locked(self, max_nodes: int = 50000, eviction_ratio: float = 0.05):
        num_nodes = self.graph_db.graph.number_of_nodes()
        if num_nodes <= max_nodes: return

        self.console.log(f"ðŸ§  [MemoryManager] Memory limit ({max_nodes}) exceeded. Evicting nodes...")
        num_to_evict = int(num_nodes * eviction_ratio)
        candidates = []
        for nid, data in self.graph_db.graph.nodes(data=True):
            if data.get('type') in ['blackhole_remnant', 'self_code']: continue
            candidates.append((self._calculate_importance_score(nid, data), nid))
        
        candidates.sort()
        for _, node_id_to_evict in candidates[:num_to_evict]:
            try:
                self.graph_db.graph.remove_node(node_id_to_evict)
                self.main_vectors.pop(node_id_to_evict, None)
            except Exception: pass
        
        self._commit_pending_additions_locked()
        self.console.log(f"ðŸ§  [MemoryManager] Evicted {num_to_evict} nodes.")

    def spike_temperature(self, node_id: str, amount: float = 1.0):
        node = self.graph_db.get_node(node_id)
        if node:
            node['temperature'] = node.get('temperature', 1.0) + amount

    def check_thermal_boundedness(self, min_temp: float = 0.01, max_temp: float = 5.0, clamp: bool = True) -> dict:
        """Scan node temperatures, emit telemetry and optionally clamp out-of-bounds values.

        Returns a summary dict with counts for low/high/out_of_bounds.
        """
        low_count = 0
        high_count = 0
        total = 0
        try:
            for nid, data in self.graph_db.graph.nodes(data=True):
                total += 1
                t = float(data.get('temperature', 1.0))
                if t < min_temp:
                    low_count += 1
                    if clamp:
                        data['temperature'] = float(min_temp)
                elif t > max_temp:
                    high_count += 1
                    if clamp:
                        data['temperature'] = float(max_temp)

            out = {'total': total, 'low': low_count, 'high': high_count}
            try:
                if hasattr(self.mind, 'metrics'):
                    self.mind.metrics.gauge('telemetry.thermal.low_count', float(low_count))
                    self.mind.metrics.gauge('telemetry.thermal.high_count', float(high_count))
                    self.mind.metrics.gauge('telemetry.thermal.total', float(total))
                    # boundedness ratio
                    bounded = max(0.0, 1.0 - (low_count + high_count) / max(1.0, float(total)))
                    self.mind.metrics.gauge('telemetry.thermal.boundedness', bounded)
            except Exception:
                pass

            # If many out-of-bounds, emit an event
            if (low_count + high_count) > max(1, total * 0.01):
                try:
                    metrics_log('thermal_out_of_bounds', out)
                except Exception:
                    pass

            return out
        except Exception:
            return {'total': 0, 'low': 0, 'high': 0}

    def find_event_horizon_e8(self, curvature_threshold=0.15, rotor_stress_threshold=0.8, age_threshold=1):
        """E8-integrated event horizon detection using lattice geometry and rotor orientations."""
        graph, candidates = self.graph_db.graph, []
        total_checked = 0
        aged_count = 0
        curvature_pass = 0
        rotor_pass = 0
        
        # E8 lattice root vectors for geometric calculations
        phi = (1 + np.sqrt(5)) / 2  # Golden ratio - fundamental to E8
        
        for nid, d in graph.nodes(data=True):
            total_checked += 1
            if d.get('age', 0) < age_threshold: continue
            aged_count += 1
            
            # Calculate E8 lattice curvature instead of simple density
            e8_curvature = self._calculate_e8_curvature(nid)
            
            # Calculate rotor stress (misalignment pressure)
            rotor_stress = self._calculate_rotor_stress(nid)
            
            # E8 geometric pressure using golden ratio scaling
            geometric_pressure = float(e8_curvature) * float(rotor_stress) * phi
            
            curvature_ok = e8_curvature > curvature_threshold
            rotor_ok = rotor_stress > rotor_stress_threshold
            
            if curvature_ok: curvature_pass += 1
            if rotor_ok: rotor_pass += 1
            
            if curvature_ok and rotor_ok:
                candidates.append((geometric_pressure, nid))
                
        # Debug logging for E8 candidate search
        try:
            if hasattr(self, 'debug_console') and self.debug_console:
                self.debug_console.log(f"[E8-EH Debug] checked={total_checked} aged={aged_count} curvature_pass={curvature_pass} rotor_pass={rotor_pass} candidates={len(candidates)}")
        except:
            pass
            
        if not candidates: return None, None
        best_pressure, best_id = max(candidates)
        return best_id, best_pressure

    def _calculate_e8_curvature(self, center_id: str) -> float:
        """Calculate lattice curvature using E8 root vector geometry."""
        try:
            # Get node's dimensional shell position
            node_vec = self.main_vectors.get(center_id)
            if node_vec is None:
                return 0.0
                
            # Find neighbors in E8 lattice space
            neighbors = list(self.graph_db.graph.neighbors(center_id))
            if len(neighbors) < 3:
                return 0.0
                
            # Calculate local E8 lattice curvature
            neighbor_vecs = [self.main_vectors.get(nid) for nid in neighbors[:8]]
            neighbor_vecs = [v for v in neighbor_vecs if v is not None]
            
            if len(neighbor_vecs) < 3:
                return 0.0
                
            # Use E8's 240 root vectors' natural geometry
            curvature = 0.0
            for i, vec1 in enumerate(neighbor_vecs):
                for j, vec2 in enumerate(neighbor_vecs[i+1:], i+1):
                    if len(vec1) == len(vec2) == len(node_vec):
                        # Calculate angular deviation from E8 lattice ideal
                        angle1 = np.arccos(np.clip(np.dot(node_vec, vec1) / (np.linalg.norm(node_vec) * np.linalg.norm(vec1) + 1e-12), -1, 1))
                        angle2 = np.arccos(np.clip(np.dot(node_vec, vec2) / (np.linalg.norm(node_vec) * np.linalg.norm(vec2) + 1e-12), -1, 1))
                        # E8 ideal angles: 60°, 90°, 120° (root vector relationships)
                        ideal_angles = [np.pi/3, np.pi/2, 2*np.pi/3]
                        deviation = min(abs(angle1 - ideal) for ideal in ideal_angles)
                        curvature += deviation
                        
            return float(curvature / max(1, len(neighbor_vecs)))
            
        except Exception:
            return 0.0

    def _calculate_rotor_stress(self, center_id: str) -> float:
        """Calculate Clifford rotor misalignment stress."""
        try:
            if not CLIFFORD_AVAILABLE:
                # Fallback to temperature-based stress
                node = self.graph_db.graph.nodes.get(center_id, {})
                return float(node.get('temperature', 0.0))
                
            # Get node's rotor orientation if available
            node = self.graph_db.graph.nodes.get(center_id, {})
            node_rotor = node.get('rotor_orientation')
            
            if node_rotor is None:
                return float(node.get('temperature', 0.0))
                
            # Calculate stress as misalignment with neighbors
            neighbors = list(self.graph_db.graph.neighbors(center_id))
            if not neighbors:
                return 0.0
                
            total_stress = 0.0
            valid_neighbors = 0
            
            for neighbor_id in neighbors[:8]:  # Sample up to 8 neighbors
                neighbor_node = self.graph_db.graph.nodes.get(neighbor_id, {})
                neighbor_rotor = neighbor_node.get('rotor_orientation')
                
                if neighbor_rotor is not None:
                    # Calculate rotor misalignment (simplified)
                    # In full implementation, would use proper Clifford algebra
                    stress = 1.0 - abs(float(getattr(node_rotor, 'scalar', 0.5)))
                    total_stress += stress
                    valid_neighbors += 1
                    
            return float(total_stress / max(1, valid_neighbors))
            
        except Exception:
            return 0.0

    def find_event_horizon(self, density_threshold=0.10, temp_threshold=0.8, age_threshold=1):
        """Legacy event horizon detection - kept for compatibility."""
        # Try E8-integrated version first
        if E8_LATTICE_QUANTIZATION and SPACETIME_CURVATURE_ENABLED:
            return self.find_event_horizon_e8(
                curvature_threshold=density_threshold * 0.75,  # Scale for E8 geometry
                rotor_stress_threshold=temp_threshold,
                age_threshold=age_threshold
            )
            
        # Fallback to original simple version
        graph, candidates = self.graph_db.graph, []
        total_checked = 0
        aged_count = 0
        temp_pass = 0
        dens_pass = 0
        
        for nid, d in graph.nodes(data=True):
            total_checked += 1
            if d.get('age', 0) < age_threshold: continue
            aged_count += 1
            
            temp = d.get('temperature', 0.0)
            density = self._local_density(nid)
            pressure = float(temp) * float(density)
            
            temp_ok = temp > temp_threshold
            dens_ok = density >= density_threshold
            
            if temp_ok: temp_pass += 1
            if dens_ok: dens_pass += 1
            
            if temp_ok and dens_ok:
                candidates.append((pressure, nid))
                
        # Debug logging for candidate search
        try:
            if hasattr(self, 'debug_console') and self.debug_console:
                self.debug_console.log(f"[EH Debug] checked={total_checked} aged={aged_count} temp_pass={temp_pass} dens_pass={dens_pass} candidates={len(candidates)}")
        except:
            pass
            
        if not candidates: return None, None
        best_pressure, best_id = max(candidates)
        return best_id, best_pressure

    def _local_density(self, center_id: str, radius: int = 4) -> float:
        try:
            nodes_in_radius = set(nx.ego_graph(self.graph_db.graph, center_id, radius=radius).nodes())
            subgraph = self.graph_db.graph.subgraph(nodes_in_radius)
            num_nodes, num_edges = subgraph.number_of_nodes(), subgraph.number_of_edges()
            possible_edges = num_nodes * (num_nodes - 1) / 2
            return num_edges / possible_edges if possible_edges > 0 else 0.0
        except (nx.NetworkXError, KeyError):
            return 0.0

    def collect_cluster_e8(self, center_id: str, radius: int = 4) -> List[str]:
        """E8-aware clustering using Weyl chambers and root vector geometry."""
        try:
            # Get center node vector
            center_vec = self.main_vectors.get(center_id)
            if center_vec is None:
                return [center_id]
                
            # Find nodes in radius using graph topology
            nodes_in_radius = list(nx.ego_graph(self.graph_db.graph, center_id, radius=radius).nodes())
            
            # Filter nodes with valid vectors
            valid_nodes = []
            valid_vectors = []
            
            for nid in nodes_in_radius:
                vec = self.main_vectors.get(nid)
                if vec is not None and len(vec) == len(center_vec):
                    valid_nodes.append(nid)
                    valid_vectors.append(vec)
                    
            if len(valid_nodes) < 3:
                return valid_nodes
                
            # Use E8 Weyl chamber clustering
            phi = (1 + np.sqrt(5)) / 2  # Golden ratio
            clusters = []
            
            # Group by E8 lattice distance and rotor alignment
            for i, (nid, vec) in enumerate(zip(valid_nodes, valid_vectors)):
                # Calculate E8 lattice distance (using golden ratio scaling)
                lattice_dist = np.linalg.norm(vec - center_vec) * phi
                
                # Calculate rotor alignment factor
                rotor_factor = self._get_rotor_alignment(center_id, nid)
                
                # Combined E8 geometric score
                e8_score = lattice_dist * (2.0 - rotor_factor)  # Lower is better
                
                # Find best cluster or create new one
                best_cluster = None
                best_score = float('inf')
                
                for cluster in clusters:
                    cluster_center = np.mean([valid_vectors[j] for j in cluster], axis=0)
                    cluster_dist = np.linalg.norm(vec - cluster_center)
                    if cluster_dist < best_score:
                        best_score = cluster_dist
                        best_cluster = cluster
                        
                # E8 clustering threshold based on root vector spacing
                e8_threshold = 0.618 * phi  # Golden ratio threshold
                
                if best_cluster is not None and best_score < e8_threshold:
                    best_cluster.append(i)
                else:
                    clusters.append([i])
                    
            # Return largest cluster (most coherent in E8 space)
            if clusters:
                largest_cluster = max(clusters, key=len)
                return [valid_nodes[i] for i in largest_cluster]
            else:
                return valid_nodes[:8]  # Fallback
                
        except Exception:
            # Fallback to legacy clustering
            return self.collect_cluster_legacy(center_id, radius)

    def _get_rotor_alignment(self, node_id1: str, node_id2: str) -> float:
        """Calculate rotor alignment factor between two nodes."""
        try:
            if not CLIFFORD_AVAILABLE:
                return 1.0  # Neutral alignment
                
            node1 = self.graph_db.graph.nodes.get(node_id1, {})
            node2 = self.graph_db.graph.nodes.get(node_id2, {})
            
            rotor1 = node1.get('rotor_orientation')
            rotor2 = node2.get('rotor_orientation')
            
            if rotor1 is None or rotor2 is None:
                return 1.0
                
            # Simplified rotor alignment calculation
            # In full implementation, would use proper Clifford algebra
            scalar1 = float(getattr(rotor1, 'scalar', 0.5))
            scalar2 = float(getattr(rotor2, 'scalar', 0.5))
            
            return abs(scalar1 * scalar2) + 0.5  # 0.5 to 1.0 range
            
        except Exception:
            return 1.0

    def collect_cluster(self, center_id: str, radius: int = 4) -> List[str]:
        """E8-aware clustering with spacetime curvature integration"""
        # Use E8 clustering if available
        if E8_LATTICE_QUANTIZATION and SPACETIME_CURVATURE_ENABLED:
            return self.collect_cluster_e8(center_id, radius)
        else:
            return self.collect_cluster_legacy(center_id, radius)
            
    def collect_cluster_legacy(self, center_id: str, radius: int = 4) -> List[str]:
        """Legacy clustering method - kept for compatibility."""
        if DBSCAN is None: 
            return list(nx.ego_graph(self.graph_db.graph, center_id, radius=2).nodes())
        
        try:
            nodes_in_radius = list(nx.ego_graph(self.graph_db.graph, center_id, radius=radius).nodes())
            vectors = [self.main_vectors.get(nid) for nid in nodes_in_radius]
            valid_indices = [i for i, v in enumerate(vectors) if v is not None and np.any(v)]
            if len(valid_indices) < 3: 
                return [nodes_in_radius[i] for i in valid_indices]
            
            valid_vectors = [vectors[i] for i in valid_indices]
            node_ids = [nodes_in_radius[i] for i in valid_indices]
            
            # Compute E8-aware distance matrix
            n = len(valid_vectors)
            distance_matrix = np.zeros((n, n))
            
            for i in range(n):
                for j in range(i+1, n):
                    dist = self._compute_e8_aware_distance(valid_vectors[i], valid_vectors[j])
                    distance_matrix[i, j] = distance_matrix[j, i] = dist
            
            # Adaptive DBSCAN parameters based on E8 lattice density
            eps, min_samples = self._compute_adaptive_clustering_params(valid_vectors)
            
            # Use precomputed distance matrix for clustering
            clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed').fit(distance_matrix)
            # If the center node has no valid vector it may not be in node_ids
            if center_id not in node_ids:
                # Fallback to local ego graph when center vector is missing
                return list(nx.ego_graph(self.graph_db.graph, center_id, radius=2).nodes())
            center_idx = node_ids.index(center_id)
            center_label = clustering.labels_[center_idx]

            if center_label == -1 or np.sum(clustering.labels_ == center_label) < 3:
                return list(nx.ego_graph(self.graph_db.graph, center_id, radius=2).nodes())
            
            cluster_nodes = [node_ids[i] for i, label in enumerate(clustering.labels_) if label == center_label]
            
            # Ensure cross-shell consistency
            cluster_nodes = self._ensure_cross_shell_consistency(cluster_nodes)
            
            return cluster_nodes
            
        except Exception as e:
            self.console.log(f"[yellow]E8 clustering failed: {e}, using fallback[/yellow]")
            return list(nx.ego_graph(self.graph_db.graph, center_id, radius=2).nodes())

    def _compute_e8_aware_distance(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Compute distance respecting E8 lattice structure and spacetime curvature"""
        try:
            # Project to E8 subspace
            e8_proj1 = vec1[:8] if vec1.size >= 8 else np.pad(vec1, (0, 8-len(vec1)))
            e8_proj2 = vec2[:8] if vec2.size >= 8 else np.pad(vec2, (0, 8-len(vec2)))
            
            # Apply spacetime metric tensor if curvature enabled
            if (SPACETIME_CURVATURE_ENABLED and 
                hasattr(self.mind, 'substrate') and 
                hasattr(self.mind.substrate, 'metric_tensor')):
                metric = self.mind.substrate.metric_tensor
                # Compute Riemannian distance: sqrt((x1-x2)^T * g * (x1-x2))
                diff = e8_proj1 - e8_proj2
                curved_dist = np.sqrt(np.abs(diff @ metric @ diff))
            else:
                curved_dist = np.linalg.norm(e8_proj1 - e8_proj2)
            
            # Add E8 lattice structure penalty
            root_idx1 = self.mind.physics.find_nearest_root_index(e8_proj1)
            root_idx2 = self.mind.physics.find_nearest_root_index(e8_proj2)
            
            if root_idx1 is not None and root_idx2 is not None and root_idx1 != root_idx2:
                # Use E8 lattice adjacency for structure-aware distance
                if hasattr(self.mind.physics, 'adj_bool'):
                    lattice_penalty = 0.0 if self.mind.physics.adj_bool[root_idx1, root_idx2] else 0.3
                    curved_dist += lattice_penalty
            
            # Normalize to cosine-like range for DBSCAN compatibility
            return min(curved_dist / 3.0, 2.0)  # Scale to reasonable range
            
        except Exception as e:
            # Fallback to cosine distance
            return 1.0 - np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-12)

    def _compute_adaptive_clustering_params(self, vectors: List[np.ndarray]) -> Tuple[float, int]:
        """Compute DBSCAN parameters based on E8 lattice density"""
        if not vectors or len(vectors) < 2:
            return 0.85, 2
        
        try:
            # Compute local E8 lattice density
            e8_projections = [v[:8] if v.size >= 8 else np.pad(v, (0, 8-len(v))) for v in vectors]
            
            # Find average distance to nearest E8 roots
            avg_root_distance = 0.0
            valid_count = 0
            
            for proj in e8_projections:
                root_idx = self.mind.physics.find_nearest_root_index(proj)
                if root_idx is not None:
                    root_vec = self.mind.physics.roots_unit[root_idx]
                    dist = np.linalg.norm(proj - root_vec)
                    avg_root_distance += dist
                    valid_count += 1
            
            if valid_count > 0:
                avg_root_distance /= valid_count
                # Adaptive eps based on local E8 density
                eps = max(0.3, min(1.2, avg_root_distance * 2.0))
                min_samples = max(2, min(int(np.sqrt(len(vectors))), len(vectors)//3))
            else:
                eps, min_samples = 0.85, 2
            
            return eps, min_samples
            
        except Exception:
            return 0.85, 2

    def _ensure_cross_shell_consistency(self, cluster_nodes: List[str]) -> List[str]:
        """Ensure clustering is consistent across dimensional shells"""
        try:
            if not hasattr(self.mind, 'dimensional_shells') or len(cluster_nodes) < 2:
                return cluster_nodes
            
            # Check which shells have meaningful representation of these nodes
            shell_presence = {}
            for shell_dim in DIMENSIONAL_SHELL_SIZES:
                shell = self.mind.dimensional_shells.get(shell_dim)
                if shell and hasattr(shell, 'vectors'):
                    present_nodes = [nid for nid in cluster_nodes if nid in shell.vectors]
                    if len(present_nodes) >= 2:  # Shell has meaningful representation
                        shell_presence[shell_dim] = present_nodes
            
            # If nodes are only in one shell or no shells, return as-is
            if len(shell_presence) <= 1:
                return cluster_nodes
            
            # Find consensus nodes that cluster consistently across shells
            consensus_nodes = set(cluster_nodes)
            
            for shell_dim, nodes in shell_presence.items():
                if len(nodes) >= 3:  # Enough for clustering analysis
                    shell = self.mind.dimensional_shells[shell_dim]
                    shell_vectors = []
                    valid_nodes = []
                    
                    for nid in nodes:
                        if nid in shell.vectors:
                            vec = shell.vectors[nid]
                            if hasattr(vec, 'value'):  # Clifford multivector
                                shell_vectors.append(np.array([vec.value]))
                            elif isinstance(vec, np.ndarray):
                                shell_vectors.append(vec)
                            else:
                                shell_vectors.append(np.array([float(vec)]))
                            valid_nodes.append(nid)
                    
                    if len(shell_vectors) >= 3:
                        # Quick consistency check: compute pairwise distances
                        distances = []
                        for i in range(len(shell_vectors)):
                            for j in range(i+1, len(shell_vectors)):
                                dist = np.linalg.norm(shell_vectors[i] - shell_vectors[j])
                                distances.append(dist)
                        
                        # If distances are too varied, this shell shows inconsistency
                        if len(distances) > 0:
                            dist_std = np.std(distances)
                            dist_mean = np.mean(distances)
                            if dist_std > dist_mean * 0.8:  # High variance indicates poor clustering
                                # Remove nodes that contribute to inconsistency
                                # For simplicity, keep the most central nodes
                                center_idx = len(valid_nodes) // 2
                                keep_nodes = valid_nodes[max(0, center_idx-1):center_idx+2]
                                consensus_nodes &= set(keep_nodes)
            
            return list(consensus_nodes) if consensus_nodes else cluster_nodes
            
        except Exception as e:
            self.console.log(f"[yellow]Cross-shell consistency check failed: {e}[/yellow]")
            return cluster_nodes

    async def synthesize_remnant(self, cluster_nodes: List[str], label_hint: str, is_macro: bool = False) -> Tuple[Optional[Dict], Optional[np.ndarray], float]:
        if not cluster_nodes: return None, None, 0.0
        
        cluster_data = [d for nid in cluster_nodes if (d := self.graph_db.get_node(nid))]
        cluster_vectors = [v for nid in cluster_nodes if (v := self.main_vectors.get(nid)) is not None]
        
        if not cluster_data or not cluster_vectors: return None, None, 0.0

        # Compute information-theoretic mass based on quantum information content
        mass = self._compute_holographic_mass(cluster_nodes, cluster_vectors)
        
        # Holographic compression using E8 boundary fabric
        remnant_vec, holographic_data = await self._holographic_compression(cluster_nodes, cluster_vectors, cluster_data)
        
        # Apply Hopfield cleanup to compressed representation
        remnant_vec = self.hopfield.clean_up(remnant_vec)
        
        avg_rating = np.average([d.get('rating', 0.5) for d in cluster_data], 
                              weights=[d.get('temperature', 1.0) for d in cluster_data])

        fragments = [d.get('metaphor', d.get('label', '')) for d in cluster_data]
        prompt = (f"Synthesize the following fragmented ideas into a single, dense, core concept. "
                  f"Provide a short, evocative label and a one-sentence metaphor for the new idea.\n\n"
                  f"Ideas: {'; '.join(fragments[:10])}\n\nRespond in JSON format with keys 'label' and 'metaphor'.")
        
        try:
            response = await self.llm_caller.enqueue_and_wait(prompt, max_tokens=120)
            if response is None:
                raise ValueError("LLM returned None for blackhole synthesis")
            parsed = _parse_json_object(response)
            new_label, new_metaphor = parsed.get('label', label_hint), parsed.get('metaphor', 'A consolidated memory.')
        except Exception:
            new_label, new_metaphor = label_hint, "A synthesized concept."
            
        return {
            "type": "blackhole_remnant", "label": new_label, "metaphor": new_metaphor,
            "embedding": remnant_vec, "rating": avg_rating, "step": self.mind.step_num, "is_macro": is_macro,
            "holographic_data": holographic_data
        }, remnant_vec, mass

    def fold_and_prune(self, cluster_nodes: List[str]):
        """Enhanced fold_and_prune with E8 cyclic memory stitching."""
        # Mark nodes as folded and cool them down
        for node_id in cluster_nodes:
            node = self.graph_db.get_node(node_id)
            if node:
                node['folded'] = True
                node['temperature'] *= 0.1
                
                # CRITICAL: Store E8 lattice position before removal for cyclic stitching
                if 'lattice_root_index' not in node:
                    node_vec = self.main_vectors.get(node_id)
                    if node_vec is not None and len(node_vec) >= 8:
                        root_idx = self.mind.physics.find_nearest_root_index(node_vec[:8])
                        if root_idx is not None:
                            node['lattice_root_index'] = root_idx
                            node['pre_fold_e8_signature'] = node_vec[:8].tolist()
                
                # Remove from dimensional shells (this is the "collapse")
                for shell in self.mind.dimensional_shells.values():
                    shell.vectors.pop(node_id, None)

    def e8_decode_remnant(self, remnant_vec: np.ndarray, holographic_data: dict) -> np.ndarray:
        """Decode holographic remnant back from E8 lattice compression to full memory space."""
        try:
            # Check if this is an E8-encoded remnant
            if not holographic_data.get('lattice_quantized', False):
                return remnant_vec  # Not E8-encoded, return as-is
                
            root_idx = holographic_data.get('lattice_root_index')
            if root_idx is None:
                return remnant_vec
                
            # Get E8 root vector for decoding
            e8_roots = getattr(self.mind.physics, 'roots_unit', None)
            if e8_roots is None or root_idx >= len(e8_roots):
                return remnant_vec
                
            lattice_root = e8_roots[root_idx]
            phi = (1 + np.sqrt(5)) / 2  # Golden ratio
            
            # E8 DECODE: Expand from lattice point back to full dimensional space
            decoded_vec = remnant_vec.copy()
            
            # Project E8 root through all dimensional shells (8D → 16D → 32D → 64D)
            shell_projections = {}
            
            for shell_dim in [8, 16, 32, 64]:
                if shell_dim <= len(decoded_vec):
                    # Expand E8 root to higher dimensions using golden ratio scaling
                    if shell_dim == 8:
                        # Direct E8 lattice
                        shell_proj = lattice_root
                    else:
                        # Expand using E8 symmetries and golden ratio
                        expansion_factor = (phi ** (shell_dim // 8 - 1))
                        shell_proj = np.zeros(shell_dim)
                        
                        # Replicate E8 pattern with golden ratio modulation
                        for i in range(0, shell_dim, 8):
                            end_idx = min(i + 8, shell_dim)
                            pattern_len = end_idx - i
                            
                            if pattern_len == 8:
                                shell_proj[i:end_idx] = lattice_root * expansion_factor
                            else:
                                shell_proj[i:end_idx] = lattice_root[:pattern_len] * expansion_factor
                                
                    shell_projections[shell_dim] = shell_proj
                    
                    # Update decoded vector with E8 projection
                    if shell_dim <= len(decoded_vec):
                        decoded_vec[:shell_dim] = shell_proj
                        
            # Store shell projections for cyclic stitching
            holographic_data['e8_shell_projections'] = {
                str(k): v.tolist() for k, v in shell_projections.items()
            }
            
            return decoded_vec
            
        except Exception as e:
            self.console.log(f"[yellow]E8 decode failed: {e}, using original vector[/yellow]")
            return remnant_vec

    def cyclic_memory_stitch(self, remnant_id: str, remnant_vec: np.ndarray, holographic_data: dict):
        """Project decoded remnant back into reality/space/memory field across all dimensional shells."""
        try:
            # Decode from E8 lattice compression first
            decoded_vec = self.e8_decode_remnant(remnant_vec, holographic_data)
            
            # Get shell projections from decode process
            shell_projections = holographic_data.get('e8_shell_projections', {})
            
            # CYCLIC STITCHING: Project into ALL dimensional shells with E8 geometry
            phi = (1 + np.sqrt(5)) / 2
            
            for shell_dim, shell in self.mind.dimensional_shells.items():
                if str(shell_dim) in shell_projections:
                    # Use computed E8 projection
                    shell_vec = np.array(shell_projections[str(shell_dim)])
                else:
                    # Fallback: Direct projection with golden ratio scaling
                    if shell_dim <= len(decoded_vec):
                        shell_vec = decoded_vec[:shell_dim] * (phi ** (shell_dim / 64))
                    else:
                        # Expand using E8 symmetries
                        shell_vec = np.zeros(shell_dim)
                        pattern_repeats = shell_dim // len(decoded_vec)
                        remainder = shell_dim % len(decoded_vec)
                        
                        for i in range(pattern_repeats):
                            start_idx = i * len(decoded_vec)
                            end_idx = start_idx + len(decoded_vec)
                            shell_vec[start_idx:end_idx] = decoded_vec * (phi ** i)
                            
                        if remainder > 0:
                            start_idx = pattern_repeats * len(decoded_vec)
                            shell_vec[start_idx:start_idx + remainder] = decoded_vec[:remainder] * (phi ** pattern_repeats)
                
                # STITCH INTO MEMORY FIELD: Add to dimensional shell
                shell.vectors[remnant_id] = normalize_vector(shell_vec)
                
            # Update main vectors with fully decoded version
            self.main_vectors[remnant_id] = normalize_vector(decoded_vec)
            
            # Log successful cyclic stitching
            self.console.log(f"🌀 [E8 Stitch] Remnant {remnant_id[:8]} stitched across {len(self.mind.dimensional_shells)} shells")
            
        except Exception as e:
            self.console.log(f"[red]Cyclic memory stitch failed: {e}[/red]")

    def _compute_holographic_mass(self, cluster_nodes: List[str], cluster_vectors: List[np.ndarray]) -> float:
        """Compute information-theoretic mass based on quantum information content."""
        total_entropy = 0.0
        
        for i, vector in enumerate(cluster_vectors):
            if vector is None or vector.size == 0:
                continue
                
            # Compute density matrix from vector (treat as quantum state)
            normalized_vec = vector / (np.linalg.norm(vector) + 1e-12)
            density_matrix = np.outer(normalized_vec, normalized_vec)
            
            # Compute von Neumann entropy: S = -Tr(Ï log Ï)
            eigenvalues = np.linalg.eigvals(density_matrix)
            # Eigenvalues may be complex due to numerical noise; use real part for entropy
            eigenvalues = np.real(eigenvalues)
            eigenvalues = eigenvalues[eigenvalues > 1e-12]  # Filter numerical zeros
            
            if len(eigenvalues) > 0:
                entropy = -np.sum(eigenvalues * np.log(eigenvalues + 1e-12))
                total_entropy += entropy
        
        # Add structural entropy from graph connectivity
        for node_id in cluster_nodes:
            try:
                degree = self.graph_db.graph.degree(node_id) if hasattr(self.graph_db.graph, 'degree') else 0
                if degree > 0:
                    total_entropy += np.log(degree + 1)  # Structural information
            except Exception:
                pass
        
        return float(total_entropy)

    async def _holographic_compression(self, cluster_nodes: List[str], cluster_vectors: List[np.ndarray], cluster_data: List[Dict]) -> Tuple[np.ndarray, Dict]:
        """Compress bulk information onto holographic boundary using E8 structure with causal modulation."""
        try:
            # Compute Q(t) emergence for causal modulation instead of multiplying final scores
            q_t_current = compute_emergence_q_t(self.mind.step_num, mass=1.0, dim=32)
            s_q_global = get_s_q_global(self.mind)
            
            # Wire E8_Q_SCALE into s_q as requested
            s_q_global *= E8_Q_SCALE
            
            # Causal modulation parameters (instead of multiplying final scores)
            target_comp = 0.5 + 0.3 * q_t_current  # Target compression increases with Q(t)
            beta_used = 1.0 + 0.5 * s_q_global     # Beta modulation increases with sQ
            
            # Store causal parameters for BH panel display
            self.mind._last_target_compression = float(target_comp)
            self.mind._last_vae_beta = float(beta_used)
            
            # Store causal parameters for metrics logging
            causal_params = {'target_comp': float(target_comp), 'beta_used': float(beta_used)}
            
            # Get holographic encoder if available
            holo_encoder = getattr(self.mind, 'holo', None)
            
            if holo_encoder and hasattr(holo_encoder, 'encode_bulk'):
                # Use proper holographic encoding with causal modulation
                bulk_features = np.array(cluster_vectors)
                
                # Apply target compression modulation to feature selection
                n_features_target = max(1, int(len(cluster_vectors) * target_comp))
                if len(cluster_vectors) > n_features_target:
                    # Select top features based on temperature weighting
                    temperatures = [d.get('temperature', 1.0) for d in cluster_data[:len(cluster_vectors)]]
                    top_indices = np.argsort(temperatures)[-n_features_target:]
                    bulk_features = bulk_features[top_indices]
                
                # Find E8 lattice positions for boundary encoding
                boundary_positions = []
                for vec in bulk_features[:8]:  # Limit to manageable boundary
                    if vec.size >= 8:
                        root_idx = self.mind.physics.find_nearest_root_index(vec[:8])
                        if root_idx is not None:
                            boundary_positions.append(root_idx)
                
                if len(boundary_positions) > 0:
                    # Encode bulk information on E8 boundary with beta modulation
                    boundary_encoding = holo_encoder.encode_bulk(
                        feat=bulk_features.mean(axis=0) * beta_used,  # Apply beta modulation
                        shadow_ids=np.array(boundary_positions),
                        slice_id=self.mind.step_num % 1000
                    )
                    
                    holographic_data = {
                        'boundary_positions': boundary_positions,
                        'compression_ratio': len(cluster_vectors) / max(1, len(boundary_positions)),
                        'encoded_dimension': boundary_encoding.shape[0] if hasattr(boundary_encoding, 'shape') else len(boundary_encoding),
                        'causal_modulation': causal_params
                    }
                    
                    return boundary_encoding, holographic_data
            
            # Fallback: E8 lattice-based compression with causal modulation
            return self._e8_lattice_compression_with_causal(cluster_vectors, cluster_data, causal_params)
            
        except Exception as e:
            self.console.log(f"[yellow]Holographic compression failed, using fallback: {e}[/yellow]")
            return self._e8_lattice_compression(cluster_vectors, cluster_data)

    def _e8_lattice_compression(self, cluster_vectors: List[np.ndarray], cluster_data: List[Dict]) -> Tuple[np.ndarray, Dict]:
        """Enhanced E8 lattice compression with spacetime curvature integration."""
        return self._e8_lattice_compression_with_causal(cluster_vectors, cluster_data, {})

    def _e8_lattice_compression_with_causal(self, cluster_vectors: List[np.ndarray], cluster_data: List[Dict], causal_params: Dict) -> Tuple[np.ndarray, Dict]:
        """Enhanced E8 lattice compression with spacetime curvature integration and causal modulation."""
        # Extract causal modulation parameters
        target_comp = causal_params.get('target_comp', 0.8)  # Default target compression
        beta_used = causal_params.get('beta_used', 1.0)      # Default beta modulation
        
        # Ensure vectors and data have same length for weight calculation
        n_vectors = len(cluster_vectors)
        n_data = len(cluster_data)
        
        if n_data < n_vectors:
            # Pad cluster_data with default values
            default_data = {'temperature': 1.0, 'rating': 0.5}
            cluster_data = cluster_data + [default_data] * (n_vectors - n_data)
        elif n_data > n_vectors:
            # Truncate cluster_data to match vectors
            cluster_data = cluster_data[:n_vectors]
        
        # Temperature-weighted averaging with causal modulation (instead of multiplying final scores)
        temperatures = np.array([d.get('temperature', 1.0) for d in cluster_data])
        
        # Apply beta modulation to temperature weighting
        modulated_temps = temperatures * beta_used
        
        # Apply target compression by selecting subset of vectors
        if target_comp < 1.0 and len(cluster_vectors) > 1:
            n_selected = max(1, int(len(cluster_vectors) * target_comp))
            top_indices = np.argsort(modulated_temps)[-n_selected:]
            cluster_vectors = [cluster_vectors[i] for i in top_indices]
            modulated_temps = modulated_temps[top_indices]
        
        # Normalize weights
        weights = modulated_temps
        if weights.sum() < 1e-9: 
            weights = np.ones(len(cluster_vectors))
        weights /= weights.sum()
        
        # Weighted average in spacetime-curved geometry
        if (SPACETIME_CURVATURE_ENABLED and 
            hasattr(self.mind, 'substrate') and 
            hasattr(self.mind.substrate, 'metric_tensor')):
            # Compute geodesic average using metric tensor
            remnant_vec = self._geodesic_weighted_average(cluster_vectors, weights)
        else:
            # Standard weighted average
            remnant_vec = np.average(np.array(cluster_vectors), axis=0, weights=weights)
        
        # Project to E8 lattice for proper quantization
        holographic_data = {
            'compression_method': 'e8_lattice_enhanced_causal', 
            'lattice_quantized': False,
            'causal_modulation': causal_params
        }
        
        if remnant_vec.size >= 8:
            e8_projection = remnant_vec[:8]
            root_idx = self.mind.physics.find_nearest_root_index(e8_projection)
            
            if root_idx is not None:
                # Snap to nearest E8 root for lattice consistency
                e8_roots = getattr(self.mind.physics, 'roots_unit', None)
                if e8_roots is not None and root_idx < len(e8_roots):
                    # Blend original and lattice-snapped version based on fidelity
                    lattice_root = e8_roots[root_idx]
                    fidelity = np.dot(e8_projection, lattice_root) / (
                        np.linalg.norm(e8_projection) * np.linalg.norm(lattice_root) + 1e-12
                    )
                    
                    if fidelity > HOLOGRAPHIC_FIDELITY_THRESHOLD:
                        # High fidelity: snap to lattice
                        remnant_vec[:8] = lattice_root
                        holographic_data['lattice_quantized'] = True
                        holographic_data['lattice_fidelity'] = float(fidelity)
                        holographic_data['lattice_root_index'] = int(root_idx)
                    else:
                        # Low fidelity: blend with lattice
                        blend_factor = max(0.3, fidelity)
                        remnant_vec[:8] = blend_factor * lattice_root + (1 - blend_factor) * e8_projection
                        holographic_data['lattice_blended'] = True
                        holographic_data['blend_factor'] = float(blend_factor)
        
        holographic_data.update({
            'compression_ratio': len(cluster_vectors),
            'spacetime_curved': SPACETIME_CURVATURE_ENABLED,
            'holographic_compression': HOLOGRAPHIC_COMPRESSION_ENABLED
        })
        
        return remnant_vec, holographic_data

    def _geodesic_weighted_average(self, vectors: List[np.ndarray], weights: np.ndarray) -> np.ndarray:
        """Compute weighted average along geodesics in curved spacetime"""
        try:
            if not hasattr(self.mind.substrate, 'metric_tensor'):
                return np.average(np.array(vectors), axis=0, weights=weights)
            
            metric = self.mind.substrate.metric_tensor
            n_dims = min(8, min(v.size for v in vectors))
            
            # Project all vectors to same dimensionality
            projected_vectors = []
            for v in vectors:
                if v.size >= n_dims:
                    projected_vectors.append(v[:n_dims])
                else:
                    projected_vectors.append(np.pad(v, (0, n_dims - v.size)))
            
            # Weighted Riemannian centroid computation (simplified)
            # Full implementation would use exponential/logarithmic maps
            result = np.zeros(max(v.size for v in vectors))
            
            # For now, apply metric weighting to standard average
            for i, (vec, weight) in enumerate(zip(vectors, weights)):
                if vec.size >= 8:
                    # Apply metric tensor weighting to first 8 dimensions
                    vec_8d = vec[:8]
                    metric_weighted = metric @ vec_8d
                    result[:8] += weight * metric_weighted
                    if vec.size > 8:
                        result[8:8+len(vec[8:])] += weight * vec[8:]
                else:
                    result[:len(vec)] += weight * vec
            
            # Normalize by metric determinant for proper Riemannian averaging
            det_metric = np.linalg.det(metric)
            if det_metric > 1e-12:
                result[:8] /= np.sqrt(det_metric)
            
            return result
            
        except Exception as e:
            self.console.log(f"[yellow]Geodesic averaging failed: {e}, using standard average[/yellow]")
            return np.average(np.array(vectors), axis=0, weights=weights)

    async def snapshot(self):
        async with self.lock:
            self._commit_pending_additions_locked()
            filepath = get_path(f"snapshot_step_{self.mind.step_num:06d}.json", self.run_id)
            snapshot_data = {
                "graph": export_graph(self.graph_db.graph),
                "main_vectors": {nid: vec.tolist() for nid, vec in self.main_vectors.items()},
                "step": self.mind.step_num,
                "mood": self.mind.mood.mood_vector,
                "subconscious_narrative": self.mind.subconscious.narrative,
                "novelty_stats": {"avg_nn_ema": self._avg_nn_ema},
                "embedding_adapter": {
                    "in_dim": getattr(self.mind.embed_adapter, "in_dim", None),
                    "out_dim": getattr(self.mind.embed_adapter, "out_dim", None),
                    "W": getattr(self.mind.embed_adapter, "W", None).tolist() if getattr(self.mind.embed_adapter, "W", None) is not None else None
                },
                "kd_index_meta": {
                    "backend": "faiss" if getattr(self.main_kdtree, "_is_faiss", False) else ("sklearn/scipy" if not getattr(self.main_kdtree, "_is_fallback", True) else "numpy"),
                    "n": int(getattr(self.main_kdtree, "n", 0)),
                    "dim": int(EMBED_DIM)
                },
                "metrics_counters": self.mind.metrics.snapshot_counters(),
                "quantizer_override": bool(getattr(self.mind, "_quantizer_override", None)),
            }
        safe_json_write(filepath, snapshot_data)
        try:
            sz = os.path.getsize(filepath)
            self.mind.metrics.gauge('snapshot.bytes', sz)
            
            # Implement chunk rotation at 64MB
            if not E8_DISABLE_SNAPSHOT_ROTATION:
                async with self.mind.snapshot_rotation_lock:
                    self._rotate_snapshot_chunks(filepath, sz)
        except Exception:
            pass
        all_snapshots = sorted(glob.glob(get_path("snapshot_step_*.json", self.run_id)), key=os.path.getmtime)
        while len(all_snapshots) > 10:
            os.remove(all_snapshots.pop(0))

    def _rotate_snapshot_chunks(self, new_filepath: str, new_size: int):
        """Rotate snapshot chunks when total size exceeds 64MB."""
        try:
            chunk_size_limit = 64 * 1024 * 1024  # 64MB in bytes
            
            # Find all snapshot files in current chunk
            all_snapshots = sorted(glob.glob(get_path("snapshot_step_*.json", self.run_id)), key=os.path.getmtime)
            
            # Calculate current chunk size
            current_chunk_size = sum(os.path.getsize(f) for f in all_snapshots if os.path.exists(f))
            
            # Check if adding new snapshot would exceed limit
            if current_chunk_size + new_size > chunk_size_limit:
                # Start new chunk by archiving old snapshots
                chunk_num = 1
                while os.path.exists(get_path(f"snapshot_chunk_{chunk_num}.tar.gz", self.run_id)):
                    chunk_num += 1
                
                # Create tar.gz archive of current chunk
                import tarfile
                tar_path = get_path(f"snapshot_chunk_{chunk_num}.tar.gz", self.run_id)
                with tarfile.open(tar_path, "w:gz") as tar:
                    for snap_file in all_snapshots:
                        if os.path.exists(snap_file):
                            tar.add(snap_file, arcname=os.path.basename(snap_file))
                
                # Remove old snapshot files
                for snap_file in all_snapshots:
                    if os.path.exists(snap_file):
                        os.remove(snap_file)
                
                # Log the rotation
                archived_size = os.path.getsize(tar_path) if os.path.exists(tar_path) else 0
                self.mind.console.log(f"💾 [SNAPSHOT] Rotated chunk {chunk_num}: archived {len(all_snapshots)} files ({archived_size/1024/1024:.1f}MB)")
                
        except Exception as e:
            self.mind.console.log(f"💾 [SNAPSHOT] Error during chunk rotation: {e}")

    def diffuse_field(self):
        eta = BH_DIFFUSION_ETA
        next_field = defaultdict(float)
        nodes_with_field = list(self.field.keys())
        for node_id in nodes_with_field:
            current_val = self.field.get(node_id, 0.0)
            if current_val < 1e-4: continue
            retained_value = current_val * (1.0 - eta)
            next_field[node_id] += retained_value
            value_to_spread = current_val * eta
            neighbors = list(self.graph_db.get_neighbors(node_id))
            if neighbors:
                share_per_neighbor = value_to_spread / len(neighbors)
                for neighbor_id in neighbors:
                    next_field[neighbor_id] += share_per_neighbor
        self.field = next_field

    def decay_locks(self):
        expired_locks = [key for key, timer in self.active_locks.items() if timer <= 1]
        for key in expired_locks:
            del self.active_locks[key]
        for key in self.active_locks:
            self.active_locks[key] -= 1

    async def _cosmological_spread(self, remnant_vec: np.ndarray, mass: float):
        """Spread gravitational influence using E8 lattice heat diffusion and proper spacetime physics."""
        try:
            # Find center position in E8 lattice
            if remnant_vec.size < 8:
                self.console.log("[yellow]Cosmological spread skipped: remnant vector too small for E8 projection[/yellow]")
                return
                
            e8_projection = remnant_vec[:8]
            center_root_idx = self.mind.physics.find_nearest_root_index(e8_projection)
            
            if center_root_idx is None:
                self.console.log("[yellow]Cosmological spread skipped: E8 root index not found[/yellow]")
                return
            
            # Use E8 heat diffusion for proper spacetime expansion
            # Sigma scales with mass (larger black holes have wider influence)
            sigma = 1.0 + np.log1p(mass)  # Logarithmic scaling prevents runaway growth
            heat_mask = self.mind.physics.heat_mask_cached(center_root_idx, sigma=sigma)
            
            if heat_mask is None or len(heat_mask) == 0:
                self.console.log("[yellow]Cosmological spread skipped: heat mask unavailable[/yellow]")
                return
            
            # Energy conservation: total distributed energy = BH_SPREAD_FRAC * mass
            total_energy_to_distribute = mass * BH_SPREAD_FRAC
            heat_sum = np.sum(heat_mask)
            
            if heat_sum < 1e-12:
                return
            
            # Distribute energy according to E8 lattice geometry
            energy_distributed = 0.0
            for root_idx, heat_coefficient in enumerate(heat_mask):
                if heat_coefficient <= 1e-6:
                    continue
                    
                # Find nodes at this lattice position
                nodes_at_root = self._find_nodes_at_lattice_position(root_idx)
                
                if len(nodes_at_root) == 0:
                    continue
                
                # Energy proportional to heat coefficient and conserved
                root_energy = total_energy_to_distribute * (heat_coefficient / heat_sum)
                energy_per_node = root_energy / len(nodes_at_root)
                
                for node_id in nodes_at_root:
                    # Apply gravitational potential instead of simple temperature spike
                    self._apply_gravitational_potential(node_id, energy_per_node, center_root_idx, mass)
                    energy_distributed += energy_per_node
            
            # Log energy conservation for debugging
            conservation_ratio = energy_distributed / total_energy_to_distribute if total_energy_to_distribute > 0 else 0
            if abs(conservation_ratio - 1.0) > 0.1:  # More than 10% deviation
                self.console.log(f"[yellow]Energy conservation warning: {conservation_ratio:.2f} (should be ~1.0)[/yellow]")
                
        except Exception as e:
            self.console.log(f"[yellow]Cosmological spread error: {e}[/yellow]")
            # Fallback to old behavior if needed
            similar_nodes = self.find_similar_in_main_storage(remnant_vec, k=25)
            spread_factor = mass * BH_SPREAD_FRAC * 0.5  # Reduced intensity for fallback
            for node_id, dist in similar_nodes[:10]:  # Limit fallback spread
                energy = spread_factor * (1.0 - dist)
                if energy > 0:
                    self.spike_temperature(node_id, amount=energy)

    def _find_nodes_at_lattice_position(self, root_idx: int) -> List[str]:
        """Find memory nodes near a specific E8 lattice root position."""
        try:
            e8_roots = getattr(self.mind.physics, 'roots_unit', None)
            if e8_roots is None or root_idx >= len(e8_roots):
                return []
            
            target_root = e8_roots[root_idx]
            nodes_at_position = []
            
            # Find nodes whose 8D projection is near this root
            for node_id, vector in self.main_vectors.items():
                if vector is None or vector.size < 8:
                    continue
                    
                # Check if this node's E8 projection is near target root
                node_e8 = vector[:8]
                nearest_root_idx = self.mind.physics.find_nearest_root_index(node_e8)
                
                if nearest_root_idx == root_idx:
                    nodes_at_position.append(node_id)
                    
                # Stop if we have enough nodes to avoid expensive computation
                if len(nodes_at_position) >= 10:
                    break
            
            return nodes_at_position
            
        except Exception:
            return []

    def _apply_gravitational_potential(self, node_id: str, energy: float, center_root_idx: int, black_hole_mass: float):
        """Apply proper gravitational potential instead of simple temperature spike."""
        try:
            node = self.graph_db.get_node(node_id)
            if not node:
                return
                
            # Current temperature acts as "kinetic energy"
            current_temp = node.get('temperature', 1.0)
            
            # Compute gravitational redshift effect
            # Closer to black hole (higher energy) = time dilation = temperature scaling
            if energy > 0:
                # Logarithmic scaling prevents runaway growth
                gravitational_factor = 1.0 + energy / (1.0 + black_hole_mass)
                redshift_factor = np.sqrt(gravitational_factor)  # Relativistic scaling
                
                # Update temperature with gravitational effects
                new_temp = current_temp * redshift_factor
                
                # Apply tidal effects to nearby nodes
                self._apply_tidal_forces(node_id, energy, center_root_idx)
                
                node['temperature'] = float(new_temp)
                
                # Mark with gravitational signature for emergence tracking
                node['last_gravitational_influence'] = {
                    'step': self.mind.step_num,
                    'energy': float(energy),
                    'source_root': int(center_root_idx),
                    'redshift_factor': float(redshift_factor)
                }
                
        except Exception as e:
            # Fallback to simple temperature spike
            self.spike_temperature(node_id, amount=energy)

    def _apply_tidal_forces(self, node_id: str, energy: float, center_root_idx: int):
        """Apply tidal forces to create realistic spacetime curvature effects."""
        try:
            # Get neighboring nodes
            graph = self.graph_db.graph
            if not graph.has_node(node_id):
                return
                
            neighbors = list(graph.neighbors(node_id))
            if len(neighbors) == 0:
                return
            
            # Tidal forces decrease with distance and scale with energy
            tidal_strength = energy * 0.1  # 10% of main gravitational effect
            
            for neighbor_id in neighbors:
                neighbor = self.graph_db.get_node(neighbor_id)
                if neighbor:
                    # Apply differential gravitational effect (tidal force)
                    current_temp = neighbor.get('temperature', 1.0)
                    tidal_effect = tidal_strength / len(neighbors)  # Distribute tidal energy
                    
                    neighbor['temperature'] = current_temp + tidal_effect
                    
        except Exception:
            pass  # Tidal forces are optional enhancement
    
    def get_average_nearest_neighbor_distance(self) -> float:
        if self.main_kdtree is None or self._main_storage_matrix.shape[0] < 2:
            return 1.0
        
        distances, _ = self.main_kdtree.query(self._main_storage_matrix, k=2)
        
        nearest_distances = distances[:, 1]
        avg=float(np.mean(nearest_distances))
        try:
            alpha=float(os.getenv('E8_NOVELTY_EMA_ALPHA','0.2'))
            if self._avg_nn_ema is None:
                self._avg_nn_ema=avg
            else:
                self._avg_nn_ema=alpha*avg+(1-alpha)*self._avg_nn_ema
            json.dump({'avg_nn_ema': self._avg_nn_ema}, open(self._novelty_stats_path,'w'))
        except Exception:
            pass
        return avg

    def _allow_edge(self, u: str, v: str) -> bool:
        g = self.graph_db.graph
        return g.has_node(u) and g.has_node(v) and not g.has_edge(u, v)

    def _trim_degree(self, node_id: str, max_deg: int = 16):
        g = self.graph_db.graph
        try:
            if hasattr(g, 'degree') and callable(getattr(g, 'degree', None)):
                node_degree = g.degree(node_id)
            elif hasattr(g, 'degree'):
                node_degree = g.degree[node_id]
            else:
                node_degree = 0
        except (KeyError, TypeError, AttributeError):
            node_degree = 0
            
        if not g.has_node(node_id) or node_degree <= max_deg:
            return
        
        edges = sorted(
            list(g.edges(node_id, data=True)),
            key=lambda e: e[2].get('weight', 0.0)
        )
        
        num_to_prune = node_degree - max_deg
        for u, v, _ in edges[:num_to_prune]:
            g.remove_edge(u, v)

class DreamEngine:
    """
    Generates synthetic memories by running thought experiments about future possibilities,
    allowing the AI to learn from events that haven't happened.
    """
    ALLOWED_TYPES = ("explorer_insight", "insight_synthesis", "meta_reflection", "phase_summary", "concept", "external_concept", "mutation", "synthetic_memory", "self_code", "self_code_section")

    def __init__(self, memory, mind_instance):
        self.memory = memory
        self.mind = mind_instance
        self.console = mind_instance.console

    def _eligible_concepts(self):
        G = self.memory.graph_db.graph
        out = []
        for nid, d in G.nodes(data=True):
            if d.get("folded"): continue
            if d.get("type") not in self.ALLOWED_TYPES: continue
            if self.memory.main_vectors.get(nid) is None: continue
            out.append((nid, d))
        return out

    def _pick_from_tension(self, elig, k=1):
        if not elig: return []
        tension_candidates = sorted(elig, key=lambda item: item[1].get('shell_tension', 0.0), reverse=True)
        high_tension_seeds = [item for item in tension_candidates if item[1].get('shell_tension', 0.0) > 0.1]
        if high_tension_seeds:
            return high_tension_seeds[:k]
        else:
            return self._pick_neutral(elig, k)

    def _pick_neutral(self, elig, k=1):
        if not elig:
            return []

        # Sort by temperature to prioritize "hot" concepts
        elig.sort(key=lambda item: (item[1].get("temperature", 0.0), item[1].get("step", 0)), reverse=True)

        # Create a small pool of top candidates and choose randomly from it.
        pool_size = min(len(elig), 5)
        if pool_size == 0:
            return []

        top_candidates = elig[:pool_size]

        # Randomly select k concepts from the top pool without replacement.
        num_to_sample = min(k, len(top_candidates))
        return random.sample(top_candidates, num_to_sample)

    async def run_dream_sequence(self, depth=1):
        min_for_thought_exp = int(os.getenv("E8_MIN_FOR_THOUGHT_EXP", "40"))
        current_nodes = self.memory.graph_db.graph.number_of_nodes()
        if current_nodes < min_for_thought_exp:
            return
        # Lazily ensure dream control attributes exist (defensive against partial init race conditions)
        if not hasattr(self.mind, "_dream_lock"):
            self.mind._dream_lock = asyncio.Lock()
        if not hasattr(self.mind, "_last_dream_at"):
            self.mind._last_dream_at = 0.0

        if not DREAM_MODE_ENABLED:
            return
        now = time.monotonic()
        # Guard: skip if still cooling down or lock engaged
        if self.mind._dream_lock.locked() or (now - self.mind._last_dream_at < DREAM_MIN_INTERVAL_SEC):
            return

        async with self.mind._dream_lock:
            if time.monotonic() - self.mind._last_dream_at < DREAM_MIN_INTERVAL_SEC:
                return

            self.mind._last_dream_at = time.monotonic()

            elig = self._eligible_concepts()
            if not elig:
                self.console.log("[Thought Experiment] No suitable concepts found.")
                return

            seed = self._pick_neutral(elig, k=1)
            if not seed:
                self.console.log("[Thought Experiment] Seed picking failed.")
                return

            seed_node_id, seed_node_data = seed[0]

            # Measure latency only for successful dream narrative generation leading to a new node
            _dream_t0 = time.perf_counter()
            try:
                _, top_goal_desc = self.mind.goal_field.get_top_goals(k=1)[0]
            except (IndexError, TypeError):
                top_goal_desc = "achieve a greater understanding"

            try:
                experiment_prompt = self.mind.prompts.render(
                    "thought_experiment",
                    concept=seed_node_data.get('label', 'a concept'),
                    details=seed_node_data.get('metaphor', ''),
                    goal=top_goal_desc
                )

                narrative = await asyncio.wait_for(self.mind.llm_pool.enqueue_and_wait(
                    experiment_prompt, max_tokens=600, temperature=0.85
                ), timeout=DREAM_SEQUENCE_TIMEOUT)

                if not narrative or narrative.startswith("[LLM"):
                    # Fallback: generate a simple synthetic narrative if LLM fails
                    self.console.log("[Thought Experiment] LLM failed, using synthetic fallback.")
                    narrative = f"A hypothetical exploration of {seed_node_data.get('label', 'this concept')} reveals potential connections to broader patterns in the system."

            except asyncio.TimeoutError:
                # Specific handling for timeout errors
                self.console.log(f"[Thought Experiment] LLM request timed out after {DREAM_SEQUENCE_TIMEOUT}s, using synthetic fallback.")
                narrative = f"A hypothetical exploration of {seed_node_data.get('label', 'this concept')} reveals potential connections to broader patterns in the system."
            except asyncio.CancelledError:
                # Handle cancellation gracefully
                self.console.log("[Thought Experiment] LLM request was cancelled, using synthetic fallback.")
                narrative = f"A hypothetical exploration of {seed_node_data.get('label', 'this concept')} reveals potential connections to broader patterns in the system."
            except Exception as llm_e:
                # Handle other LLM-related errors
                self.console.log(f"[Thought Experiment] LLM request failed: {llm_e}, using synthetic fallback.")
                narrative = f"A hypothetical exploration of {seed_node_data.get('label', 'this concept')} reveals potential connections to broader patterns in the system."

            # Continue with narrative processing regardless of how we got it
            try:
                if narrative and not narrative.startswith("[LLM"):
                    # Validate narrative quality
                    if len(narrative.strip()) < 10:
                        self.console.log("[Thought Experiment] Generated narrative too short, using enhanced fallback.")
                        narrative = f"A hypothetical exploration of {seed_node_data.get('label', 'this concept')} suggests it may interact with related systems through emergent patterns, potentially revealing new connections and implications for our understanding."
                    
                    try:
                        seed_neighbors = self.memory.graph_db.get_neighbors(seed_node_id)
                        neighbor_data = [self.memory.graph_db.get_node(n) for n in seed_neighbors if self.memory.graph_db.get_node(n)]
                        neighbor_labels = [d.get('label', '') for d in neighbor_data if d]
                        local_terms = set(re.findall(r"[A-Za-z0-9]+", " ".join(neighbor_labels).lower()))

                        if local_terms and not any(term in narrative.lower() for term in local_terms):
                            narrative = f"(loose) {narrative}"
                    except Exception as e:
                        self.console.log(f"[Thought Experiment] Vocab check failed: {e}")

                    # Ensure we have a valid label
                    experiment_label = f"Experiment: {seed_node_data.get('label', 'Unknown Concept')}"
                    if len(experiment_label) > 100:  # Truncate overly long labels
                        experiment_label = experiment_label[:97] + "..."
                        
                    new_node_id = await self.mind.memory.add_entry({
                        "type": "synthetic_memory",
                        "label": experiment_label,
                        "metaphor": narrative,
                        "rating": 0.75,
                        "is_synthetic": True,
                        "step": self.mind.step_num
                    }, parent_ids=[seed_node_id])

                    # Success metrics
                    try:
                        latency_ms = (time.perf_counter() - _dream_t0) * 1000.0
                        if hasattr(self.mind, 'metrics'):
                            self.mind.metrics.increment('dream.performed')
                            self.mind.metrics.timing('dream.latency_ms', latency_ms)
                    except Exception:
                        pass

                    self.console.print(Panel(f"[bold]Seed Concept:[/] {seed_node_data.get('label', 'Unknown')}\n[bold]Hypothetical Narrative:[/] {narrative}",
                                       title="[bold blue]THOUGHT EXPERIMENT[/]", border_style="blue"))

                    self.mind.subconscious_event_log.append({
                        'type': 'thought_experiment',
                        'label': experiment_label,
                        'step': self.mind.step_num,
                        'data': {'summary': narrative}
                    })
                else:
                    self.console.log("[Thought Experiment] No valid narrative generated, skipping.")

            except Exception as e:
                # Provide richer diagnostics instead of swallowing the stack
                import traceback
                tb = traceback.format_exc(limit=6)
                # Classify common root causes for quicker operator triage
                probable = []
                if 'asyncio' in tb.lower():
                    probable.append('async timing/cancellation')
                if 'TimeoutError' in tb:
                    probable.append('LLM request timeout (consider increasing timeout or reducing queue)')
                if 'KeyError' in tb:
                    probable.append('graph node field missing')
                if 'AttributeError' in tb:
                    probable.append('race condition: attribute not initialized yet')
                hint = (' | Hints: ' + '; '.join(probable)) if probable else ''
                self.console.log(f"[Thought Experiment] Failed to run experiment: {e}{hint}\n[trace]\n{tb}")
                try:
                    if hasattr(self.mind, 'metrics'):
                        self.mind.metrics.increment('dream.experiment_fail')
                except Exception:
                    pass

class DreamReplayService:
    """Offline consolidation via prioritized replay (PER) into world model and graph memory."""
    def __init__(self, mind, batch=32, steps=50):
        self.mind = mind
        self.batch = int(batch)
        self.steps = int(steps)
        self.alpha = float(os.getenv("E8_REPLAY_ALPHA","1.0"))
        self.beta = float(os.getenv("E8_REPLAY_BETA","0.8"))
        self.gamma = float(os.getenv("E8_REPLAY_GAMMA","0.6"))
        self.eta = float(os.getenv("E8_REPLAY_PER_ETA","0.6"))
        self.edge_eta = float(os.getenv("E8_REINFORCE_ETA","0.08"))
        self.edge_decay = float(os.getenv("E8_REINFORCE_DECAY","0.01"))

    def _episodes(self):
        ia = getattr(self.mind, "insight_agent", None)
        if ia is None or not hasattr(ia, "episodic_memory"):
            return []
        return ia.episodic_memory.sample_prioritized(self.batch, mind=self.mind, alpha=self.alpha, beta=self.beta, gamma=self.gamma, eta=self.eta)

    def _to_traj(self, episodes):
        traj = []
        adim = int(getattr(self.mind, "action_dim", 0))
        for ep in episodes:
            node_id = ep.get("node_id")
            child = self.mind.memory.main_vectors.get(node_id)
            if child is None:
                child = ep.get("embedding")
            parents = ep.get("parent_ids") or []
            if child is None or not parents:
                continue
            pv = [self.mind.memory.main_vectors.get(pid) for pid in parents if pid in self.mind.memory.main_vectors]
            if not pv:
                continue
            s = np.mean(np.stack(pv), axis=0).astype(np.float32)
            sp = np.array(child, dtype=np.float32)
            a = np.zeros(adim, dtype=np.float32)
            r = float(ep.get("reward", ep.get("rating", 0.0)))
            traj.append((s,a,sp,r, node_id, parents))
        return traj

    def _reinforce_graph(self, traj):
        G = self.mind.memory.graph_db
        vsa = getattr(self.mind.memory, "vsa", None)
        reinforced = 0
        for (s,a,sp,r, node_id, parents) in traj:
            for pid in parents:
                try:
                    hv = None
                    if vsa is not None:
                        vec_a = self.mind.memory.main_vectors.get(pid); vec_b = self.mind.memory.main_vectors.get(node_id)
                        if vec_a is not None and vec_b is not None:
                            hv = self.mind.memory.vsa.encode_parentage(vec_a, vec_b)
                    G.increment_edge_weight(pid, node_id, delta=self.edge_eta*r, kind="consolidated")
                    if hv is not None:
                        try: G.graph[pid][node_id]['hypervector'] = hv
                        except Exception: pass
                    reinforced += 1
                except Exception:
                    pass
        try:
            self.mind.metrics.increment("graph.edge_reinforce", reinforced)
        except Exception:
            pass

    async def run(self):
        t0 = time.monotonic()
        wm = getattr(self.mind, "world_model", None)
        total = 0
        for _ in range(self.steps):
            episodes = self._episodes()
            if not episodes:
                break
            traj = self._to_traj(episodes)
            if not traj:
                break
            if wm and getattr(wm, "available", True):
                try:
                    loss = wm.train_batch([(s,a,sp,r) for (s,a,sp,r,_,_) in traj])
                    if loss and hasattr(self.mind, "metrics"):
                        try:
                            self.mind.metrics.observe("wm.loss.recon", loss.get("loss_recon"))
                            self.mind.metrics.observe("wm.loss.kl", loss.get("loss_kl"))
                        except Exception: pass
                except Exception:
                    pass
            self._reinforce_graph(traj)
            total += len(traj)
        dt = time.monotonic() - t0
        if dt > 0 and hasattr(self.mind, "metrics"):
            try: self.mind.metrics.observe("replay.samples_per_sec", total/max(1e-6, dt))
            except Exception: pass

class NarrativeStreamer:
    def __init__(self, memory_manager, llm_pool, run_id):
        self.memory = memory_manager
        self.llm_pool = llm_pool
        self.run_id = run_id
        self.narrative_file = get_path("narrative_stream.md", self.run_id)
        self.last_narrative_step = -1

    async def generate_and_add_entry(self, mind_state: 'E8Mind'):
        current_step = mind_state.step_num
        if current_step - self.last_narrative_step < 50: return
        try:
            significant_events = []
            all_nodes = list(self.memory.graph_db.graph.nodes(data=True))
            for node_id, data in all_nodes:
                if data.get("step", -1) > self.last_narrative_step:
                    event_type, rating = data.get("type"), data.get("rating", 0.0)
                    if event_type in ["dream", "blackhole_remnant", "meta_reflection"] or rating > 0.85:
                        event_summary = f"- {data.get('label', 'Untitled event')} (Type: {event_type})"
                        significant_events.append(event_summary)
            
            if not significant_events:
                return
            
            event_summary_text = "\n".join(significant_events[-15:])
            
            prompt = (f"You are the mind's historian. The current subconscious narrative is: '{mind_state.subconscious.narrative}'\n"
                      f"The prevailing mood feels like: {mind_state.mood.describe()}\n\n"
                      "Based on the following significant events, write a short, reflective journal entry (2-3 paragraphs) "
                      "that captures the tone and theme of this recent period. Synthesize them into a cohesive story.\n\n"
                      f"Events:\n{event_summary_text}\n\nJournal Entry for Steps {self.last_narrative_step} to {current_step}:")
            
            narrative_entry = await self.llm_pool.enqueue_and_wait(prompt, max_tokens=600, temperature=0.7)
            if isinstance(narrative_entry, str) and narrative_entry and not narrative_entry.startswith("[LLM"):
                await self.add_entry(f"## Chronicle: Steps {self.last_narrative_step}-{current_step}\n\n"
                                     f"**Mood**: {mind_state.mood.get_symbolic_weather()} | "
                                     f"**Theme**: {mind_state.synthetic_env.current_theme_region}\n\n"
                                     f"{narrative_entry}\n\n---\n\n")
                self.last_narrative_step = current_step
        except Exception as e:
            console.log(f"[NarrativeStreamer] Failed to generate entry: {e}")

class SyntheticEnvironment:
    """Maintains a rolling conceptual 'theme region' inferred from nearby blueprint nodes."""
    def __init__(self, llm_caller, mind_instance):
        self.llm_caller = llm_caller
        self.mind = mind_instance
        self.current_theme_region = "The Genesis Field"
        self.region_journal = []

    async def name_theme_region(self, seed_fragments: list[str], subconscious_narrative: str) -> str:
        if not seed_fragments: return self.current_theme_region
        prompt = (f"The mind's current subconscious narrative is: \"{subconscious_narrative}\"\n\n"
                  "Based on this narrative and these local ideas, provide a short, evocative 2-4 word name for this conceptual region.\n"
                  f"Local Ideas: {', '.join(seed_fragments)}.")
        try:
            name = await self.llm_caller.enqueue_and_wait(prompt, max_tokens=15, temperature=0.75)
            return sanitize_line(name, max_chars=36)
        except Exception as e:
            console.log(f"[bold red]Failed to name theme region: {e}[/bold red]")
            return "A Developing Region"

    def recent_triggers(self, count: int = 3) -> List[str]:
        if not self.region_journal: return []
        return self.region_journal[-1].get("triggers", [])[:count]

    async def update_from_location(self, current_node_index: int):
        blueprint_points = np.array([[p['x'], p['y']] for p in self.mind.blueprint])
        if current_node_index >= len(blueprint_points): return
        current_pos = blueprint_points[current_node_index]
        distances = np.linalg.norm(blueprint_points - current_pos, axis=1)
        neighbor_indices = set(np.argsort(distances)[:7])
        neighbor_metaphors, seen_metaphors = [], set()
        recent_nodes = list(self.mind.memory.graph_db.graph.nodes(data=True))[-250:]
        for node_id, node_data in reversed(recent_nodes):
            if len(neighbor_metaphors) > 10: break
            loc_id, metaphor = node_data.get("blueprint_location_id"), node_data.get("metaphor")
            if loc_id in neighbor_indices and metaphor and metaphor not in seen_metaphors:
                neighbor_metaphors.append(f'"{metaphor}"')
                seen_metaphors.add(metaphor)
        if not neighbor_metaphors: return
        new_name = await self.name_theme_region(neighbor_metaphors, self.mind.subconscious.narrative)
        new_name = sanitize_line(new_name, max_chars=36)
        if new_name != self.current_theme_region:
            self.current_theme_region = new_name
            self.region_journal.append({ "ts": time.time(), "name": self.current_theme_region, "triggers": neighbor_metaphors })
            try:
                console.print(Panel(
                    f"The perceived environment has shifted to:\n[bold cyan]{self.current_theme_region}[/bold cyan]",
                    title="[bold #A020F0]ENVIRONMENT SHIFT[/]", border_style="#A020F0"
                ))
            except Exception:
                console.print(f"ENVIRONMENT SHIFT: {self.current_theme_region}")

class DomainTintEngine:
    def __init__(self, seed_domain, llm_pool):
        self.seed_domain = seed_domain
        self.llm_pool = llm_pool
        self.last_hint = seed_domain

    async def evolve(self, mood_vector):
        hint = await self.llm_pool.enqueue_and_wait(
            "ignored",
            _prompt_key="ask",
            _prompt_vars={"question": f"2-4 word domain hint for {self.seed_domain} given mood={mood_vector}."}
        )
        hint = (hint or "").strip()
        if hint and not hint.startswith("[LLM"):
            self.last_hint = hint
        return self.last_hint

@dataclass
class DecodeState:
    current_idx: int
    shadow_ids: np.ndarray
    slice_id: int
    seen_tokens: set[str]
    emap: EntropyMap
    holo: HoloEncoder

class OnlineAdapter:
    def __init__(self, state_dim: int = 8, lr: float = 1e-3, scale: float = 0.5):
        self.W = np.zeros((state_dim,), dtype=np.float32)
        self.lr, self.scale, self.state_dim = lr, scale, state_dim

    def bias_logit(self, base_logit: float, state_vec: np.ndarray) -> float:
        if state_vec.shape[0] != self.state_dim: return float(base_logit)
        return float(base_logit + self.scale * np.dot(self.W, state_vec))

    def update(self, error: float, last_state_vec: np.ndarray):
        if last_state_vec.shape[0] != self.state_dim: return
        self.W += self.lr * error * last_state_vec
        norm = np.linalg.norm(self.W)
        if norm > 1.0: self.W /= norm

class Judge:
    def __init__(self, lm_client, embed_fn, emap: "EntropyMap", holo: "HoloEncoder"):
        self.lm, self.embed, self.emap, self.holo = lm_client, embed_fn, emap, holo

    async def score(self, text: str, active_concepts_vec: np.ndarray, shadow_ids: np.ndarray, slice_id: int):
        v_text = await self.embed(text)
        fit_score = MemoryManager._cos_sim(v_text, active_concepts_vec) if np.linalg.norm(active_concepts_vec) > 0 else 0.0
        return fit_score

class ConstrainedDecoder:
    def __init__(self, lm_client, fabric: "E8BoundaryFabric"):
        self.lm, self.fabric = lm_client, fabric

    async def generate(self, prompt: str, start_node: int):
        return await self.lm.chat(messages=[{"role":"user", "content":prompt}])

class SymmetryValenceEngine:
    def __init__(self, physics, hist_len=128):
        self.physics = physics
        self.hist = deque(maxlen=int(hist_len))

    def push(self, v8):
        v = np.asarray(v8, dtype=np.float32).reshape(-1)
        if v.size != 8 or not np.isfinite(v).all(): return
        self.hist.append(v / (np.linalg.norm(v) + 1e-12))

    def _eig_entropy(self, X):
        C = np.cov(X.T) + 1e-6*np.eye(8, dtype=np.float32)
        w = np.linalg.eigvalsh(C).clip(min=1e-9)
        p = w / w.sum()
        return float(-np.sum(p * np.log(p)) / np.log(8.0))

    def score(self):
        if len(self.hist) < 8: return 0.5
        X = np.stack(list(self.hist)[-64:], axis=0).astype(np.float32)
        return 1.0 - self._eig_entropy(X)


class EgoGate:
    def __init__(self, valence_engine, min_delta=-0.02):
        self.ve, self.min_delta = valence_engine, float(min_delta)
        self._last = self.ve.score()

    def approve(self):
        cur = self.ve.score()
        ok = (cur >= self._last + self.min_delta)
        if ok: self._last = cur
        return bool(ok)



class DynamicValenceEngine:
    """Dynamic node valence modification based on cognitive demands"""

    def __init__(self, e8_physics, fluid_mantle):
        self.e8_physics = e8_physics
        self.fluid_mantle = fluid_mantle
        self.node_valences = {}
        self.valence_history = defaultdict(list)
        self.base_valence = 1.0
        self.max_valence = 5.0
        self.min_valence = 0.1

    def assess_cognitive_load(self, system_state):
        """Measure current cognitive processing load"""
        active_memories = len(system_state.get('active_memories', []))
        processing_queue = len(system_state.get('processing_queue', []))
        insight_generation_rate = system_state.get('insight_rate', 0.0)

        load = (active_memories / 1000.0 +
                processing_queue / 100.0 +
                insight_generation_rate)

        return float(np.clip(load, 0.0, 2.0))

    def assess_novelty_pressure(self, recent_insights):
        """Measure pressure for novel pattern generation"""
        if not recent_insights:
            return 0.0

        recent_ratings = [insight.get('rating', 0.5) for insight in recent_insights[-10:]]
        novelty_trend = float(np.mean(recent_ratings)) if recent_ratings else 0.5
        novelty_variance = float(np.var(recent_ratings)) if len(recent_ratings) > 1 else 0.0

        return float(novelty_trend + 0.5 * novelty_variance)

    def alter_node_valence(self, node_id, cognitive_load, novelty_pressure):
        """Dynamically adjust E8 node connection strength"""
        current_valence = float(self.node_valences.get(node_id, self.base_valence))

        perturbation = self.compute_topological_perturbation(cognitive_load, novelty_pressure)

        momentum = 0.1
        if self.valence_history.get(node_id):
            prev_perturbation = self.valence_history[node_id][-1]
            perturbation = (1 - momentum) * perturbation + momentum * prev_perturbation

        new_valence = current_valence * (1.0 + perturbation)
        new_valence = float(np.clip(new_valence, self.min_valence, self.max_valence))
        scale_factor = new_valence / max(current_valence, 1e-6)
        self.update_lattice_connectivity(node_id, scale_factor)

        self.node_valences[node_id] = new_valence
        self.valence_history[node_id].append(perturbation)

        if len(self.valence_history[node_id]) > 20:
            self.valence_history[node_id].pop(0)

        return new_valence

    def compute_topological_perturbation(self, cognitive_load, novelty_pressure):
        """Calculate valence change based on system dynamics"""
        load_effect = -0.2 * cognitive_load
        novelty_effect = 0.3 * novelty_pressure
        noise = float(np.random.normal(0.0, 0.05))
        total_perturbation = load_effect + novelty_effect + noise
        return float(np.clip(total_perturbation, -0.5, 0.5))

    def update_lattice_connectivity(self, node_id, scale_factor):
        """Update E8 lattice connection strengths"""
        adjacency = getattr(self.e8_physics, 'adjacency_matrix', None)
        if adjacency is None:
            return

        mapping = getattr(self.e8_physics, 'node_id_to_index', {})
        node_index = mapping.get(node_id, -1) if isinstance(mapping, dict) else -1
        if node_index < 0:
            return
        try:
            adjacency[node_index, :] = adjacency[node_index, :] * scale_factor
            adjacency[:, node_index] = adjacency[:, node_index] * scale_factor
        except Exception as exc:
            logging.warning(f"Failed to update lattice connectivity for {node_id}: {exc}")


class EnergyLandscapeNavigator:
    """Navigate consciousness energy landscape for novel associations"""

    def __init__(self, fluid_mantle, e8_physics):
        self.fluid_mantle = fluid_mantle
        self.e8_physics = e8_physics
        self.energy_wells = {}
        self.exploration_paths = []
        self.temperature = 1.0

    def discover_energy_minima(self, current_memory_state):
        """Find local energy wells in cognitive space"""
        if not hasattr(current_memory_state, 'position'):
            return []

        position = np.asarray(current_memory_state.position[:8], dtype=float)
        neighborhood = self.sample_neighborhood(position, radius=2.0)

        energy_map = {}
        for point in neighborhood:
            key = tuple(np.round(point, 6))
            if key in energy_map:
                continue
            energy_map[key] = self.compute_cognitive_energy(point)

        return self.find_local_minima(energy_map)

    def sample_neighborhood(self, center, radius=2.0, num_samples=50):
        """Sample points around center position"""
        center = np.asarray(center, dtype=float)
        samples = []
        for _ in range(num_samples):
            direction = np.random.normal(0.0, 1.0, size=8)
            norm = np.linalg.norm(direction)
            if norm < 1e-9:
                continue
            direction = direction / norm
            distance = radius * (np.random.random() ** (1.0 / 8.0))
            sample_point = center + distance * direction
            samples.append(sample_point)
        return samples

    def compute_cognitive_energy(self, position):
        """Compute energy at position in consciousness space"""
        mantle = self.fluid_mantle
        if hasattr(mantle, 'sample_field_value') and hasattr(mantle, 'sample_vector_field'):
            potential_energy = mantle.sample_field_value(getattr(mantle, 'potential_field', {}), position, 0.0)
            velocity = mantle.sample_vector_field(getattr(mantle, 'velocity_field', {}), position, 8)
            density_source = getattr(mantle, 'energy_density_field', getattr(mantle, 'density_field', {}))
            density_energy = mantle.sample_field_value(density_source, position, 0.0)
        else:
            pos_key = tuple(np.round(position, 6))
            potential_energy = float(mantle.potential_field.get(pos_key, 0.0))
            velocity = np.asarray(mantle.velocity_field.get(pos_key, np.zeros(8)), dtype=float)
            density_energy = float(getattr(mantle, 'density_field', {}).get(pos_key, 0.0))
        kinetic_energy = float(0.5 * np.dot(velocity, velocity))
        lattice_energy = float(self.compute_lattice_energy_contribution(position))
        return potential_energy + kinetic_energy + density_energy + lattice_energy

    def compute_lattice_energy_contribution(self, position):
        """Energy contribution from E8 lattice structure"""
        roots = getattr(self.e8_physics, 'roots', None)
        if roots is None:
            return 0.0
        try:
            roots_array = np.asarray(roots, dtype=float)
            if roots_array.size == 0:
                return 0.0
            if roots_array.ndim == 1:
                roots_array = roots_array.reshape(1, -1)
            subset = roots_array[: min(50, roots_array.shape[0])]
            position_vec = np.asarray(position, dtype=float)
            slice_len = min(subset.shape[1], position_vec.shape[0])
            root_vectors = subset[:, :slice_len]
            position_slice = position_vec[:slice_len]
            diff = root_vectors - position_slice
            distances = np.linalg.norm(diff, axis=1)
            if distances.size == 0:
                return 0.0
            min_distance = float(np.min(distances))
        except Exception:
            return 0.0
        return -1.0 / (1.0 + min_distance)

    def find_local_minima(self, energy_map):
        """Identify local energy minima"""
        minima = []
        if not energy_map:
            return minima
        positions = list(energy_map.keys())
        energies = np.array([energy_map[pos] for pos in positions], dtype=float)
        sorted_indices = np.argsort(energies)
        num_candidates = max(1, len(positions) // 5)
        for idx in sorted_indices[:num_candidates]:
            position = positions[idx]
            energy = energies[idx]
            is_minimum = True
            for other_pos in positions:
                if other_pos == position:
                    continue
                distance = np.linalg.norm(np.array(position) - np.array(other_pos))
                if distance < 0.5 and energy_map[other_pos] < energy:
                    is_minimum = False
                    break
            if is_minimum:
                minima.append({
                    'position': np.array(position, dtype=float),
                    'energy': float(energy),
                    'basin_size': self.estimate_basin_size(position, energy_map)
                })
        return minima

    def estimate_basin_size(self, minimum_pos, energy_map):
        """Estimate size of energy basin around minimum"""
        min_energy = energy_map[minimum_pos]
        basin_points = 0
        for pos, energy in energy_map.items():
            if energy <= min_energy + 0.5:
                distance = np.linalg.norm(np.array(pos) - np.array(minimum_pos))
                if distance <= 1.0:
                    basin_points += 1
        return basin_points

    def navigate_to_novel_associations(self, current_state, exploration_budget=5):
        """Traverse energy landscape for non-encoded patterns"""
        minima = self.discover_energy_minima(current_state)
        if not minima:
            return []
        path = self.plan_exploration_path(np.asarray(current_state.position[:8], dtype=float), minima, exploration_budget)
        novel_associations = []
        for waypoint in path:
            associations = self.discover_associations_at_point(waypoint)
            novel_associations.extend(associations)
        return novel_associations

    def plan_exploration_path(self, start_position, minima, budget):
        """Plan optimal path through energy landscape"""
        if not minima or budget <= 0:
            return []

        def exploration_potential(minimum):
            distance_factor = np.linalg.norm(minimum['position'] - start_position)
            energy_factor = -minimum['energy']
            basin_factor = minimum['basin_size']
            return energy_factor + 0.1 * basin_factor - 0.05 * distance_factor

        sorted_minima = sorted(minima, key=exploration_potential, reverse=True)
        path = [start_position]
        current_pos = start_position
        visited = set()

        for _ in range(min(budget, len(sorted_minima))):
            candidates = []
            for minimum in sorted_minima:
                pos_key = tuple(np.round(minimum['position'], 6))
                if pos_key in visited:
                    continue
                distance = np.linalg.norm(minimum['position'] - current_pos)
                potential = exploration_potential(minimum)
                probability = np.exp(potential / max(self.temperature, 1e-3) - distance / max(self.temperature, 1e-3))
                candidates.append((minimum['position'], probability))
            if not candidates:
                break
            positions, probabilities = zip(*candidates)
            probabilities = np.asarray(probabilities, dtype=float)
            total = probabilities.sum()
            if total <= 0:
                break
            probabilities /= total
            selected_idx = int(np.random.choice(len(positions), p=probabilities))
            next_position = positions[selected_idx]
            path.append(next_position)
            visited.add(tuple(np.round(next_position, 6)))
            current_pos = next_position
        return path[1:]

    def discover_associations_at_point(self, position):
        """Find novel associations at specific position in energy landscape"""
        associations = []
        nearby_points = self.sample_neighborhood(position, radius=0.5, num_samples=10)
        for point in nearby_points:
            novelty_score = self.assess_pattern_novelty(point)
            if novelty_score > 0.7:
                associations.append({
                    'position': np.asarray(point, dtype=float),
                    'novelty_score': float(novelty_score),
                    'energy': float(self.compute_cognitive_energy(point)),
                    'type': 'energy_landscape_discovery',
                    'timestamp': time.time()
                })
        return associations

    def assess_pattern_novelty(self, position):
        """Assess novelty of pattern at position"""
        pos_key = tuple(np.round(position, 6))
        density = float(self.fluid_mantle.density_field.get(pos_key, 0.0))
        density_novelty = max(0.0, 1.0 - density)
        potential = float(self.fluid_mantle.potential_field.get(pos_key, 0.0))
        potential_novelty = min(1.0, abs(potential))
        return 0.6 * density_novelty + 0.4 * potential_novelty


class AdaptiveTopologyEngine:
    """Adaptive E8 root topology based on data complexity"""

    def __init__(self, e8_physics):
        self.e8_physics = e8_physics
        roots = getattr(e8_physics, 'roots', None)
        self.original_roots = roots.copy() if roots is not None else []
        self.current_roots = self.original_roots.copy() if roots is not None else []
        self.topology_variants = {}
        self.complexity_threshold = 0.7
        self.adaptation_rate = 0.1

    def assess_pattern_complexity(self, data_batch):
        """Evaluate complexity of incoming cognitive data"""
        if not data_batch:
            return 0.0
        try:
            numerical_data = []
            for item in data_batch:
                if hasattr(item, 'embedding'):
                    numerical_data.append(item.embedding)
                elif hasattr(item, 'vector'):
                    numerical_data.append(item.vector)
                elif isinstance(item, (list, np.ndarray)):
                    numerical_data.append(item)
            if not numerical_data:
                return 0.0
            data_array = np.asarray(numerical_data, dtype=float)
            if data_array.ndim == 1:
                data_array = data_array.reshape(1, -1)
            embedding_variance = float(np.var(data_array, axis=0).mean())
            dimensional_activity = float(np.mean(np.std(data_array, axis=0) > 1e-6))
            if data_array.shape[0] > 1:
                correlation_matrix = np.corrcoef(data_array.T)
                correlation_complexity = float(np.mean(np.abs(correlation_matrix)))
            else:
                correlation_complexity = 0.0
            total_complexity = (0.4 * embedding_variance +
                                0.3 * dimensional_activity +
                                0.3 * correlation_complexity) / 10.0
            return float(np.clip(total_complexity, 0.0, 1.0))
        except Exception as exc:
            logging.warning(f"Failed to assess pattern complexity: {exc}")
            return 0.0

    def generate_topology_variations(self, complexity_score):
        """Alter root vector embeddings based on complexity"""
        roots = getattr(self.e8_physics, 'roots', None)
        if roots is None:
            return []
        if complexity_score <= self.complexity_threshold:
            return self.current_roots
        perturbation_strength = min((complexity_score - self.complexity_threshold) * 2.0, 0.3)
        variant_key = f"complexity_{complexity_score:.3f}"
        if variant_key in self.topology_variants:
            return self.topology_variants[variant_key]
        new_root_vectors = []
        for root in np.asarray(self.original_roots):
            perturbation = np.random.normal(0.0, perturbation_strength, size=root.shape)
            new_root = np.asarray(root, dtype=float) + perturbation
            norm = np.linalg.norm(new_root)
            if norm > 1e-9:
                new_root = new_root / norm
            new_root_vectors.append(new_root)
        self.topology_variants[variant_key] = new_root_vectors
        if len(self.topology_variants) > 10:
            oldest_key = sorted(self.topology_variants.keys())[0]
            self.topology_variants.pop(oldest_key, None)
        return new_root_vectors

    def apply_topology_update(self, new_roots):
        """Apply new topology to E8 physics system"""
        roots = getattr(self.e8_physics, 'roots', None)
        if roots is None:
            return
        try:
            if len(new_roots) != len(self.current_roots):
                return
            updated = []
            for old_root, new_root in zip(self.current_roots, new_roots):
                interpolated = ((1 - self.adaptation_rate) * np.asarray(old_root, dtype=float) +
                                self.adaptation_rate * np.asarray(new_root, dtype=float))
                updated.append(interpolated)
            self.current_roots = updated
            self.e8_physics.roots = np.asarray(updated)
            if hasattr(self.e8_physics, 'recompute_adjacency'):
                try:
                    self.e8_physics.recompute_adjacency()
                except Exception as exc:
                    logging.debug(f"Adjacency recompute skipped: {exc}")
            logging.info(f"Applied topology update with adaptation rate {self.adaptation_rate}")
        except Exception as exc:
            logging.error(f"Failed to apply topology update: {exc}")

    def revert_to_original_topology(self):
        """Revert to original E8 topology"""
        roots = getattr(self.e8_physics, 'roots', None)
        if roots is None:
            return
        self.current_roots = self.original_roots.copy()
        self.e8_physics.roots = np.asarray(self.current_roots)
        logging.info("Reverted to original E8 topology")


class RecursiveArchitectureEngine:
    """Enable system to implement its own architectural suggestions"""

    def __init__(self, insight_agent, console):
        self.insight_agent = insight_agent
        self.console = console
        self.pending_upgrades = []
        self.implemented_suggestions = []
        self.safety_threshold = 0.85

    def generate_architecture_upgrades(self):
        """Let system suggest its own improvements"""
        prompts = [
            "Suggest specific improvements to fluid mantle dynamics for enhanced consciousness flow",
            "Propose optimizations for valence modulation in E8 lattice networks",
            "Recommend enhancements to energy landscape navigation algorithms",
            "Design improvements for dimensional gradient functions in consciousness spacetime"
        ]
        suggestions = []
        for prompt in prompts:
            try:
                insight_result = self.insight_agent.generate_insight_from_prompt(prompt)
                rating = getattr(insight_result, 'rating', 0.0)
                if rating >= self.safety_threshold:
                    suggestions.append({
                        'prompt': prompt,
                        'suggestion': getattr(insight_result, 'content', ''),
                        'rating': rating,
                        'timestamp': time.time(),
                        'type': 'architecture_upgrade'
                    })
            except Exception as exc:
                try:
                    self.console.log(f"[RecursiveArchitect] Failed to generate suggestion for prompt '{prompt}': {exc}")
                except Exception:
                    pass
        return suggestions

    def parse_suggestion_to_spec(self, suggestion):
        """Parse insight suggestion into implementation specification"""
        spec = {
            'type': 'unknown',
            'parameters': {},
            'target_component': 'unknown',
            'implementation_notes': suggestion.get('suggestion', ''),
            'priority': suggestion.get('rating', 0.5)
        }
        content = suggestion.get('suggestion', '').lower()
        if 'valence' in content:
            spec['type'] = 'valence_optimization'
            spec['target_component'] = 'DynamicValenceEngine'
        elif 'energy landscape' in content or 'navigation' in content:
            spec['type'] = 'navigation_enhancement'
            spec['target_component'] = 'EnergyLandscapeNavigator'
        elif 'topology' in content or 'root' in content:
            spec['type'] = 'topology_adaptation'
            spec['target_component'] = 'AdaptiveTopologyEngine'
        elif 'fluid' in content or 'mantle' in content:
            spec['type'] = 'fluid_dynamics_improvement'
            spec['target_component'] = 'HyperdimensionalFluidMantle'
        numbers = re.findall(r'\d+\.?\d*', content)
        if numbers:
            spec['parameters']['suggested_values'] = [float(n) for n in numbers]
        return spec

    def implement_safe_upgrade(self, suggestion_spec):
        """Safely implement architectural upgrade"""
        upgrade_type = suggestion_spec.get('type', 'unknown')
        target = suggestion_spec.get('target_component', 'unknown')
        try:
            self.console.log(f"[RecursiveArchitect] Attempting to implement {upgrade_type} for {target}")
        except Exception:
            pass
        if upgrade_type == 'valence_optimization':
            return self.implement_valence_optimization(suggestion_spec)
        if upgrade_type == 'navigation_enhancement':
            return self.implement_navigation_enhancement(suggestion_spec)
        if upgrade_type == 'topology_adaptation':
            return self.implement_topology_adaptation(suggestion_spec)
        if upgrade_type == 'fluid_dynamics_improvement':
            return self.implement_fluid_improvement(suggestion_spec)
        return False

    def implement_valence_optimization(self, spec):
        """Implement valence engine optimizations"""
        suggested_values = spec['parameters'].get('suggested_values', [])
        if suggested_values:
            new_adaptation_rate = float(np.clip(suggested_values[0] / 100.0, 0.001, 0.1))
            try:
                self.console.log(f"[RecursiveArchitect] Adjusted valence adaptation rate to {new_adaptation_rate}")
            except Exception:
                pass
            return True
        return False

    def implement_navigation_enhancement(self, spec):
        """Implement energy landscape navigation improvements"""
        suggested_values = spec['parameters'].get('suggested_values', [])
        if suggested_values:
            new_temperature = float(np.clip(suggested_values[0] / 10.0, 0.1, 5.0))
            try:
                self.console.log(f"[RecursiveArchitect] Adjusted exploration temperature to {new_temperature}")
            except Exception:
                pass
            return True
        return False

    def implement_topology_adaptation(self, spec):
        """Implement topology adaptation improvements"""
        suggested_values = spec['parameters'].get('suggested_values', [])
        if suggested_values:
            new_threshold = float(np.clip(suggested_values[0] / 100.0, 0.1, 0.9))
            try:
                self.console.log(f"[RecursiveArchitect] Adjusted topology complexity threshold to {new_threshold}")
            except Exception:
                pass
            return True
        return False

    def implement_fluid_improvement(self, spec):
        """Implement fluid dynamics improvements"""
        suggested_values = spec['parameters'].get('suggested_values', [])
        if suggested_values:
            new_viscosity = float(np.clip(suggested_values[0] / 1000.0, 0.001, 1.0))
            try:
                self.console.log(f"[RecursiveArchitect] Adjusted consciousness viscosity to {new_viscosity}")
            except Exception:
                pass
            return True
        return False

class HypothesisValidator:
    def __init__(self, mind_instance: 'E8Mind'):
        self.mind = mind_instance
        self.llm_pool = mind_instance.llm_pool
        # Provide direct console reference (some internal calls referenced self.console)
        try:
            self.console = mind_instance.console
            # Get undimmed console for critical validation messages
            if hasattr(mind_instance.console, '_original_console'):
                self.undimmed_console = mind_instance.console._original_console
            else:
                self.undimmed_console = mind_instance.console
        except Exception:
            self.console = None
            self.undimmed_console = None

    async def validate_insight(self, insight_node_id: str, prompt_vars: Dict[str, Any] = None):
        insight_data = self.mind.memory.graph_db.get_node(insight_node_id)
        if not insight_data:
            self.mind.console.log(f"[Validator] Could not find insight data for node {insight_node_id}")
            return

        hypothesis_text = insight_data.get('metaphor', insight_data.get('label', ''))
        if not hypothesis_text:
            return

        # Sanitize text using semantics if available
        if hasattr(self.mind, "semantics") and hasattr(self.mind.semantics, "sanitize_for_validation"):
            safe_text = self.mind.semantics.sanitize_for_validation(hypothesis_text)
        else:
            safe_text = hypothesis_text

        # Build prompt variables if not provided
        if prompt_vars is None:
            rating = insight_data.get('rating', 0.0)
            epoch = getattr(self.mind, 'epoch', 0)
            prompt_vars = {
                "insight_text": safe_text,
                "insight_rating": f"{rating:.3f}" if rating is not None else "null",
                "node_id": insight_node_id,
                "epoch": epoch,
            }

        # Try to use the new validator prompt from prompts.yaml
        try:
            prompts = getattr(self.mind, 'prompts', {})
            if 'validator' in prompts:
                validation_result = await self._validate_with_new_prompt(safe_text, prompt_vars)
                if validation_result:
                    # Write back to node
                    await self._write_validation_to_node(insight_node_id, validation_result)
                    return
        except Exception as e:
            self.mind.console.log(f"[Validator] New prompt validation failed, falling back: {e}")

        # Fallback to old validation system
        # Structured validation object scaffold
        started = time.time()
        validation_obj: Dict[str, Any] = {
            "run_id": getattr(self.mind, 'run_id', None),
            "node_id": insight_node_id,
            "label": insight_data.get('label'),
            "hypothesis": hypothesis_text,
            "method": None,
            "confidence": None,
            "inputs": {"source_type": insight_data.get('type'), "rating": insight_data.get('rating')},
            "verdict": "unknown",
            "deltas": {},
            "next_action": None,
            "reasoning": None,
            "step": getattr(self.mind, 'step_num', None),
        }

        # Use undimmed console for key validation messages
        validation_console = self.undimmed_console if self.undimmed_console else self.mind.console
        
        # Enhanced hypothesis validation logging
        try:
            hypothesis_preview = hypothesis_text[:120] + "..." if len(hypothesis_text) > 120 else hypothesis_text
            validation_console.print(f"\n[bold magenta]🔬 VALIDATING HYPOTHESIS[/]: [cyan]{insight_data.get('label','')[:64]}[/]")
            validation_console.print(f"[dim]   Hypothesis:[/] {hypothesis_preview}")
            validation_console.print(f"[dim]   Node:[/] {insight_node_id[:12]}...")
        except Exception:
            # Fallback to original logging
            validation_console.log(f"[Validator] â‡¢ validating '{insight_data.get('label','')[:64]}' â€¦")
        # Metric-causal attention: compute a light-cone filtered local context for telemetry and potential use
        try:
            G = self.mind.memory.graph_db.graph
            neighbor_ids = []
            try:
                neighbor_ids = list(G.neighbors(insight_node_id))[:32]
            except Exception:
                neighbor_ids = []
            origin_id = insight_node_id
            shell_dim = None
            if hasattr(self.mind, 'dimensional_shells'):
                for d in DIMENSIONAL_SHELL_SIZES:
                    shell = self.mind.dimensional_shells.get(d)
                    if not shell or not getattr(shell, 'vectors', None):
                        continue
                    if origin_id in shell.vectors:
                        any_nb_here = any((nb in shell.vectors) for nb in neighbor_ids)
                        if any_nb_here:
                            shell_dim = d
                            break
            gated_neighbors = neighbor_ids
            if origin_id and shell_dim and hasattr(self.mind, 'proximity_engine') and neighbor_ids:
                gated_neighbors = self.mind.proximity_engine.filter_by_light_cone(origin_id, neighbor_ids, shell_dim)
            validation_obj["context_nodes"] = gated_neighbors[:10]
            try:
                metrics_log("validator.light_cone", {"event": "validator.cone", "origin": origin_id, "dim": shell_dim, "candidates": len(neighbor_ids), "kept": len(gated_neighbors or [])})
            except Exception:
                pass
        except Exception:
            pass
        try:
            classification = await self._classify_hypothesis(hypothesis_text)
            if not isinstance(classification, dict):
                raise ValueError("classification not dict")
        except Exception as e:
            classification = {"type": "unknown", "reasoning": f"Classifier failed: {e}"}
            self.mind.console.log(f"[Validator] Classification failure for {insight_node_id}: {e}")

        node = self.mind.memory.graph_db.get_node(insight_node_id)
        if node is not None:
            node['validation_status'] = classification

        # Populate structured fields
        validation_obj["method"] = "classify_then_plan"
        validation_obj["confidence"] = 0.65 if classification.get('type') == 'computationally_testable' else 0.50
        validation_obj["reasoning"] = classification.get('reasoning')

        # Determine verdict
        verdict = "pass" if classification.get('type') == 'computationally_testable' else "unknown"
        validation_obj["verdict"] = verdict

# --- Enhanced entanglement hook with research-based confidence threshold ---
        entangle_conf_thresh = float(os.getenv("E8_ENTANGLE_CONF_THRESH", "0.9"))  # From research (high-confidence >0.9)
        if verdict == 'pass' and validation_obj.get('confidence', 0.0) > entangle_conf_thresh:
            asyncio.create_task(self.mind._trigger_entanglement_event(insight_node_id, event_type="validation"))

        # Attempt test plan if computationally testable
        test_plan: Dict[str, Any] = {}
        if verdict == "pass":
            try:
                test_plan = await self._design_test_plan(hypothesis_text)
            except Exception as e:
                # Will be further enhanced in next todo for explicit reason capture
                test_plan = {"required_data": None, "steps": [], "error": str(e)}
                self.mind.console.log(f"[Validator] Test plan generation failed: {e}")
        if node is not None:
            if test_plan:
                node['validation_plan'] = test_plan
        validation_obj["next_action"] = (test_plan.get('steps', [None])[0] if test_plan else None)
        validation_obj["test_plan"] = test_plan

        # Perform writeback adjustments & capture deltas (simplified: only rating change)
        before_rating = node.get('rating') if node else None
        try:
            self.mind._bump_edge_weights(insight_node_id, verdict)
        except Exception as e:
            self.mind.console.log(f"[Validator] Edge weight bump failed: {e}")
        after_rating = node.get('rating') if node else None
        if before_rating is not None and after_rating is not None and after_rating != before_rating:
            validation_obj['deltas']['rating'] = after_rating - before_rating

        latency_ms = int((time.time() - started) * 1000)
        validation_obj['latency_ms'] = latency_ms

        # Emit concise console panel
        try:
            summary_line = (
                f"VERDICT={verdict} method={validation_obj['method']} conf={validation_obj['confidence']:.2f} "
                f"Î”rating={validation_obj['deltas'].get('rating',0):+.3f} latency={latency_ms}ms"
            )
            # Enhanced validation result display with colors
            verdict_colors = {"pass": "green", "fail": "red", "unknown": "yellow"}
            verdict_color = verdict_colors.get(verdict, "white")
            
            validation_console.print(f"\n[bold {verdict_color}]🎯 HYPOTHESIS VALIDATION RESULT[/]:")
            validation_console.print(f"   [bold]Verdict:[/] [{verdict_color}]{verdict.upper()}[/]")
            validation_console.print(f"   [dim]Confidence:[/] {validation_obj['confidence']:.2f}")
            validation_console.print(f"   [dim]Method:[/] {validation_obj['method']}")
            
            if validation_obj['deltas'].get('rating'):
                rating_change = validation_obj['deltas']['rating']
                rating_color = "green" if rating_change > 0 else "red" if rating_change < 0 else "white"
                validation_console.print(f"   [dim]Rating Change:[/] [{rating_color}]{rating_change:+.3f}[/]")
            
            validation_console.print(f"   [dim]Processing Time:[/] {latency_ms}ms")
            
            if test_plan and verdict == 'pass':
                validation_console.print(f"   [dim]Next Steps:[/] {', '.join(test_plan.get('steps', [])[:3])}")
            
            # Also keep original summary for logs
            validation_console.log(f"[Validator] {summary_line}")
        except Exception:
            pass

        # Write structured JSON line to insights file
        try:
            insights_path = os.getenv("E8_INSIGHTS_PATH", "insights.ndjson")
            with open(insights_path, 'a', encoding='utf-8') as fobj:
                fobj.write(json.dumps({**validation_obj, "event": "validator.outcome"}, ensure_ascii=False) + "\n")
        except Exception:
            pass

        # Metrics counters
        try:
            outcome_key = f"validation_outcome_{verdict}"
            metrics_log("validator.outcome", {"event": "validator", "run_id": validation_obj['run_id'], "step": validation_obj['step'], "node_id": insight_node_id, "verdict": verdict, "latency_ms": latency_ms, outcome_key: 1})
        except Exception:
            pass

        # Legacy panels (suppressed heavy formatting): keep minimal when pass
        if verdict == 'pass' and test_plan:
            try:
                plan_text = " | ".join(step for step in test_plan.get('steps', [])[:5])
                validation_console.log(f"[ValidatorPlan] data={test_plan.get('required_data')} steps={plan_text}")
            except Exception:
                pass

    async def _classify_hypothesis(self, hypothesis: str) -> Dict[str, Any]:
        """
        Return one of:
          {"type": "computationally_testable" | "physically_testable", "reasoning": str}
        Never returns 'unknown'.
        """
        import re

        def _balanced_object(s: str) -> dict:
            # Defensive: LLM response might be None or non-string
            if not isinstance(s, str) or '{' not in s:
                return {}
            start = s.find('{')
            if start == -1:
                return {}
            depth = 0
            for i in range(start, len(s)):
                ch = s[i]
                if ch == '{':
                    depth += 1
                elif ch == '}':
                    depth -= 1
                    if depth == 0:
                        frag = s[start:i+1]
                        try:
                            obj = _parse_json_object(frag)
                            return obj if isinstance(obj, dict) else {}
                        except Exception:
                            return {}
            return {}

        def _extract_json(s: str) -> dict:
            if not isinstance(s, str) or not s:
                return {}
            # Attempt direct parse first
            try:
                whole = _parse_json_object(s)
                if isinstance(whole, dict) and whole:
                    return whole
            except Exception:
                pass
            # Fallback: balanced brace scan
            d = _balanced_object(s)
            if d:
                return d
            # Final fallback: last resort regex (short, non-greedy)
            try:
                m = re.search(r"\{[^{}]*\}", s)
                if m:
                    frag = m.group(0)
                    obj = _parse_json_object(frag)
                    if isinstance(obj, dict):
                        return obj
            except Exception:
                pass
            # Log failure (non-fatal)
            try:
                self.mind.console.log("[VALIDATOR] JSON extraction failed; using deterministic defaults.")
            except Exception:
                pass
            return {}

        def _normalize_type(t: str, raw: str) -> str:
            t = (t or "").strip().lower()
            # map common synonyms / typos â†’ canonical
            if re.search(r"comput", t) or re.search(r"\b(simulate|model|algorithm|code|program|mining|embedding)\b", raw, re.I):
                return "computationally_testable"
            if re.search(r"physic", t) or re.search(r"\b(experiment|sensor|measure|lab|hardware|device)\b", raw, re.I):
                return "physically_testable"
            # default bias: if nothing explicit, prefer computational
            return "computationally_testable"

        base_prompt = (
            "You are a precise classifier.\n"
            "Goal: classify the hypothesis as either computationally_testable or physically_testable.\n"
            "Definitions:\n"
            "  - computationally_testable: can be tested fully in-silico (algorithms, simulations, embeddings, code).\n"
            "  - physically_testable: needs real-world measurement, sensors, lab hardware, or experiments.\n\n"
            "Hypothesis:\n"
            f"{hypothesis}\n\n"
            "Respond with ONLY this JSON object (no prose, no code fences):\n"
            '{{"type":"computationally_testable|physically_testable","reasoning":"<brief why>"}}'
        )

        raw = await self.llm_pool.enqueue_and_wait(base_prompt, max_tokens=96, temperature=0.0)
        if not isinstance(raw, str):  # Normalize None / unexpected types
            raw = str(raw) if raw is not None else ""
        data = _extract_json(raw)

        # Retry once with stricter instruction if needed
        if not isinstance(data, dict) or "type" not in data:
            strict_prompt = base_prompt + "\nSTRICT: Output only the JSON object, nothing else."
            raw2 = await self.llm_pool.enqueue_and_wait(strict_prompt, max_tokens=96, temperature=0.0)
            if not isinstance(raw2, str):
                raw2 = str(raw2) if raw2 is not None else ""
            data = _extract_json(raw2) or {}

        # Normalize + finalize (never return 'unknown')
        ctype = _normalize_type(data.get("type"), raw if isinstance(raw, str) else "")
        reasoning = (data.get("reasoning") or "").strip()
        if not reasoning:
            # lightweight deterministic reasoning if LLM omitted it
            if ctype == "computationally_testable":
                reasoning = "Mentions compute/simulation/algorithmic evaluation or lacks physical measurement cues."
            else:
                reasoning = "Requires real-world measurement, sensors, or lab conditions."

        # Pretty console panel
        try:
            async with self.mind.console_lock:
                try:
                    self.console.print(
                        Panel(
                            f"[bold]Classification:[/bold] {ctype}\n"
                            f"[bold]Reasoning:[/bold] {escape(reasoning)}",
                            title="[bold yellow]VALIDATOR: CLASSIFICATION[/]",
                            border_style="red",
                        )
                    )
                except Exception as e:
                    # Log the specific error to help debug Panel issues
                    self.console.log(f"[VALIDATOR] Rich Panel formatting failed: {e}")
                    # Enhanced fallback that maintains some visual structure
                    self.console.print("â”Œâ”€ âœ… VALIDATOR: CLASSIFICATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
                    self.console.print(f"â”‚ Classification: {ctype:<43} â”‚")
                    self.console.print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
                    # Break reasoning into multiple lines if needed
                    reasoning_lines = [reasoning[i:i+53] for i in range(0, len(reasoning), 53)]
                    for line in reasoning_lines:
                        self.console.print(f"â”‚ {line:<55} â”‚")
                    self.console.print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        except Exception as e:
            # If console lock fails, at least show something
            self.console.log(f"[VALIDATOR] Console lock failed: {e}")
            self.console.print(f"VALIDATOR: {ctype} - {reasoning}")

        return {"type": ctype, "reasoning": reasoning}

    async def _design_test_plan(self, hypothesis: str) -> Dict[str, Any]:
        prompt = (
            "You are a principal investigator designing an experiment. For the following computationally testable "
            "hypothesis, create a validation plan.\n\n"
            f"Hypothesis: \"{hypothesis}\"\n\n"
            "Respond in JSON format with two keys: 'required_data' (a brief description of the datasets needed, e.g., 'Historical S&P 500 price data and news sentiment scores') "
            "and 'steps' (an array of strings outlining the high-level steps for the analysis, e.g., ['Clean and align datasets by date', 'Perform time-series cross-correlation analysis', 'Check for statistical significance'])."
        )
        # Guard: empty or null-like hypothesis
        if not hypothesis or not hypothesis.strip():
            return {"required_data": None, "steps": [], "reason": "missing_hypothesis"}
        try:
            response = await asyncio.wait_for(self.llm_pool.enqueue_and_wait(prompt, max_tokens=400), timeout=HYPOTHESIS_TIMEOUT)
            parsed = _parse_json_object(response)
            if not isinstance(parsed, dict):
                raise ValueError("non-dict response")
            # Minimal validation of keys
            if 'steps' not in parsed or not isinstance(parsed.get('steps'), (list, tuple)):
                parsed['steps'] = []
            return parsed
        except asyncio.TimeoutError:
            self.mind.console.log("[Validator] Test plan timeout")
            return {"required_data": None, "steps": [], "reason": "timeout"}
        except Exception as e:
            self.mind.console.log(f"[Validator] Test plan design failed: {e}")
            return {"required_data": None, "steps": [], "reason": f"error:{type(e).__name__}"}

    async def _validate_with_new_prompt(self, insight_text: str, prompt_vars: Dict[str, Any]) -> Dict[str, Any]:
        """Use the new validator prompt from prompts.yaml to validate an insight."""
        try:
            prompts = getattr(self.mind, 'prompts', {})
            # Handle both dictionary-like objects and YamlPromptPack objects
            if hasattr(prompts, '_d') and 'validator' in prompts._d:
                validator_prompts = prompts._d['validator']
            elif hasattr(prompts, 'get'):
                validator_prompts = prompts.get('validator', {})
            else:
                validator_prompts = {}
            
            if not validator_prompts:
                raise ValueError("Validator prompts not found")
            
            sys_prompt = validator_prompts.get('system', '')
            user_prompt = validator_prompts.get('user', '').format(**prompt_vars)
            
            # Add validator persona if available
            if hasattr(self.mind, 'semantics') and hasattr(self.mind.semantics, 'validator_persona'):
                persona = self.mind.semantics.validator_persona()
                if persona:
                    sys_prompt = persona + "\n\n" + sys_prompt
            
            # Combine system and user prompts
            full_prompt = sys_prompt + "\n\n" + user_prompt
            
            # Get LLM response
            response = await asyncio.wait_for(
                self.llm_pool.enqueue_and_wait(full_prompt, max_tokens=200, temperature=0.0),
                timeout=HYPOTHESIS_TIMEOUT
            )
            
            # Parse JSON response
            if not isinstance(response, str):
                response = str(response) if response is not None else ""
            
            # Clean up response - remove code fences if present
            response = response.strip()
            if response.startswith('```json'):
                response = response[7:]
            if response.startswith('```'):
                response = response[3:]
            if response.endswith('```'):
                response = response[:-3]
            response = response.strip()
            
            try:
                result = _parse_json_object(response)
                if not isinstance(result, dict):
                    raise ValueError("Response is not a valid JSON object")
                
                # Validate required fields
                required_fields = ["validation_status", "validation_score", "validator_notes", "evidence_refs", "checked_at"]
                for field in required_fields:
                    if field not in result:
                        if field == "validation_status":
                            result[field] = "inconclusive"
                        elif field == "validation_score":
                            result[field] = 0.0
                        elif field == "validator_notes":
                            result[field] = "incomplete response"
                        elif field == "evidence_refs":
                            result[field] = []
                        elif field == "checked_at":
                            from datetime import datetime, timezone
                            result[field] = datetime.now(timezone.utc).isoformat()
                
                # Validate field types and values
                if result["validation_status"] not in ["pass", "fail", "inconclusive"]:
                    result["validation_status"] = "inconclusive"
                
                try:
                    result["validation_score"] = float(result["validation_score"])
                    if not (0.0 <= result["validation_score"] <= 1.0):
                        result["validation_score"] = 0.0
                except (ValueError, TypeError):
                    result["validation_score"] = 0.0
                
                # Truncate notes to 280 chars
                result["validator_notes"] = str(result["validator_notes"])[:280]
                
                # Ensure evidence_refs is a list
                if not isinstance(result["evidence_refs"], list):
                    result["evidence_refs"] = []
                result["evidence_refs"] = result["evidence_refs"][:5]  # Max 5 items
                
                return result
                
            except Exception as parse_error:
                self.mind.console.log(f"[Validator] JSON parse failed: {parse_error}")
                # Return fallback inconclusive result
                from datetime import datetime, timezone
                return {
                    "validation_status": "inconclusive",
                    "validation_score": 0.0,
                    "validator_notes": "invalid JSON from model",
                    "evidence_refs": [],
                    "checked_at": datetime.now(timezone.utc).isoformat()
                }
                
        except Exception as e:
            self.mind.console.log(f"[Validator] New prompt validation failed: {e}")
            return None

    async def _write_validation_to_node(self, node_id: str, validation_result: Dict[str, Any]):
        """Write validation results back to the node."""
        try:
            node = self.mind.memory.graph_db.get_node(node_id)
            if node is not None:
                # Write the expected fields that the UI looks for
                node["validation_status"] = validation_result["validation_status"]
                node["validation_score"] = float(validation_result["validation_score"])
                node["validator_notes"] = validation_result["validator_notes"][:280]
                node["validator_refs"] = validation_result["evidence_refs"][:5]
                node["validated_at"] = validation_result["checked_at"]
                
                # Log the validation
                self.mind.console.log(
                    f"[Validator] {node_id}: status={validation_result['validation_status']} "
                    f"score={validation_result['validation_score']:.3f} "
                    f"notes={validation_result['validator_notes'][:50]}{'...' if len(validation_result['validator_notes']) > 50 else ''}"
                )
                
                # Emit metrics
                try:
                    metrics_log("validator.new_format", {
                        "event": "validator.new_format",
                        "node_id": node_id,
                        "status": validation_result["validation_status"],
                        "score": validation_result["validation_score"]
                    })
                except Exception:
                    pass
        except Exception as e:
            self.mind.console.log(f"[Validator] Failed to write validation to node {node_id}: {e}")

class DataIngestionPipeline:
    """Continuously ingest external data and convert to memory concepts."""
    def __init__(self, mind_instance: 'E8Mind'):
        self.mind = mind_instance
        self.console = mind_instance.console
        self.sources = {}
        self.state = {}
        self._task: Optional[asyncio.Task] = None
        self.running = False
        self.state_file = get_path("ingestion_state.json", self.mind.run_id)
        self._recent_ids = {}

    def add_source(self, name: str, config: Dict[str, Any]):
        self.sources[name] = config
        self.console.log(f"[Ingestion] Added data source: '{name}' (type: {config.get('type')})")

    async def start(self):
        if self.running or aiohttp is None:
            return
        self.running = True
        self.state = safe_json_read(self.state_file, default={})
        self._task = asyncio.create_task(self._run())
        self.console.log("[Ingestion] Pipeline started.")

    def stop(self):
        self.running = False
        if self._task:
            self._task.cancel()
        safe_json_write(self.state_file, self.state)
        self.console.log("[Ingestion] Pipeline stopped.")

    async def _run(self):
        while self.running:
            now = time.monotonic()
            for name, config in self.sources.items():
                last_checked = self.state.get(name, {}).get("last_checked_monotonic", 0)
                interval_seconds = config.get("schedule_minutes", 60) * 60
                if now - last_checked > interval_seconds:
                    try:
                        await self._process_source(name, config)
                    except Exception as e:
                        console.log(f"[bold red][Ingestion] Error processing source '{name}': {e}[/bold red]")
                    finally:
                        if name not in self.state: self.state[name] = {}
                        self.state[name]["last_checked_monotonic"] = now
            await asyncio.sleep(60)

    async def _process_source(self, name: str, config: Dict[str, Any]):
        source_type = config.get("type")
        self.console.log(f"[Ingestion] Checking source: '{name}'")
        if source_type == "arxiv_api":
            await self._process_arxiv(name, config)
        elif source_type == "file":
            await self._process_file(name, config)
        elif source_type == "research_ingest":
            await self._process_research_ingest(name, config)
        elif source_type == "pubmed_api":
            await self._process_pubmed_api(name, config)
        elif source_type == "json_api":
            await self._process_json_api(name, config)
        else:
            self.console.log(f"[yellow][Ingestion] Unknown source type '{source_type}' for '{name}'[/yellow]")

    async def _process_arxiv(self, name: str, config: Dict[str, Any]):
        # Load persisted state
        name_state = self.state.get(name, {})
        last_processed_id = name_state.get("last_processed_id", None)
        seen_ids = set(name_state.get('seen_ids', []))
        # Track limited rolling hash set for stronger dedup (title+summary hash)
        seen_hashes = set(name_state.get('seen_hashes', []))
        new_entries_to_add: List[tuple[str,str]] = []
        processed_ids_this_session: set[str] = set()
        if name not in self._recent_ids:
            from collections import deque as _deque
            self._recent_ids[name] = _deque(maxlen=4096)
        async with aiohttp.ClientSession() as session:
            async with session.get(config["url"]) as response:
                if response.status != 200:
                    self.console.log(f"[bold red][Ingestion] arXiv fetch failed for '{name}': HTTP {response.status}[/bold red]")
                    return
                feed_xml = await response.text()

        root = ET.fromstring(feed_xml)
        ns = {'atom': 'http://www.w3.org/2005/Atom'}
        
        # arXiv feeds are typically newest-first; we stop at last_processed_id to avoid re-walking entire history.
        for entry in root.findall('atom:entry', ns):
            try:
                entry_id = entry.find('atom:id', ns).text.strip()
            except Exception:
                continue

            if entry_id == last_processed_id:
                break  # we've reached already ingested boundary
            if entry_id in seen_ids or entry_id in set(self._recent_ids.get(name, [])):
                continue  # fast skip

            title_el = entry.find('atom:title', ns)
            summary_el = entry.find('atom:summary', ns)
            if title_el is None or summary_el is None:
                continue
            title = (title_el.text or '').strip()
            summary = (summary_el.text or '').strip().replace('\n', ' ')
            content_text = f"{title}: {summary}".strip()
            h = hashlib.sha1(content_text.encode('utf-8', 'ignore')).hexdigest()[:20]
            if h in seen_hashes:
                continue
            new_entries_to_add.append((entry_id, content_text))
            processed_ids_this_session.add(entry_id)
            seen_hashes.add(h)

        if new_entries_to_add:
            new_entries_to_add.reverse()
            
            for _, text in new_entries_to_add:
                await self._add_text_as_concept(text, source_name=name)

            latest_id = new_entries_to_add[-1][0]
            if name not in self.state:
                self.state[name] = {}
            # Update rolling recent ids and seen sets
            try:
                recent_deque = self._recent_ids.get(name)
                for eid in processed_ids_this_session:
                    recent_deque.append(eid)
            except Exception:
                pass
            # Persist trimmed sets to bound state size
            seen_ids.update(processed_ids_this_session)
            max_seen = int(os.getenv('E8_INGEST_SEEN_MAX', '20000'))
            if len(seen_ids) > max_seen:
                # drop oldest by converting to list; order is not preserved so just slice arbitrary after list conversion
                seen_ids = set(list(seen_ids)[-max_seen:])
            max_hashes = int(os.getenv('E8_INGEST_HASH_MAX', '40000'))
            if len(seen_hashes) > max_hashes:
                seen_hashes = set(list(seen_hashes)[-max_hashes:])
            self.state[name]["last_processed_id"] = latest_id
            self.state[name]['seen_ids'] = list(seen_ids)
            self.state[name]['seen_hashes'] = list(seen_hashes)
            safe_json_write(self.state_file, self.state)
            self.console.log(f"[Ingestion] Added {len(new_entries_to_add)} new concepts from '{name}'. (latest_id={latest_id})")
        else:
            # Still persist any growth in seen sets even if no new entries passed filters
            if processed_ids_this_session:
                self.state.setdefault(name, {})
                self.state[name]['seen_ids'] = list(seen_ids)
                self.state[name]['seen_hashes'] = list(seen_hashes)
                safe_json_write(self.state_file, self.state)

    async def _process_file(self, name: str, config: Dict[str, Any]):
        filepath = config.get("path")
        if not os.path.exists(filepath):
            return

        last_mod_time = self.state.get(name, {}).get("last_mod_time", 0)
        current_mod_time = os.path.getmtime(filepath)

        if current_mod_time > last_mod_time:
            self.console.log(f"[Ingestion] File '{filepath}' has been updated. Processing.")
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()

            chunks = [p.strip() for p in content.split('\n\n') if p.strip()]
            for chunk in chunks:
                await self._add_text_as_concept(chunk, source_name=name)

            if name not in self.state:
                self.state[name] = {}
            self.state[name]["last_mod_time"] = current_mod_time
            safe_json_write(self.state_file, self.state)
            self.console.log(f"[Ingestion] Added {len(chunks)} new concepts from file '{name}'.")

    async def _add_text_as_concept(self, text: str, source_name: str):
        if not text: return
        rating = await self.mind.rate_concept(text)
        entry = {
            "type": "external_concept",
            "label": sanitize_line(text, 40),
            "metaphor": sanitize_block(text, 5, 500),
            "rating": rating,
            "step": self.mind.step_num,
            "source": source_name,
        }
        
        # Use add_entry which has built-in EHS lock management
        # This avoids double-locking since add_entry already handles EHS writer locks
        await self.mind.memory.add_entry(entry)

    async def _process_research_ingest(self, name: str, config: Dict[str, Any]):
        max_total = config.get("max_total", 80)
        items = await asyncio.to_thread(gather_ingest, max_total=max_total)
        for it in items:
            text = f"{it['title']}\n\n{it['snippet']}\n\n{it['url']}"
            await self._add_text_as_concept(text, source_name=it['source'])
        self.console.log(f"[Ingestion] Added {len(items)} research items from '{name}'.")

    # New API handlers added to support pubmed_api and json_api source types
    async def _process_pubmed_api(self, name: str, config: Dict[str, Any]):
        # Placeholder implementation for PubMed API
        # This would fetch from PubMed API and process results
        url = config.get("url")
        if not url:
            self.console.log(f"[yellow][Ingestion] No URL provided for PubMed source '{name}'[/yellow]")
            return
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    if response.status != 200:
                        self.console.log(f"[bold red][Ingestion] PubMed API fetch failed for '{name}': HTTP {response.status}[/bold red]")
                        return
                    data = await response.json()
                    
            # Process the JSON data (assuming PubMed API format)
            if isinstance(data, dict) and 'esearchresult' in data:
                # PubMed esearch result
                ids = data['esearchresult'].get('idlist', [])
                for paper_id in ids[:10]:  # Limit to 10 for now
                    text = f"PubMed Paper ID: {paper_id}"
                    await self._add_text_as_concept(text, source_name=name)
                self.console.log(f"[Ingestion] Added {len(ids[:10])} PubMed papers from '{name}'.")
            else:
                self.console.log(f"[yellow][Ingestion] Unexpected PubMed API response format for '{name}'[/yellow]")
        except Exception as e:
            self.console.log(f"[bold red][Ingestion] Error processing PubMed API for '{name}': {e}[/bold red]")

    async def _process_json_api(self, name: str, config: Dict[str, Any]):
        # Generic JSON API handler
        url = config.get("url")
        if not url:
            self.console.log(f"[yellow][Ingestion] No URL provided for JSON API source '{name}'[/yellow]")
            return
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    if response.status != 200:
                        self.console.log(f"[bold red][Ingestion] JSON API fetch failed for '{name}': HTTP {response.status}[/bold red]")
                        return
                    data = await response.json()
            
            # Handle structured API responses (OpenAlex, Crossref, etc.)
            items = []
            if isinstance(data, dict):
                # OpenAlex format: {"results": [...], "meta": {...}}
                if "results" in data and isinstance(data["results"], list):
                    items = data["results"]
                # Crossref format: {"message": {"items": [...]}}
                elif "message" in data and isinstance(data.get("message"), dict):
                    message = data["message"]
                    if "items" in message and isinstance(message["items"], list):
                        items = message["items"]
                # Generic list response
                elif isinstance(data, list):
                    items = data
            elif isinstance(data, list):
                items = data
            
            if items:
                processed_count = 0
                for item in items[:20]:  # Limit to 20 items
                    if isinstance(item, dict):
                        # Extract title from various possible fields
                        title_raw = (
                            item.get('title') or 
                            item.get('display_name') or 
                            item.get('name') or 
                            'Unknown Title'
                        )
                        # Ensure title is a string (handle case where it might be a list)
                        if isinstance(title_raw, list):
                            title = ' '.join(str(t) for t in title_raw if t)
                        else:
                            title = str(title_raw) if title_raw else 'Unknown Title'
                        
                        # Extract abstract/description from various fields
                        abstract_raw = (
                            item.get('abstract') or
                            item.get('snippet') or 
                            item.get('description') or
                            ''
                        )
                        # Ensure abstract is a string
                        if isinstance(abstract_raw, list):
                            abstract = ' '.join(str(a) for a in abstract_raw if a)
                        else:
                            abstract = str(abstract_raw) if abstract_raw else ''
                        
                        # For OpenAlex, reconstruct abstract from inverted index
                        if not abstract and 'abstract_inverted_index' in item:
                            try:
                                inverted = item['abstract_inverted_index']
                                if isinstance(inverted, dict):
                                    # Reconstruct abstract from inverted index
                                    word_positions = []
                                    for word, positions in inverted.items():
                                        if isinstance(positions, (list, str)):
                                            pos_list = positions if isinstance(positions, list) else [int(p) for p in positions.split()]
                                            for pos in pos_list:
                                                word_positions.append((int(pos), word))
                                    if word_positions:
                                        word_positions.sort()
                                        abstract = ' '.join([word for _, word in word_positions[:50]])  # Limit to first 50 words
                            except Exception:
                                pass  # Skip if reconstruction fails
                        
                        # Extract URL from various fields
                        url_raw = (
                            item.get('url') or
                            item.get('link') or
                            item.get('DOI') or
                            item.get('doi') or
                            (item.get('primary_location') or {}).get('landing_page_url') or
                            ''
                        )
                        # Ensure URL is a string
                        if isinstance(url_raw, list):
                            url = str(url_raw[0]) if url_raw else ''
                        else:
                            url = str(url_raw) if url_raw else ''
                        
                        # Format the content - ensure all parts are strings
                        text_parts = [str(title)]
                        if abstract:
                            text_parts.append(str(abstract))
                        if url:
                            text_parts.append(f"URL: {str(url)}")
                        
                        text = '\n\n'.join(text_parts)
                        await self._add_text_as_concept(text, source_name=name)
                        processed_count += 1
                
                self.console.log(f"[Ingestion] Added {processed_count} items from JSON API '{name}'.")
            else:
                self.console.log(f"[yellow][Ingestion] No processable items found in JSON API response for '{name}' (response type: {type(data)})[/yellow]")
        except Exception as e:
            self.console.log(f"[bold red][Ingestion] Error processing JSON API for '{name}': {e}[/bold red]")

def restricted_basis(weights, hops, N, hops_limit):
    neighbors = np.where(hops <= hops_limit)[0]
    if len(neighbors) > N: return neighbors[np.argsort(-weights[neighbors])[:N]]
    return neighbors

def build_local_L_norm(W_local):
    if W_local.shape[0] == 0: return np.array([[]], dtype=np.float32)
    deg = np.sum(W_local, axis=1)
    D_inv_sqrt = np.diag(1.0 / np.sqrt(np.maximum(deg, 1e-9)))
    return np.eye(W_local.shape[0]) - D_inv_sqrt @ W_local @ D_inv_sqrt

def build_joint_H(L1, V1, L2, V2, J, gamma):
    H1 = gamma * L1 + np.diag(V1)
    H2 = gamma * L2 + np.diag(V2)
    H_joint = np.kron(H1, np.eye(len(L2))) + np.kron(np.eye(len(L1)), H2)
    H_joint += J * np.eye(len(H_joint))
    return H_joint

def sample_from_2d(P_2d):
    P_flat = P_2d.flatten()
    if P_flat.sum() < 1e-9:
        return np.unravel_index(0, P_2d.shape)
    P_flat /= np.sum(P_flat)
    sample_index = np.random.choice(len(P_flat), p=P_flat)
    return np.unravel_index(sample_index, P_2d.shape)

class MetaTelemetryLogger:
    def __init__(self, mind_instance: 'E8Mind', run_id: str):
        self.mind = mind_instance
        self.diary_file = get_path("mind_diary.md", run_id)
        self.shell_tension_history = deque(maxlen=100)
        self.bh_event_steps = deque(maxlen=20)
        self.mood_history = deque(maxlen=100)

    def log_step(self):
        all_tensions = [d.get('shell_tension', 0.0) for _, d in self.mind.memory.graph_db.graph.nodes(data=True) if d.get('shell_tension') is not None]
        avg_tension = np.mean(all_tensions) if all_tensions else 0.0
        self.shell_tension_history.append(avg_tension)
        self.mood_history.append(list(self.mind.mood.mood_vector.values()))

    def log_bh_event(self, step_num: int):
        self.bh_event_steps.append(step_num)

    async def generate_diary_entry(self):
        if len(self.mood_history) < self.mood_history.maxlen: return
        avg_tension = np.mean(self.shell_tension_history)
        bh_frequency = len(self.bh_event_steps) / len(self.shell_tension_history)
        mood_variance = np.mean(np.var(np.array(list(self.mood_history)), axis=0))
        metrics_summary = (f"- Average Cognitive Tension: {avg_tension:.4f}\n"
                           f"- Black Hole Event Frequency: {bh_frequency:.3f} events/step\n"
                           f"- Mood Stability (lower is more stable): {mood_variance:.4f}")
        prompt = ("You are a mind reflecting on your own internal state. Based on the following metrics from the last 100 steps, "
                  "write a short, metaphorical, first-person diary entry. Do not list the metrics; interpret their meaning.\n\n"
                  f"Internal State Metrics:\n{metrics_summary}\n\nDiary Entry:")
        try:
            entry = await asyncio.wait_for(self.mind.llm_pool.enqueue_and_wait(prompt, max_tokens=200, temperature=0.75), timeout=DIARY_ENTRY_TIMEOUT)
            if entry and not entry.startswith("[LLM"):
                with open(self.diary_file, "a", encoding="utf-8") as f:
                    f.write(f"## Step {self.mind.step_num}\n\n{entry}\n\n---\n\n")
                self.mind.console.print(Panel(entry, title="[bold #FFD700]Mind's Diary[/]", border_style="#FFD700"))
        except Exception as e:
            self.mind.console.log(f"[Diary] Failed to generate entry: {e}")

class TopologyMonitor:
    def __init__(self, eps=0.35):
        self.eps = float(eps)
        self.prev_betti = {}

    def _betti(self, X):
        try:
            import numpy as np
            from scipy.spatial.distance import pdist, squareform
        except Exception:
            return (0,0)
        if X is None or len(X)==0:
            return (0,0)
        D = squareform(pdist(X, 'euclidean'))
        A = (D <= self.eps).astype(np.int32)
        np.fill_diagonal(A, 0)
        V = A.shape[0]; E = int(A.sum()//2)
        parent = list(range(V))
        def find(a):
            while parent[a]!=a:
                parent[a]=parent[parent[a]]; a=parent[a]
            return a
        def union(a,b):
            ra,rb=find(a),find(b)
            if ra!=rb: parent[rb]=ra
        for i in range(V):
            for j in range(i+1,V):
                if A[i,j]: union(i,j)
        C = len({find(i) for i in range(V)})
        beta0 = C; beta1 = max(0, E - V + C)
        return (beta0, beta1)

    def delta_betti(self, shell):
        try:
            X, _ = shell.get_all_vectors_as_matrix()
        except Exception:
            return 0.0
        b0,b1 = self._betti(X)
        prev = self.prev_betti.get(shell.dim, (b0,b1))
        self.prev_betti[shell.dim] = (b0,b1)
        return float(abs(b0 - prev[0]) + abs(b1 - prev[1]))

class CognitiveScheduler:
    def __init__(self, mind_instance: 'E8Mind'):
        self.mind = mind_instance
        self.GEODESIC_SYNTHESIS_INTERVAL = int(os.getenv("E8_GEODESIC_INTERVAL", "31"))
        self.GEODESIC_OFFSET = int(os.getenv("E8_GEODESIC_OFFSET", "17"))
        self.PROXIMITY_ALERT_INTERVAL = int(os.getenv("E8_PROXIMITY_INTERVAL", "7"))  # Reduced from 11 to 7 for more frequent alerts
        self.INSIGHT_SYNTHESIS_INTERVAL = 23
        self.DREAM_INTERVAL = 5
        self.NARRATIVE_SUMMARY_INTERVAL = 37
        self.SNAPSHOT_INTERVAL = 10000
        self.DECAY_INTERVAL = 24
        self.DREAM_REPLAY_INTERVAL = int(os.getenv('E8_DREAM_EVERY_STEPS','200'))
        self.PROXIMITY_MIN_GAP = int(os.getenv('E8_PROXIMITY_MIN_GAP', '4'))
        self.ENABLE_EXTRA_PROXIMITY = os.getenv('E8_PROXIMITY_EXTRA', '1') != '0'
        self.EXTRA_PROXIMITY_LOG_VERBOSE = os.getenv('E8_PROXIMITY_EXTRA_LOG', '0') == '1'
    # Track consecutive dream skips to allow a forced dream after N skips
        self._dream_skip_streak = 0
        self.TEACHER_ASK_EVERY = TEACHER_ASK_EVERY
        self.TEACHER_OFFSET = TEACHER_OFFSET
        self.EXPLORER_OFFSET = EXPLORER_OFFSET

        # Apply cadence scaling/profile to scheduler intervals
        if E8_CADENCE_PROFILE == "m20":
            try:
                self.PROXIMITY_ALERT_INTERVAL = int(os.getenv("E8_PROXIMITY_ALERT_INTERVAL", "7"))
            except Exception:
                pass
        self.PROXIMITY_ALERT_INTERVAL = _scale_steps(self.PROXIMITY_ALERT_INTERVAL)
        self.INSIGHT_SYNTHESIS_INTERVAL = _scale_steps(self.INSIGHT_SYNTHESIS_INTERVAL)
        self.DREAM_INTERVAL = _scale_steps(self.DREAM_INTERVAL)
        self.NARRATIVE_SUMMARY_INTERVAL = _scale_steps(self.NARRATIVE_SUMMARY_INTERVAL)
        self.SNAPSHOT_INTERVAL = _scale_steps(self.SNAPSHOT_INTERVAL)
        self.DECAY_INTERVAL = _scale_steps(self.DECAY_INTERVAL)
        self.DREAM_REPLAY_INTERVAL = _scale_steps(self.DREAM_REPLAY_INTERVAL)
        self.PROXIMITY_MIN_GAP = _scale_steps(self.PROXIMITY_MIN_GAP)

        # Scheduler diagnostic logging interval (reduce spam). Default 72 steps
        try:
            # Default increased to 720 steps to reduce console spam; can be overridden with env var
            self.SCHEDULER_LOG_INTERVAL = int(os.getenv('E8_SCHEDULER_LOG_INTERVAL', '720'))
        except Exception:
            self.SCHEDULER_LOG_INTERVAL = 720

        # Track last scheduler diagnostic emission to avoid spamming
        self._last_scheduler_log_step = -1

        # Cadence banner
        try:
            self.mind.console.log(
                f"[CADENCE] profile={E8_CADENCE_PROFILE} scale={E8_CADENCE_SCALE} "
                f"proximity_every={self.PROXIMITY_ALERT_INTERVAL} "
                f"dream_every={self.DREAM_INTERVAL} "
                f"bh_cooldown={BLACK_HOLE_COOLDOWN_STEPS} "
                f"teacher_every={TEACHER_ASK_EVERY}"
            )
        except Exception:
            pass

    def _fire(self, step: int, interval: int, offset: int) -> bool:
        return interval > 0 and step >= offset and ((step - offset) % interval == 0)

    
    def _should_fire(self, key: str, step: int) -> bool:
        """Prevent duplicate scheduling within the same step."""
        try:
            last = self.mind._fired_step.get(key, -1)
        except Exception:
            if not hasattr(self.mind, "_fired_step"):
                self.mind._fired_step = {}
            last = self.mind._fired_step.get(key, -1)
        if last == step:
            return False
        self.mind._fired_step[key] = step
        return True
    def tick(self, step: int):
        try:
            qdepth = self.mind.llm_pool.queue.qsize() if getattr(self.mind, 'llm_pool', None) else 0
        except Exception:
            qdepth = 0
        entropy = float(self.mind.mood.mood_vector.get('entropy', 0.0)) if getattr(self.mind, 'mood', None) else 0.0
        max_q = int(os.getenv('E8_LLM_MAX_QUEUE', '64'))
        # Slightly loosen dream gating entropy threshold (raise default to allow more dreams)
        high_entropy = float(os.getenv('E8_HIGH_ENTROPY', '0.90'))
        skip_dream = qdepth > max_q or entropy > high_entropy
        dream_forced = False
        # Configuration: force a dream after N consecutive skips (safe fallback)
        try:
            DREAM_FORCE_AFTER = int(os.getenv('E8_DREAM_FORCE_AFTER_SKIPS', '3'))
        except Exception:
            DREAM_FORCE_AFTER = 3
        try:
            self.mind.metrics.gauge('llm.queue_depth', qdepth)
            self.mind.metrics.gauge('mood.entropy', entropy)
            if skip_dream:
                self.mind.metrics.increment('dream.skipped')
        except Exception:
            pass
        
        # Diagnostic scheduling logging to help debug missing Teacher/Explorer runs
        try:
            scheduled_teacher = False
            scheduled_explorer = False
            if self.mind.teacher_question is None and self._fire(step, self.TEACHER_ASK_EVERY, self.TEACHER_OFFSET):
                if not getattr(self.mind, 'teacher_disabled', False):
                    self._should_fire('teacher', step) and self.mind.slots.teacher.start(self.mind._teacher_ask_new_question())
                    scheduled_teacher = True

            elif self.mind.teacher_question is not None and self._fire(step, self.TEACHER_ASK_EVERY, self.EXPLORER_OFFSET):
                # If gated by queue or entropy, increment skip streak; optionally force dream after threshold
                if not skip_dream:
                    # normal path: run explorer and reset skip streak
                    self._should_fire('explorer', step) and self.mind.slots.explorer.start(self.mind._explorer_answer_pending_question())
                    scheduled_explorer = True
                    self._dream_skip_streak = 0
                else:
                    # gated path: increment skip streak and possibly force a dream
                    try:
                        self._dream_skip_streak = getattr(self, '_dream_skip_streak', 0) + 1
                    except Exception:
                        self._dream_skip_streak = 1

                    # Emit metric and log
                    try:
                        self.mind.metrics.increment('dream.skipped')
                    except Exception:
                        pass

                    try:
                        if E8_LOG_SCHEDULER:
                            self.mind.console.log(f"[SCHEDULER] Dream skipped (step={step}) streak={self._dream_skip_streak}/{DREAM_FORCE_AFTER}")
                    except Exception:
                        pass

                    if self._dream_skip_streak >= DREAM_FORCE_AFTER:
                        # Force a dream once threshold is hit
                        try:
                            self._should_fire('explorer', step) and self.mind.slots.explorer.start(self.mind._explorer_answer_pending_question())
                            scheduled_explorer = True
                            dream_forced = True
                            # reset the streak after forcing
                            self._dream_skip_streak = 0
                            try:
                                self.mind.metrics.increment('dream.forced')
                            except Exception:
                                pass
                            try:
                                if E8_LOG_SCHEDULER:
                                    self.mind.console.log(f"[SCHEDULER] Forced dream at step={step} after {DREAM_FORCE_AFTER} skips")
                            except Exception:
                                pass
                        except Exception:
                            # if forcing failed, keep streak so we can retry later
                            pass

            # Emit a concise scheduler diagnostic to the console for debugging
            try:
                # Only log at configured interval to avoid spam, or when key events happen
                # Only treat scheduling/skip events as important; persistent teacher_question should not
                # trigger a diagnostic on every tick (reduces log spam).
                important_event = (
                    scheduled_teacher or scheduled_explorer or skip_dream
                )
                should_log = False
                if self.SCHEDULER_LOG_INTERVAL <= 0:
                    should_log = True
                else:
                    if (step - self._last_scheduler_log_step) >= self.SCHEDULER_LOG_INTERVAL:
                        should_log = True
                    # also log immediately on important events even if interval hasn't elapsed
                    if important_event:
                        should_log = True

                # Periodic beat telemetry
                if step % 100 == 0:
                    try:
                        print(f"[BEAT] step={step} teacher_running={self.mind.slots.teacher.running()} "
                              f"explorer_running={self.mind.slots.explorer.running()} "
                              f"dream_running={self.mind.slots.dream.running()} "
                              f"insight_running={self.mind.slots.insight.running()}")
                    except Exception:
                        pass
                        should_log = True
                    else:
                        if (step - self._last_scheduler_log_step) >= self.SCHEDULER_LOG_INTERVAL:
                            should_log = True
                        # also log immediately on important events even if interval hasn't elapsed
                        if important_event:
                            should_log = True

                if should_log and E8_LOG_SCHEDULER:
                    # compact log message to reduce console noise; render dim to de-emphasize
                    try:
                        self.mind.console.log(
                            f"[dim][SCHEDULER] step={step} qdepth={qdepth} entropy={entropy:.3f} skip_dream={skip_dream} "
                            f"teacher_q={self.mind.teacher_question is not None} t_sched={scheduled_teacher} e_sched={scheduled_explorer}[/dim]"
                        )
                    except Exception:
                        # fall back to plain log if rich markup isn't available
                        self.mind.console.log(
                            f"[SCHEDULER] step={step} qdepth={qdepth} entropy={entropy:.3f} skip_dream={skip_dream} "
                            f"teacher_q={self.mind.teacher_question is not None} t_sched={scheduled_teacher} e_sched={scheduled_explorer}"
                        )
                    self._last_scheduler_log_step = step
            except Exception:
                pass
        except Exception:
            # Keep scheduler robust; don't let diagnostics break scheduling
            try:
                if E8_LOG_SCHEDULER:
                    self.mind.console.log(f"[SCHEDULER] Diagnostic logging failed at step {step}")
            except Exception:
                pass

        def _schedule_proximity_insight(reason: str) -> bool:
            if getattr(self.mind, "_insight_cycle_pending", False):
                return False
            if self.mind.insight_cycle_lock.locked():
                return False
            last_step = getattr(self.mind, "_last_insight_cycle_step", -1)
            if last_step >= 0 and (step - last_step) < self.PROXIMITY_MIN_GAP:
                return False

            self.mind._insight_cycle_pending = True

            async def _runner(step_snapshot=step):
                try:
                    await self.mind._run_insight_cycle()
                except Exception as insight_err:
                    try:
                        self.mind.console.log(f"[PROXIMITY] Insight cycle error: {insight_err}")
                    except Exception:
                        pass
                finally:
                    self.mind._insight_cycle_pending = False
                    self.mind._last_insight_cycle_step = step_snapshot

            asyncio.create_task(_runner())
            if reason == "interval" or self.EXTRA_PROXIMITY_LOG_VERBOSE:
                try:
                    if E8_LOG_SCHEDULER:
                        self.mind.console.log(
                            f"[SCHEDULER] Queued proximity insight (reason={reason}, step={step}, interval={self.PROXIMITY_ALERT_INTERVAL})"
                        )
                except Exception:
                    pass
            return True

        if self._fire(step, self.PROXIMITY_ALERT_INTERVAL, 5):
            _schedule_proximity_insight("interval")
        elif self.ENABLE_EXTRA_PROXIMITY and step % 3 == 0 and hasattr(self.mind, 'memory') and self.mind.memory.graph_db.graph.number_of_nodes() > 10:
            _schedule_proximity_insight("memory-active")


        if self._fire(step, self.INSIGHT_SYNTHESIS_INTERVAL, 13):
            asyncio.create_task(self.mind._run_proactive_insight_synthesis())
            
        if self._fire(step, self.DREAM_INTERVAL, 0):
            # Manage skip streak only on actual dream interval ticks
            if skip_dream:
                self._dream_skip_streak += 1
                try:
                    self.mind.metrics.gauge('dream.skip_streak', self._dream_skip_streak)
                except Exception:
                    pass
                # Force a dream after 10 consecutive skips regardless of gating conditions
                if self._dream_skip_streak >= int(os.getenv('E8_DREAM_MAX_SKIPS', '10')):
                    skip_dream = False
                    dream_forced = True
                    try:
                        self.mind.metrics.increment('dream.forced')
                    except Exception:
                        pass
            else:
                # Reset streak when a dream is allowed
                self._dream_skip_streak = 0

            if not skip_dream:
                self.mind.slots.dream.start(self.mind.dream_engine.run_dream_sequence())
                if dream_forced:
                    metrics_log('dream_forced_execution', {'step': step})
            
        if self._fire(step, self.NARRATIVE_SUMMARY_INTERVAL, 2):
            self.mind.slots.subnarr.start(self.mind._generate_subconscious_narrative())
            
        if self._fire(step, self.SNAPSHOT_INTERVAL, 0):
            self.mind.slots.snapshot.start(self.mind.memory.snapshot())
            
        if self._fire(step, self.DECAY_INTERVAL, 21):
            self.mind.slots.decay.start(self.mind.memory.apply_decay())

        # Optional: periodic validator sweep over recent insights
        try:
            period = int(os.getenv("E8_VALIDATOR_PERIOD_STEPS", "0"))
        except Exception:
            period = 0
        if period and period > 0 and (step % period == 0):
            try:
                limit = int(os.getenv("E8_VALIDATOR_RECENT_LIMIT", "8"))
            except Exception:
                limit = 8
            try:
                # Pull recent nodes from graph directly (newest-first scan)
                g = getattr(getattr(self.mind, 'memory', None), 'graph_db', None)
                graph = getattr(g, 'graph', None)
                if graph is not None:
                    nodes_data = list(graph.nodes(data=True))
                else:
                    nodes_data = []
                count = 0
                for nid, data in nodes_data[::-1]:
                    t = (data or {}).get("type", "")
                    if t in ("insight_synthesis", "explorer_insight"):
                        # Use helper to handle gating and thresholds
                        try:
                            self.mind._maybe_validate_new_insight(nid, kind="periodic", rating=data.get("rating"))
                        except Exception:
                            pass
                        count += 1
                        if count >= limit:
                            break
                if count > 0 and E8_LOG_SCHEDULER:
                    try:
                        self.mind.console.log(f"[SCHEDULER] Periodic validator sweep queued {count} insights")
                    except Exception:
                        pass
            except Exception:
                pass

        import numpy as _np
        if str(os.getenv("E8_GEODESIC_ENABLE", "1")) != "0":
            interval = int(self.GEODESIC_SYNTHESIS_INTERVAL)
            mantle = getattr(self.mind, "fluid_mantle", None)
            if mantle and hasattr(mantle, "find_high_curvature_regions"):
                try:
                    hs = mantle.find_high_curvature_regions(top_n=1)
                    avg_curv = float(hs[0][1]) if hs else 0.0
                    avg_curv = float(_np.clip(avg_curv, -1.0, 1.0))
                    interval = max(10, int(interval - avg_curv * 10))
                except Exception:
                    pass
            if self._fire(step, interval, self.GEODESIC_OFFSET):
                asyncio.create_task(self.mind._proactive_synthesis_from_geodesics())

class MetricsManager:
    def __init__(self, run_id: str):
        self.run_id = run_id
        self.log_file = get_path("metrics.ndjson", self.run_id)
        os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
        self.lock = threading.Lock()
        self.console = console
        self._counters = defaultdict(int)

    def snapshot_counters(self):
        return dict(self._counters)

    def _log(self, metric_type: str, data: dict):
        try:
            log_entry = {
                "ts": datetime.now(timezone.utc).isoformat(),
                "run_id": self.run_id,
                "type": metric_type,
                **data
            }
            with self.lock:
                with open(self.log_file, "a", encoding="utf-8") as f:
                    f.write(json.dumps(log_entry, cls=NumpyEncoder) + "\n")
        except Exception as e:
            self.console.log(f"[bold red][MetricsManager] Failed to log metric: {e}[/bold red]")

    def increment(self, name: str, value: int = 1, tags: Optional[Dict[str, Any]] = None):
        self._log("counter", {"name": name, "value": value, "tags": tags or {}})
        try:
            self._counters[name] += int(value)
        except Exception:
            pass

    def gauge(self, name: str, value: float, tags: Optional[Dict[str, Any]] = None):
        self._log("gauge", {"name": name, "value": value, "tags": tags or {}})

    def timing(self, name: str, duration_ms: float, tags: Optional[Dict[str, Any]] = None):
        self._log("timing", {"name": name, "duration_ms": duration_ms, "tags": tags or {}})

    def event(self, name: str, data: dict):
        self._log("event", {"name": name, **data})

class ContextBandit:
    """
    LinUCB/Thompson with ridge, online normalization, and optional forgetting.
    Backward-compatible .pull(context) -> arm_index and .update(arm, reward, context).
    """

    def __init__(self, arms: list, state_dim: int, path_json: str, alpha: float = 1.0):
        self.arms = arms
        self.num_arms = len(arms)
        self.state_dim = state_dim
        self.path = path_json
        self.alpha = alpha

        # New knobs (env)
        # Ridge regularization (raised baseline to 5e-3 for improved numerical stability without over-shrinking)
        self.lambda_ridge = float(os.getenv("E8_BANDIT_RIDGE", "5e-3"))
        self.forget = float(os.getenv("E8_BANDIT_FORGET", "0.0"))   # 0..1 (interpreted as decay applied per update)
        self.policy = os.getenv("E8_BANDIT_POLICY", "linucb").lower()  # linucb|thompson

        # Enhanced stability features (M24 improvements)
        self.symmetrize = E8_BANDIT_SYMMETRIZE
        self.row_normalize = E8_BANDIT_ROW_NORMALIZE
        self.clip_exploration = E8_BANDIT_CLIP_EXPLORATION
        self.clip_percentile = E8_BANDIT_CLIP_PERCENTILE
        self.exploration_history = []  # Track exploration bonuses for percentile clipping

        # Online normalization buffers
        self._mu = np.zeros((state_dim, 1), dtype=float)
        self._m2 = np.zeros((state_dim, 1), dtype=float)
        self._n  = 0

        # Per-arm design matrices
        self.A = [np.identity(state_dim, dtype=float) for _ in range(self.num_arms)]
        self.b = [np.zeros((state_dim, 1), dtype=float) for _ in range(self.num_arms)]

        self._load()
        self._updates = 0  # count total updates for periodic health logging
        # Telemetry stats
        self.arm_N = [0 for _ in range(self.num_arms)]
        self.arm_reward_sum = [0.0 for _ in range(self.num_arms)]
        self.arm_last_reward = [0.0 for _ in range(self.num_arms)]
        self.arm_last_pick_step = [-1 for _ in range(self.num_arms)]
        self._explore_exploit_mode = 'explore'

    # ---------- persistence ----------
    def _load(self):
        try:
            with open(self.path, "r", encoding="utf-8") as f:
                data = json.load(f)
            A_list = data.get("A", [])
            b_list = data.get("b", [])
            if len(A_list) == self.num_arms and len(b_list) == self.num_arms:
                self.A = [np.array(A_list[i], dtype=float) for i in range(self.num_arms)]
                self.b = [np.array(b_list[i], dtype=float).reshape((-1,1)) for i in range(self.num_arms)]
                console.log("ðŸ“ˆ [ContextBandit] Loaded enhanced models with stability features.")
            # Load telemetry if present
            self.arm_N = data.get('arm_N', [0 for _ in range(self.num_arms)])
            self.arm_reward_sum = data.get('arm_reward_sum', [0.0 for _ in range(self.num_arms)])
            self.arm_last_reward = data.get('arm_last_reward', [0.0 for _ in range(self.num_arms)])
            self.arm_last_pick_step = data.get('arm_last_pick_step', [-1 for _ in range(self.num_arms)])
        except Exception:
            pass

    def _save(self):
        try:
            out = {"A": [a.tolist() for a in self.A], "b": [b.tolist() for b in self.b],
                   "arm_N": self.arm_N, "arm_reward_sum": self.arm_reward_sum,
                   "arm_last_reward": self.arm_last_reward, "arm_last_pick_step": self.arm_last_pick_step}
            with open(self.path, "w", encoding="utf-8") as f:
                json.dump(out, f)
        except Exception:
            pass

    # ---------- normalization ----------
    def _norm(self, x: np.ndarray) -> np.ndarray:
        if x.ndim == 1: x = x.reshape((-1,1))
        self._n += 1
        delta = x - self._mu
        self._mu += delta / max(self._n, 1)
        self._m2 += delta * (x - self._mu)
        var = np.clip(self._m2 / max(self._n - 1, 1), 1e-8, None)
        return (x - self._mu) / np.sqrt(var)

    # ---------- matrix stabilization (M24 improvements) ----------
    def _stabilize_matrix(self, A: np.ndarray) -> np.ndarray:
        """Apply symmetrization and row normalization for stability."""
        A_stable = A.copy()
        
        # Symmetrize: A = (A + A^T) / 2
        if self.symmetrize:
            A_stable = (A_stable + A_stable.T) / 2.0
        
        # Row normalize: each row sums to 1 (with epsilon for safety)
        if self.row_normalize:
            row_sums = np.sum(np.abs(A_stable), axis=1, keepdims=True)
            row_sums = np.maximum(row_sums, 1e-8)  # Avoid division by zero
            A_stable = A_stable / row_sums
        
        return A_stable

    def _clip_exploration_bonus(self, bonus: float) -> float:
        """Clip exploration bonus to percentile threshold."""
        if not self.clip_exploration:
            return bonus
        
        # Track exploration bonuses
        self.exploration_history.append(abs(bonus))
        
        # Keep only recent history (last 1000 samples)
        if len(self.exploration_history) > 1000:
            self.exploration_history = self.exploration_history[-1000:]
        
        # Clip to percentile if we have enough samples
        if len(self.exploration_history) >= 50:
            try:
                threshold = np.percentile(self.exploration_history, self.clip_percentile)
                clipped_bonus = np.clip(bonus, -threshold, threshold)
                return clipped_bonus
            except Exception:
                return bonus
        
        return bonus

    # ---------- policy ----------
    def pull(self, context: np.ndarray) -> int:
        if context is None or len(context) != self.state_dim:
            return random.randrange(self.num_arms)
        x = self._norm(np.asarray(context, dtype=float).reshape((self.state_dim, 1)))
        scores = np.zeros(self.num_arms, dtype=float)

        for i in range(self.num_arms):
            # Apply matrix stabilization
            A_stable = self._stabilize_matrix(self.A[i])
            A_reg = A_stable + self.lambda_ridge * np.identity(self.state_dim)
            
            # Solve is numerically better than inverse
            theta = np.linalg.solve(A_reg, self.b[i])

            if self.policy == "linucb":
                # mean + α * uncertainty (with clipping)
                mean = float((theta.T @ x).item())
                y = np.linalg.solve(A_reg, x)  # A^{-1} x
                
                # Numerical safety: x.T @ A^{-1} @ x should be >= 0, but numerical errors can make it negative
                uncertainty_squared = (x.T @ y).item()
                uncertainty_squared = max(1e-12, uncertainty_squared)  # Ensure non-negative with small epsilon
                uncert = float(np.sqrt(uncertainty_squared))
                
                exploration_bonus = float(self.alpha) * uncert
                
                # Apply exploration bonus clipping
                clipped_bonus = self._clip_exploration_bonus(exploration_bonus)
                scores[i] = mean + clipped_bonus

            elif self.policy == "thompson":
                # θ ~ N(mean=theta, cov=A_reg^{-1}) ; assume unit noise
                Sigma = np.linalg.inv(A_reg)
                sample_theta = np.random.multivariate_normal(theta.ravel(), Sigma).reshape((-1,1))
                scores[i] = float((sample_theta.T @ x).item())

            else:
                # fallback: greedy mean
                scores[i] = float((theta.T @ x).item())

        arm = int(np.argmax(scores))
        # Track exploreâ†’exploit transition heuristic: if alpha small and counts large
        try:
            explore_ratio = sum(1 for n in self.arm_N if n < 5) / max(1, self.num_arms)
            mode_now = 'exploit' if explore_ratio < 0.25 else 'explore'
            if mode_now != self._explore_exploit_mode:
                metrics_log('bandit.policy_change', {"event": "bandit", "from": self._explore_exploit_mode, "to": mode_now})
                self._explore_exploit_mode = mode_now
        except Exception:
            pass
        return arm

    def update(self, arm_index: int, reward: float, context: np.ndarray):
        if context is None or len(context) != self.state_dim:
            return
        x = self._norm(np.asarray(context, dtype=float).reshape((self.state_dim, 1)))

        # Optional forgetting to bound condition number
        if self.forget > 0.0:
            self.A[arm_index] *= (1.0 - self.forget)
            self.b[arm_index] *= (1.0 - self.forget)

        # Rank-1 update
        self.A[arm_index] += x @ x.T
        self.b[arm_index] += float(reward) * x
        # Telemetry stats
        try:
            self.arm_N[arm_index] += 1
            self.arm_reward_sum[arm_index] += float(reward)
            self.arm_last_reward[arm_index] = float(reward)
            self.arm_last_pick_step[arm_index] = getattr(console, 'step_num', -1)
        except Exception:
            pass

        # Occasionally save
        if (arm_index + self.A[arm_index].trace()) % 17 < 1:
            self._save()

        # Periodic health telemetry every 250 updates
        self._updates += 1
        if self._updates % 250 == 0:
            self.log_health()

    # ---------- health telemetry ----------
    def log_health(self, step: int = None):
        try:
            conds = []
            stab_conds = []  # Condition numbers after stabilization
            for i in range(self.num_arms):
                # Original condition number
                A_reg = self.A[i] + self.lambda_ridge * np.identity(self.state_dim)
                conds.append(float(np.linalg.cond(A_reg)))
                
                # Stabilized condition number
                A_stable = self._stabilize_matrix(self.A[i])
                A_stable_reg = A_stable + self.lambda_ridge * np.identity(self.state_dim)
                stab_conds.append(float(np.linalg.cond(A_stable_reg)))
            
            # Enhanced metrics
            metrics_log("bandit_health", {
                "step": step, "policy": self.policy,
                "ridge": self.lambda_ridge, "forget": self.forget,
                "conds": conds, "stabilized_conds": stab_conds, "arms": self.num_arms,
                "symmetrize": self.symmetrize, "row_normalize": self.row_normalize,
                "clip_exploration": self.clip_exploration, "clip_percentile": self.clip_percentile,
                "exploration_history_size": len(self.exploration_history)
            })
            try:
                max_cond = max(conds) if conds else 0
                max_stab_cond = max(stab_conds) if stab_conds else 0
                if max_cond > 1e5:
                    metrics_log("bandit_condition_warning", {
                        "max_cond": max_cond, "max_stabilized_cond": max_stab_cond, 
                        "ridge": self.lambda_ridge
                    })
            except Exception:
                pass
        except Exception as e:
            metrics_log("bandit_health_error", {"step": step, "err": str(e)})

    # ---------- backward compatibility ----------
    def load(self):
        """Legacy method name - delegates to _load()"""
        self._load()

    def save(self):
        """Legacy method name - delegates to _save()"""
        self._save()

class NoOpWorldModel:
    def __init__(self):
        self.available = False
        self.ready = False
    async def imagine_with_policy(self, *args, **kwargs):
        return []
    def score_transition(self, state, action):
        return 0.0

class StateVAEWorldModel:
    def __init__(self, input_dim, action_dim, latent_dim=32, rnn_hidden_dim=256):
        if not TORCH_AVAILABLE:
            self.available = False
            self.ready = False
            return
        self.input_dim = input_dim
        self.action_dim = action_dim
        self.latent_dim = latent_dim
        self.rnn_hidden_dim = rnn_hidden_dim
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.available = True
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128), nn.ReLU(),
            nn.Linear(128, 64), nn.ReLU()
        ).to(self.device)
        self.fc_mu = nn.Linear(64, latent_dim).to(self.device)
        self.fc_logvar = nn.Linear(64, latent_dim).to(self.device)
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 64), nn.ReLU(),
            nn.Linear(64, 128), nn.ReLU(),
            nn.Linear(128, input_dim)
        ).to(self.device)

        self.rnn = nn.GRU(latent_dim + action_dim, rnn_hidden_dim, batch_first=True).to(self.device)
        self.fc_next_latent = nn.Linear(rnn_hidden_dim, latent_dim).to(self.device)

        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)
        self.replay_buffer = deque(maxlen=10000)
        self.ready = False
        console.log(f"ðŸŒ [WorldModel] Initialized with VAE and RNN on device: {self.device}.")

    def parameters(self):
        import itertools
        return itertools.chain(
            self.encoder.parameters(), self.fc_mu.parameters(), self.fc_logvar.parameters(),
            self.decoder.parameters(), self.rnn.parameters(), self.fc_next_latent.parameters()
        )

    def _reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def _encode(self, state):
        h = self.encoder(state)
        return self.fc_mu(h), self.fc_logvar(h)

    def observe(self, state, action, next_state, reward):
        self.replay_buffer.append((state, action, next_state, reward))
        if len(self.replay_buffer) > 256:
            # Throttle training if parent mind imposes limits
            parent = getattr(self, 'mind_ref', None)
            can_train = True
            try:
                if parent is not None and hasattr(parent, '_can_train_more'):
                    can_train = parent._can_train_more()
            except Exception:
                can_train = True
            if can_train:
                self._train()
                if parent is not None and hasattr(parent, '_record_train_call'):
                    try: parent._record_train_call()
                    except Exception: pass
                if not self.ready:
                    self.ready = True
                    console.log("ðŸŒ [WorldModel] Model is now ready for imagination.")

    def _train(self, batch_size=64):
        if len(self.replay_buffer) < batch_size:
            return

        batch = random.sample(self.replay_buffer, batch_size)
        states, actions, next_states, _ = zip(*batch)

        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)
        actions = torch.tensor(np.array(actions), dtype=torch.float32).to(self.device)
        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device)

        mu, logvar = self._encode(torch.cat([states, next_states]))
        z = self._reparameterize(mu, logvar)
        recon_states = self.decoder(z)
        recon_loss = F.mse_loss(recon_states, torch.cat([states, next_states]))
        kld_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
        
        with torch.no_grad():
            mu_z, _ = self._encode(states)
            mu_next_z, _ = self._encode(next_states)
        
        rnn_input = torch.cat([mu_z, actions], dim=1).unsqueeze(1)
        rnn_output, _ = self.rnn(rnn_input)
        pred_next_z = self.fc_next_latent(rnn_output.squeeze(1))
        transition_loss = F.mse_loss(pred_next_z, mu_next_z)

        loss = recon_loss + 0.1 * kld_loss + transition_loss
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def imagine_with_policy(self, start_state, policy, horizon=10):
        if not self.ready:
            return []
        
        with torch.no_grad():
            start_state_t = torch.tensor(start_state, dtype=torch.float32).to(self.device).unsqueeze(0)
            mu, _ = self._encode(start_state_t)
            
            imagined_latents = []
            current_z = mu
            rnn_hidden = torch.zeros(1, 1, self.rnn_hidden_dim).to(self.device)

            for _ in range(horizon):
                current_state_recon = safe_tensor_to_numpy(self.decoder(current_z)).flatten()
                action = policy.select_action(current_state_recon, deterministic=True)
                action_t = torch.tensor(action, dtype=torch.float32).to(self.device).unsqueeze(0)
                
                rnn_input = torch.cat([current_z, action_t], dim=1).unsqueeze(1)
                rnn_output, rnn_hidden = self.rnn(rnn_input, rnn_hidden)
                next_z = self.fc_next_latent(rnn_output.squeeze(1))
                
                imagined_latents.append(self.decoder(next_z))
                current_z = next_z

        return imagined_latents

    def is_ready(self):
        return self.ready

    def train_batch(self, traj, recon_w=1.0, kl_w=0.1, trans_w=1.0):
        if not TORCH_AVAILABLE or not getattr(self, 'available', True):
            return None
        try:
            states, actions, next_states, rewards = [], [], [], []
            for (s,a,sp,r) in traj:
                states.append(np.array(s, dtype=np.float32))
                actions.append(np.array(a, dtype=np.float32))
                next_states.append(np.array(sp, dtype=np.float32))
                rewards.append(float(r))
            states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)
            actions = torch.tensor(np.array(actions), dtype=torch.float32).to(self.device)
            next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device)
            mu, logvar = self._encode(torch.cat([states, next_states]))
            z = self._reparameterize(mu, logvar)
            recon_states = self.decoder(z)
            recon_loss = F.mse_loss(recon_states, torch.cat([states, next_states]))
            kld_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
            with torch.no_grad():
                mu_z, _ = self._encode(states)
                mu_next_z, _ = self._encode(next_states)
            rnn_input = torch.cat([mu_z, actions], dim=1).unsqueeze(1)
            rnn_output, _ = self.rnn(rnn_input)
            pred_next_z = self.fc_next_latent(rnn_output.squeeze(1))
            transition_loss = F.mse_loss(pred_next_z, mu_next_z)
            loss = recon_w*recon_loss + kl_w*kld_loss + trans_w*transition_loss
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)
            self.optimizer.step()
            self.ready = True
            return {
                "loss_total": float(loss.detach().cpu().item()),
                "loss_recon": float(recon_loss.detach().cpu().item()),
                "loss_kl": float(kld_loss.detach().cpu().item()),
                "loss_trans": float(transition_loss.detach().cpu().item()),
            }
        except Exception:
            return None

class CausalEngine:
    def __init__(self, console_instance):
        if nx is None:
            raise ImportError("networkx is required for the CausalEngine.")
        self.console = console_instance
        self.graph = nx.DiGraph()
        self.correlation_matrix = defaultdict(float)
        self.update_counts = defaultdict(int)
        self.learning_rate = 0.01
        self.last_state_reward = {"reward": 0.0}
        self.console.log("ðŸ”— [CausalEngine] Initialized with correlation tracker.")

    def update_on_step(self, mind_state: 'E8Mind', action: np.ndarray, reward: float):
        current_state_reward = self._get_state_reward_dict(mind_state, action, reward)
        
        deltas = {
            key: current_state_reward.get(key, 0) - self.last_state_reward.get(key, 0)
            for key in current_state_reward
        }
        
        reward_delta = deltas.get("reward", 0)
        
        if abs(reward_delta) > 1e-4:
            for key, delta in deltas.items():
                if key == "reward" or abs(delta) < 1e-4:
                    continue
                
                correlation_direction = 1.0 if (delta * reward_delta) > 0 else -1.0
                
                edge = (key, "reward")
                self.correlation_matrix[edge] = \
                    (1 - self.learning_rate) * self.correlation_matrix[edge] + self.learning_rate * correlation_direction
                self.update_counts[edge] += 1
                
                if abs(self.correlation_matrix[edge]) > 0.3 and self.update_counts[edge] > 50:
                    self.graph.add_edge(key, "reward", weight=self.correlation_matrix[edge])

        self.last_state_reward = current_state_reward

    def _get_state_reward_dict(self, mind: 'E8Mind', action: np.ndarray, reward: float) -> Dict[str, float]:
        data = {"reward": reward}
        data.update({f"mood_{k}": v for k, v in mind.mood.mood_vector.items()})
        data.update({f"action_{i}": v for i, v in enumerate(action)})
        return data
        
    def get_strongest_influences(self, target: str = "reward", k: int = 3):
        if not self.graph.has_node(target):
            return []
        
        influences = sorted(
            self.graph.in_edges(target, data=True),
            key=lambda edge: abs(edge[2].get('weight', 0)),
            reverse=True
        )
        return influences[:k]

class _NullPE:
    def calculate_potential_and_get_reward(self):
        return 0.0

class HierarchicalController:
    def __init__(self, goal_field: 'GoalField', potential_evaluator: 'StatePotentialEvaluator', console_instance: 'Console'):
        self.console = console_instance
        self.goal_field = goal_field
        self.potential_evaluator = potential_evaluator
        self.update_interval = 100
        self.current_goal_name = "synthesis"
        self.current_goal_embedding: Optional[np.ndarray] = None
        self.last_goal_similarity = 0.0
        self.intrinsic_reward_scale = 0.1
        console.log("ðŸ‘‘ [HRL] Initialized with goal-directed reward shaping.")
        self.last_logged_goal = None
        self.last_goal_log_step = -1
        self.mind_ref = None

    def maybe_update(self, step: int):
        try:
            if self.mind_ref and hasattr(self.mind_ref, 'memory'):
                concept_count = self.mind_ref.memory.graph_db.graph.number_of_nodes()
                if concept_count == 0:
                    if step % 1000 == 0:
                        self.console.log("[HRL] Waiting for concepts to be added to memory before setting goals...")
                    return
        except Exception:
            pass
        
        if step % self.update_interval == 0 and self.goal_field.is_initialized:
            top_goals = self.goal_field.get_top_goals(k=1)
            if top_goals:
                new_goal_name, _ = top_goals[0]
                if (new_goal_name != self.current_goal_name or step - self.last_goal_log_step > self.update_interval * 5):
                    self.current_goal_name = new_goal_name
                    self.current_goal_embedding = self.goal_field.goals[self.current_goal_name]["embedding"]
                    self.console.print(Panel(f"New high-level objective set: [bold cyan]{self.current_goal_name.upper()}[/]",
                                           title="[bold yellow]HRL OBJECTIVE[/]", border_style="red"))
                    self.last_goal_log_step = step
                else:
                    self.current_goal_name = new_goal_name
                    self.current_goal_embedding = self.goal_field.goals[self.current_goal_name]["embedding"]

    def shape_reward(self, state: np.ndarray, next_state: np.ndarray, base_reward: float) -> float:
        if self.current_goal_embedding is None or not self.goal_field.is_initialized:
            return base_reward
        
        current_potential = self.potential_evaluator.last_potential
        intrinsic_reward = base_reward * self.intrinsic_reward_scale
        return base_reward + intrinsic_reward

class EmergenceController:
    """HTTP-based controller for toggling between assisted and self-emergent modes."""


    async def start_server(self):
        """Start the HTTP control server."""
        try:
            from aiohttp import web
            import aiohttp

            self.loop = asyncio.get_event_loop()

            app = web.Application()
            app.router.add_get('/', self.handle_status)
            app.router.add_get('/emergence/mode', self.handle_emergence_mode_get)
            app.router.add_post('/emergence/mode', self.handle_emergence_mode_post)
            app.router.add_post('/emergence/anneal', self.handle_emergence_anneal)
            app.router.add_get('/stats', self.handle_stats)
            app.router.add_post('/threshold', self.handle_threshold_update)

            self.server = web.TCPSite(await asyncio.start_server(
                lambda r, w: self._handle_http_request(r, w, app),
                self.host, self.port
            ))

            await self.server.start()
            self.running = True
            self.console.log(f"[EmergenceController] HTTP server started on http://{self.host}:{self.port}")

        except ImportError:
            self.console.log("[yellow][EmergenceController] aiohttp not available, HTTP control disabled[/yellow]")
        except Exception as e:
            self.console.log(f"[yellow][EmergenceController] Failed to start HTTP server: {e}[/yellow]")

    async def handle_emergence_mode_get(self, request):
        """Handle GET /emergence/mode endpoint."""
        try:
            from aiohttp import web

            response = {
                'mode': self.emergence_mode,
                'coupling_gains': self.get_current_couplings(),
                'field_energy': self.emergence_metrics.get('field_energy_total', 0.0),
                'ray_success_rate': self.emergence_metrics.get('ray_success_rate', 0.0),
                'annealing': self.get_annealing_status(),
                'step': self.mind.step_num
            }

            return web.json_response(response)

        except Exception as e:
            return web.json_response({'error': str(e)}, status=500)

    async def handle_emergence_mode_post(self, request):
        """Handle POST /emergence/mode endpoint."""
        try:
            from aiohttp import web

            data = await request.json()
            new_mode = data.get('mode')

            if new_mode not in ['assisted', 'self-emergent']:
                return web.json_response({'error': 'Invalid mode. Must be "assisted" or "self-emergent"'}, status=400)

            old_mode = self.emergence_mode
            self.set_emergence_mode(new_mode)

            response = {
                'success': True,
                'old_mode': old_mode,
                'new_mode': new_mode,
                'coupling_gains': self.get_current_couplings(),
                'step': self.mind.step_num
            }

            return web.json_response(response)

        except Exception as e:
            return web.json_response({'error': str(e)}, status=500)

    async def handle_emergence_anneal(self, request):
        """Handle POST /emergence/anneal endpoint."""
        try:
            from aiohttp import web

            data = await request.json()
            steps = data.get('steps', 100)

            if not isinstance(steps, int) or steps <= 0:
                return web.json_response({'error': 'Steps must be a positive integer'}, status=400)

            result = self.anneal_to_self(steps)

            if 'error' in result:
                return web.json_response(result, status=400)

            response = {
                'status': result['status'],
                'steps': steps,
                'annealing': self.get_annealing_status(),
                'step': self.mind.step_num
            }

            return web.json_response(response)

        except Exception as e:
            return web.json_response({'error': str(e)}, status=500)

    async def stop_server(self):
        """Stop the HTTP control server."""
        if self.server:
            await self.server.stop()
            self.running = False
            self.console.log("[EmergenceController] HTTP server stopped")

    async def handle_status(self, request):
        """Handle status endpoint."""
        try:
            from aiohttp import web

            status = {
                'mode': self.emergence_mode,
                'confidence': float(self.emergent_confidence),
                'threshold': float(self.assisted_threshold),
                'step': self.mind.step_num,
                'uptime': time.time() - getattr(self.mind, 'start_time', time.time())
            }

            return web.json_response(status)

        except Exception as e:
            return web.json_response({'error': str(e)}, status=500)

    async def handle_mode_switch(self, request):
        """Handle mode switch endpoint."""
        try:
            from aiohttp import web

            data = await request.json()
            new_mode = data.get('mode')

            if new_mode not in ['assisted', 'self-emergent']:
                return web.json_response({'error': 'Invalid mode'}, status=400)

            old_mode = self.emergence_mode
            self.set_emergence_mode(new_mode)

            response = {
                'success': True,
                'old_mode': old_mode,
                'new_mode': new_mode,
                'step': self.mind.step_num
            }

            return web.json_response(response)

        except Exception as e:
            return web.json_response({'error': str(e)}, status=500)

    async def handle_stats(self, request):
        """Handle stats endpoint."""
        try:
            from aiohttp import web

            stats = dict(self.emergence_stats)
            stats['current_mode'] = self.emergence_mode
            stats['confidence'] = float(self.emergent_confidence)
            stats['threshold'] = float(self.assisted_threshold)

            return web.json_response(stats)

        except Exception as e:
            return web.json_response({'error': str(e)}, status=500)

    async def handle_threshold_update(self, request):
        """Handle threshold update endpoint."""
        try:
            from aiohttp import web

            data = await request.json()
            new_threshold = data.get('threshold')

            if not isinstance(new_threshold, (int, float)) or not 0.0 <= new_threshold <= 1.0:
                return web.json_response({'error': 'Invalid threshold'}, status=400)

            old_threshold = self.assisted_threshold
            self.assisted_threshold = float(new_threshold)

            response = {
                'success': True,
                'old_threshold': old_threshold,
                'new_threshold': new_threshold
            }

            return web.json_response(response)

        except Exception as e:
            return web.json_response({'error': str(e)}, status=500)

    async def _handle_http_request(self, reader, writer, app):
        """Handle raw HTTP requests (fallback implementation)."""
        try:
            # Simple HTTP response for basic functionality
            response = "HTTP/1.1 200 OK\r\nContent-Type: application/json\r\n\r\n"
            response += '{"status": "EmergenceController active", "mode": "' + self.emergence_mode + '"}'
            writer.write(response.encode())
            await writer.drain()
            writer.close()

        except Exception as e:
            self.console.log(f"[EmergenceController] HTTP request handling failed: {e}")


    def _notify_mode_change(self, new_mode: str):
        """Notify the mind instance of mode change."""
        try:
            if hasattr(self.mind, 'emergence_mode_changed'):
                self.mind.emergence_mode_changed(new_mode)
        except Exception as e:
            self.console.log(f"[yellow][EmergenceController] Mode change notification failed: {e}[/yellow]")

    def should_use_assisted_mode(self) -> bool:
        """Determine if assisted mode should be used based on current conditions."""
        # Check cooldown
        if self.mind.step_num - self.last_mode_switch < self.mode_switch_cooldown:
            return self.emergence_mode == "assisted"

        # Check confidence threshold
        if self.emergent_confidence < self.assisted_threshold:
            return True

        # Check system stability
        stability_score = self._compute_system_stability()
        if stability_score < 0.5:
            return True

        return False

    def _compute_system_stability(self) -> float:
        """Compute system stability score."""
        try:
            stability = 1.0

            # Check memory health
            if hasattr(self.mind, 'memory'):
                node_count = self.mind.memory.graph_db.graph.number_of_nodes()
                if node_count < 10:
                    stability *= 0.5

            # Check mood stability
            if hasattr(self.mind, 'mood'):
                mood_vector = self.mind.mood.mood_vector
                mood_variance = np.var(mood_vector)
                stability *= (1.0 - min(mood_variance, 0.5))

            # Check field stability
            if hasattr(self.mind, 'substrate'):
                field_energy = self.mind.substrate.compute_pressure_proxy()
                if field_energy > 1e6:  # Very high energy indicates instability
                    stability *= 0.3

            return max(0.0, min(1.0, stability))

        except Exception:
            return 0.5

    def update_emergent_confidence(self, success: bool, feedback_strength: float = 0.1):
        """Update emergent confidence based on success/failure."""
        try:
            if success:
                # Increase confidence on success
                self.emergent_confidence = min(1.0, self.emergent_confidence + feedback_strength)
                self.emergence_stats['successful_emergence'] += 1
            else:
                # Decrease confidence on failure
                self.emergent_confidence = max(0.0, self.emergent_confidence - feedback_strength)
                self.emergence_stats['failed_emergence'] += 1

        except Exception as e:
            self.console.log(f"[yellow][EmergenceController] Confidence update failed: {e}[/yellow]")

    def make_decision(self, decision_type: str, options: List[Any]) -> Any:
        """Make a decision using current emergence mode."""
        try:
            if self.should_use_assisted_mode():
                self.emergence_stats['assisted_decisions'] += 1
                return self._make_assisted_decision(decision_type, options)
            else:
                self.emergence_stats['emergent_decisions'] += 1
                return self._make_emergent_decision(decision_type, options)

        except Exception as e:
            self.console.log(f"[yellow][EmergenceController] Decision making failed: {e}[/yellow]")
            # Fallback to random choice
            return random.choice(options) if options else None

    def _make_assisted_decision(self, decision_type: str, options: List[Any]) -> Any:
        """Make decision with assistance from external guidance."""
        try:
            # Use LLM or predefined rules for assisted decisions
            if hasattr(self.mind, 'llm_caller') and self.mind.llm_caller:
                # Use LLM for complex decisions
                prompt = f"Given options {options}, which is best for {decision_type}? Respond with just the chosen option."
                response = self.mind.llm_caller.enqueue_and_wait(prompt, max_tokens=50)
                if response and not response.startswith("[LLM"):
                    for option in options:
                        if str(option) in response:
                            return option

            # Fallback to highest-rated option
            return self._select_highest_rated_option(options)

        except Exception:
            return self._select_highest_rated_option(options)

    def _make_emergent_decision(self, decision_type: str, options: List[Any]) -> Any:
        """Make decision using emergent self-organization."""
        try:
            # Use system state for emergent decisions
            if hasattr(self.mind, 'mood'):
                mood_influence = np.dot(self.mind.mood.mood_vector, np.random.randn(len(self.mind.mood.mood_vector)))
                mood_bias = np.tanh(mood_influence * 0.1)

                # Bias selection based on mood
                if mood_bias > 0.2:
                    return options[len(options)//2] if len(options) > 1 else options[0]
                elif mood_bias < -0.2:
                    return options[0] if options else None

            # Use field manifold for decision guidance
            if hasattr(self.mind, 'substrate'):
                field_energy = self.mind.substrate.compute_pressure_proxy()
                energy_bias = np.tanh(field_energy * 1e-6)

                if energy_bias > 0.1:
                    return options[-1] if options else None  # Prefer last option under high energy

            # Default emergent behavior
            return random.choice(options) if options else None

        except Exception:
            return random.choice(options) if options else None

    def _select_highest_rated_option(self, options: List[Any]) -> Any:
        """Select the highest-rated option from available choices."""
        try:
            if not options:
                return None

            # Try to find ratings in memory
            if hasattr(self.mind, 'memory'):
                best_option = None
                best_rating = -float('inf')

                for option in options:
                    # Look for option in memory
                    similar = self.mind.memory.find_similar_in_main_storage(
                        self.mind.embed_adapter(str(option)), k=1
                    )

                    if similar:
                        node_id, _ = similar[0]
                        node = self.mind.memory.graph_db.get_node(node_id)
                        if node:
                            rating = node.get('rating', 0.5)
                            if rating > best_rating:
                                best_rating = rating
                                best_option = option

                if best_option is not None:
                    return best_option

            # Fallback to first option
            return options[0]

        except Exception:
            return options[0] if options else None

    def get_status(self) -> Dict[str, Any]:
        """Get current status of the emergence controller."""
        return {
            'mode': self.emergence_mode,
            'confidence': float(self.emergent_confidence),
            'threshold': float(self.assisted_threshold),
            'stats': dict(self.emergence_stats),
            'stability': float(self._compute_system_stability()),
            'server_running': self.running,
            'couplings': self.get_current_couplings(),
            'annealing': self.get_annealing_status(),
            'metrics': self.get_emergence_metrics()
        }

    # --- Coupling Parameters and Annealing (from blueprint) ---
    def __init__(self, mind_instance: 'E8Mind', host: str = "localhost", port: int = 8080):
        self.mind = mind_instance
        self.console = mind_instance.console
        self.host = host
        self.port = port

        # Emergence mode settings
        self.emergence_mode = "assisted"  # "assisted" or "self-emergent"
        self.assisted_threshold = 0.7     # Confidence threshold for assisted mode
        self.emergent_confidence = 0.0    # Current emergent confidence level
        self.mode_switch_cooldown = 300   # Steps between mode switches
        self.last_mode_switch = -self.mode_switch_cooldown

        # Coupling parameters (from blueprint)
        self.couplings = {
            'rotor_to_source_gain': 1.0,    # Map rotor angular speed to boundary current
            'field_damping': 0.1,           # Suppress ringing on shells
            'curvature_feedback': 1.0,      # Feed EM stress to weak gravity
            'use_field_rays': False         # Ray casting for field navigation
        }

        # Annealing state
        self.annealing_active = False
        self.annealing_steps = 0
        self.annealing_total_steps = 100
        self.annealing_start_step = 0
        self.annealing_paused = False
        self.annealing_pause_reason = ""

        # Mode-specific coupling presets (from blueprint)
        self.mode_presets = {
            'assisted': {
                'rotor_to_source_gain': 1.0,
                'field_damping': 0.1,
                'curvature_feedback': 1.0,
                'use_field_rays': False
            },
            'self-emergent': {
                'rotor_to_source_gain': 0.1,
                'field_damping': 0.01,
                'curvature_feedback': 0.1,
                'use_field_rays': True  # Adaptive
            }
        }

        # Metrics tracking
        self.emergence_metrics = {
            'field_energy_total': 0.0,
            'ray_success_rate': 0.0,
            'mean_ray_latency': 0.0,
            'rotor_activity': 0.0,
            'rotor_pressure': 0.0,
            'path_cache_size': 0,
            'per_shell_energy': {},
            'emergence_mode': self.emergence_mode
        }

        # Safeguards
        self.safeguard_state = {
            'last_stability_check': 0,
            'consecutive_failures': 0,
            'damping_floor': 0.001,
            'recovery_mode': False
        }

        # HTTP server components
        self.server = None
        self.loop = None
        self.running = False

        # Performance tracking
        self.emergence_stats = {
            'assisted_decisions': 0,
            'emergent_decisions': 0,
            'successful_emergence': 0,
            'failed_emergence': 0,
            'mode_switches': 0
        }

        self.console.log(f"[EmergenceController] Initialized in {self.emergence_mode} mode on {host}:{port}")

    def get_current_couplings(self) -> Dict[str, Any]:
        """Get current coupling parameters."""
        return dict(self.couplings)

    def get_annealing_status(self) -> Dict[str, Any]:
        """Get current annealing status."""
        if not self.annealing_active:
            return {'active': False}

        current_step = self.mind.step_num - self.annealing_start_step
        progress = min(1.0, current_step / self.annealing_total_steps)

        return {
            'active': True,
            'progress': progress,
            'current_step': current_step,
            'total_steps': self.annealing_total_steps,
            'paused': self.annealing_paused,
            'pause_reason': self.annealing_pause_reason
        }

    def get_emergence_metrics(self) -> Dict[str, Any]:
        """Get current emergence metrics."""
        return dict(self.emergence_metrics)

    def set_emergence_mode(self, mode: str):
        """Set the emergence mode with coupling updates."""
        if mode not in ['assisted', 'self-emergent']:
            self.console.log(f"[yellow][EmergenceController] Invalid mode: {mode}[/yellow]")
            return

        if mode != self.emergence_mode:
            old_mode = self.emergence_mode
            self.emergence_mode = mode
            self.emergence_stats['mode_switches'] += 1
            self.last_mode_switch = self.mind.step_num

            # Update couplings immediately
            self._apply_mode_couplings(mode)

            # Reset annealing if active
            if self.annealing_active:
                self.annealing_active = False
                self.console.log("[EmergenceController] Annealing cancelled due to manual mode switch")

            self.console.log(f"[EmergenceController] Switched to {mode} mode")
            self.emergence_metrics['emergence_mode'] = mode

            # Notify mind of mode change
            self._notify_mode_change(mode)

    def anneal_to_self(self, steps: int = 100) -> Dict[str, Any]:
        """Start annealing from assisted to self-emergent mode."""
        if self.emergence_mode != 'assisted':
            return {'error': 'Annealing only available from assisted mode'}

        self.annealing_active = True
        self.annealing_steps = 0
        self.annealing_total_steps = steps
        self.annealing_start_step = self.mind.step_num
        self.annealing_paused = False
        self.annealing_pause_reason = ""

        self.console.log(f"[EmergenceController] Starting annealing to self-emergent over {steps} steps")

        return {
            'status': 'started',
            'steps': steps,
            'start_step': self.annealing_start_step
        }

    def _apply_mode_couplings(self, mode: str):
        """Apply coupling preset for the given mode."""
        if mode in self.mode_presets:
            preset = self.mode_presets[mode]
            self.couplings.update(preset)
            self.console.log(f"[EmergenceController] Applied {mode} couplings: {preset}")

    def _update_annealing(self):
        """Update annealing progress and apply interpolated couplings."""
        if not self.annealing_active or self.annealing_paused:
            return

        current_step = self.mind.step_num - self.annealing_start_step
        if current_step >= self.annealing_total_steps:
            # Annealing complete
            self.set_emergence_mode('self-emergent')
            self.annealing_active = False
            self.console.log("[EmergenceController] Annealing complete, switched to self-emergent mode")
            return

        # Check conditional pause conditions
        if self._should_pause_annealing():
            self.annealing_paused = True
            self.annealing_pause_reason = self._get_pause_reason()
            self.console.log(f"[EmergenceController] Annealing paused: {self.annealing_pause_reason}")
            return

        # Linear interpolation: g_t = (1-a_t) * g_assist + a_t * g_self
        alpha_t = current_step / self.annealing_total_steps

        assisted_couplings = self.mode_presets['assisted']
        self_couplings = self.mode_presets['self-emergent']

        for param in ['rotor_to_source_gain', 'field_damping', 'curvature_feedback']:
            g_assist = assisted_couplings[param]
            g_self = self_couplings[param]
            g_t = (1 - alpha_t) * g_assist + alpha_t * g_self
            self.couplings[param] = g_t

        # Update use_field_rays based on ray success rate
        self._update_field_rays_adaptive()

    def _should_pause_annealing(self) -> bool:
        """Check if annealing should be paused based on metrics."""
        try:
            # Check ray success rate
            ray_success = self.emergence_metrics.get('ray_success_rate', 1.0)
            if ray_success < 0.5:
                return True

            # Check BH average (if available)
            if hasattr(self.mind, '_bh_ma50'):
                bh_avg = self.mind._bh_ma50
                if bh_avg > 2.0:  # High BH pressure
                    return True

            # Check explorer/validator latencies (simplified check)
            if hasattr(self.mind, 'insight_agent') and hasattr(self.mind.insight_agent, 'latency_tracker'):
                latency = self.mind.insight_agent.latency_tracker.get('mean_latency', 0)
                if latency > 10.0:  # High latency
                    return True

            return False
        except Exception:
            return False

    def _get_pause_reason(self) -> str:
        """Get the reason for annealing pause."""
        try:
            ray_success = self.emergence_metrics.get('ray_success_rate', 1.0)
            if ray_success < 0.5:
                return f"Low ray success rate: {ray_success:.2f}"

            if hasattr(self.mind, '_bh_ma50') and self.mind._bh_ma50 > 2.0:
                return f"High BH pressure: {self.mind._bh_ma50:.2f}"

            if hasattr(self.mind, 'insight_agent') and hasattr(self.mind.insight_agent, 'latency_tracker'):
                latency = self.mind.insight_agent.latency_tracker.get('mean_latency', 0)
                if latency > 10.0:
                    return f"High latency: {latency:.2f}"

            return "Unknown stability issue"
        except Exception:
            return "Stability check failed"

    def _update_field_rays_adaptive(self):
        """Update use_field_rays based on success rate and stability."""
        try:
            ray_success = self.emergence_metrics.get('ray_success_rate', 0.0)
            stability = self._compute_system_stability()

            # Enable rays if success rate is good and system is stable
            should_use_rays = ray_success > 0.7 and stability > 0.6
            self.couplings['use_field_rays'] = should_use_rays

        except Exception:
            self.couplings['use_field_rays'] = False

    def update_metrics(self):
        """Update emergence-related metrics."""
        try:
            # Update field energy
            if hasattr(self.mind, 'fluid_mantle') and self.mind.fluid_mantle:
                try:
                    summary = self.mind.fluid_mantle.get_shell_energy_summary()
                    self.emergence_metrics['field_energy_total'] = float(summary.get('total_energy', 0.0))
                    self.emergence_metrics['per_shell_energy'] = dict(summary.get('per_shell', {}))
                except Exception:
                    pass

            # Update ray metrics (simplified - would need actual ray tracking)
            # For now, use placeholder values
            self.emergence_metrics['ray_success_rate'] = 0.8  # Placeholder
            self.emergence_metrics['mean_ray_latency'] = 0.05  # Placeholder

            # Update rotor activity (simplified)
            rotor_activity = 0.0
            rotor_pressure = 0.0
            if hasattr(self.mind, 'physics') and hasattr(self.mind.physics, 'roots'):
                # Simple measure of rotor spread
                try:
                    rotor_positions = np.array([r for r in self.mind.physics.roots])
                    rotor_activity = float(np.std(rotor_positions))
                    
                    # Calculate rotor pressure from black hole pressure component
                    if hasattr(self.mind, 'black_hole_pressure'):
                        # Approximate rotor pressure as portion of black hole pressure
                        rotor_pressure = float(self.mind.black_hole_pressure * 0.25)  # 25% weighting as in E8 blend
                except Exception:
                    pass
            self.emergence_metrics['rotor_activity'] = rotor_activity
            self.emergence_metrics['rotor_pressure'] = rotor_pressure

            # Update path cache size (placeholder)
            self.emergence_metrics['path_cache_size'] = 0  # Would need actual cache tracking

        except Exception as e:
            self.console.log(f"[EmergenceController] Metrics update failed: {e}")

    def apply_safeguards(self):
        """Apply safety measures to prevent system destabilization."""
        try:
            stability = self._compute_system_stability()

            # Hard floor on field damping
            if self.couplings['field_damping'] < self.safeguard_state['damping_floor']:
                old_damping = self.couplings['field_damping']
                self.couplings['field_damping'] = self.safeguard_state['damping_floor']
                self.console.log(f"[EmergenceController] Applied damping floor: {old_damping:.4f} ? {self.couplings['field_damping']:.4f}")

            # Failure policy implementation
            if stability < 0.3:
                self.safeguard_state['consecutive_failures'] += 1

                if self.safeguard_state['consecutive_failures'] >= 3:
                    if self.emergence_mode == 'assisted':
                        # Assisted mode destabilizing - increase damping, reduce curvature
                        self.couplings['field_damping'] = min(0.5, self.couplings['field_damping'] * 2)
                        self.couplings['curvature_feedback'] = max(0.1, self.couplings['curvature_feedback'] * 0.5)
                        self.console.log("[EmergenceController] Applied assisted mode safeguards")
                    else:
                        # Self mode anemic - temporarily boost couplings
                        self.couplings['rotor_to_source_gain'] = min(0.5, self.couplings['rotor_to_source_gain'] * 2)
                        self.couplings['curvature_feedback'] = min(0.5, self.couplings['curvature_feedback'] * 2)
                        self.safeguard_state['recovery_mode'] = True
                        self.console.log("[EmergenceController] Applied self-emergent recovery boosts")
            else:
                self.safeguard_state['consecutive_failures'] = 0
                if self.safeguard_state['recovery_mode']:
                    # Recovery successful, gradually reduce boosts
                    self.couplings['rotor_to_source_gain'] = max(0.1, self.couplings['rotor_to_source_gain'] * 0.9)
                    self.couplings['curvature_feedback'] = max(0.1, self.couplings['curvature_feedback'] * 0.9)
                    if self.couplings['rotor_to_source_gain'] <= 0.15 and self.couplings['curvature_feedback'] <= 0.15:
                        self.safeguard_state['recovery_mode'] = False
                        self.console.log("[EmergenceController] Recovery mode deactivated")

        except Exception as e:
            self.console.log(f"[EmergenceController] Safeguards application failed: {e}")

    def get_state(self) -> Dict[str, Any]:
        """Get complete emergence controller state."""
        return {
            'mode': self.emergence_mode,
            'couplings': self.get_current_couplings(),
            'annealing': self.get_annealing_status(),
            'metrics': self.get_emergence_metrics(),
            'safeguards': dict(self.safeguard_state),
            'stats': dict(self.emergence_stats)
        }

def bump_temps(memory: 'MemoryManager', node_ids: List[str], amount: float = 0.6):
    # First, apply temperature bumps to the provided nodes using MemoryManager
    for nid in node_ids:
        node = memory.graph_db.get_node(nid)
        if node:
            current_temp = node.get('temperature', 0.1)
            boost = amount / (1 + np.log1p(current_temp))
            memory.spike_temperature(nid, boost)

    # Try to obtain an E8Mind reference from the memory instance. If it's
    # not present, stop after applying temperature bumps — the remaining
    # initialization is optional (world model / causal / hrl wiring).
    mind = getattr(memory, 'mind', None)
    console_local = None
    if mind is None:
        # Nothing more to initialize here.
        return
    console_local = getattr(mind, 'console', getattr(memory, 'console', None))

    # Ensure action_dim is set (use a sensible default when missing).
    mind.action_dim = int(getattr(mind, "ACTION_SIZE_NO_LOCK", 8))

    # Lazily attach a world-model initializer if not already present.
    if getattr(mind, "world_model", None) is None:
        mind.world_model = None
        def _wm_lazy_init(state_dim):
            try:
                wm = StateVAEWorldModel(input_dim=int(state_dim), action_dim=int(mind.action_dim))
                wm.mind_ref = mind  # Set reference for throttling
            except Exception as e:
                if console_local:
                    console_local.log(f"[Attach] World Model init failed: {e}")
                wm = None
            setattr(mind, "world_model", wm)
        setattr(mind, "_wm_lazy_init", _wm_lazy_init)

    if getattr(mind, "causal", None) is None:
        mind.causal = CausalEngine(getattr(mind, "console", console_local))

    if getattr(mind, "hrl", None) is None:
        pe = getattr(mind, "potential_evaluator", None) or _NullPE()
        mind.hrl = HierarchicalController(getattr(mind, "goal_field", None), pe, getattr(mind, "console", console_local))
        mind.hrl.mind_ref = mind

    # semantics/prompts initialization moved into E8Mind.__init__

def _increment_metric(key: str):
    """Module-level safe no-op metric increment helper.

    Some code paths define a local `_increment_metric` and shadow this, but
    other call sites expect a global symbol. Provide a harmless fallback
    to avoid NameError in those paths.
    """
    return None


class MemoryCurvatureFieldManager:
    """
    Curvature is sourced by memory weights. We solve L φ = ρ̃ with ρ̃ = ρ - mean(ρ).
    κ_display = tanh(alpha * (L φ)) for visualization stability.
    """
    def __init__(self, mind):
        self.mind = mind
        self.eps = float(os.getenv("E8_CURVATURE_SOFTENING", "0.01"))
        self.alpha_display = float(os.getenv("E8_CURVATURE_DISPLAY_ALPHA", "2.0"))
        self.reg = float(os.getenv("E8_LAPLACIAN_REGULARIZATION", "1e-8"))
        self.k_splat = int(os.getenv("E8_RHO_K_NEIGHBORS", "8"))
        self.k_graph = int(os.getenv("E8_LAPLACIAN_K", "16"))
        self.r_graph = float(os.getenv("E8_LATTICE_CONNECT_THRESHOLD", "2.0"))
        self.solver_kind = os.getenv("E8_CURVATURE_SOLVER", "sparse").lower()

        self.rho = None          # density
        self.rho_tilde = None    # mean-zero density
        self.phi = None          # potential
        self.kappa = None        # display curvature
        self._lap_cache = None   # sparse or dense Laplacian
        self._last_residual = None
        self._last_neutrality = None
        # stabilizer config
        try:
            self._residual_threshold = float(os.getenv("E8_STABILIZER_RESIDUAL", "5.0"))
        except Exception:
            self._residual_threshold = 5.0
        try:
            self._clamp_temp = float(os.getenv("E8_STABILIZER_TEMP_CLAMP", "0.5"))
        except Exception:
            self._clamp_temp = 0.5
        try:
            self._damp_on_residual = float(os.getenv("E8_STABILIZER_DAMP", "0.05"))
        except Exception:
            self._damp_on_residual = 0.05

    # ---------- Density construction ----------
    def _memory_weight(self, data, stepnum):
        mass = float(data.get("cognitive_mass", 1.0))
        temp = float(data.get("temperature", 0.1))
        age_steps = stepnum - data.get("step", stepnum)
        age_factor = 1.0 / (1.0 + 0.001 * max(0, age_steps))
        return mass * (1.0 + temp) * age_factor

    def _splat_normalized(self, rho, vec8d, weight, k):
        # nearest neighbors on E8 roots KDTree
        dists, idxs = self.mind.physics.roots_kdtree.query(vec8d.reshape(1, -1), k=min(k, len(self.mind.physics.roots)))
        ds = dists[0]; js = idxs[0]
        ks = 1.0 / np.sqrt(ds * ds + self.eps * self.eps)
        ks_sum = ks.sum() + 1e-12
        rho[js] += weight * (ks / ks_sum)

    def build_memory_density_field(self):
        try:
            graph = self.mind.memory.graph_db.graph
            nodes = list(graph.nodes(data=True))
            n_roots = len(self.mind.physics.roots)
            if n_roots == 0 or not nodes:
                self.rho = np.zeros(n_roots, dtype=float); return self.rho

            rho = np.zeros(n_roots, dtype=float)
            stepnum = getattr(self.mind, "step", getattr(self.mind, "stepnum", 0))

            for node_id, data in nodes:
                # Stabilizer: clamp hot nodes' temperature for weight computation
                try:
                    if isinstance(data, dict):
                        data = self.clamp_temperatures(data)
                except Exception:
                    pass
                w = self._memory_weight(data, stepnum)
                emb = self.mind.memory.main_vectors.get(node_id)
                if emb is None:
                    # uniform fallback (rare)
                    rho += w / n_roots
                    continue
                vec8d = self.mind.memory.sdm._get_vec8d(emb)
                if vec8d is None:
                    rho += w / n_roots
                    continue
                self._splat_normalized(rho, vec8d, w, self.k_splat)

            self.rho = rho
            return rho
        except Exception as e:
            if _CURV_CONSOLE: _CURV_CONSOLE.log(f"[CURVATURE] Density build failed: {e}")
            self.rho = np.zeros(len(self.mind.physics.roots), dtype=float)
            return self.rho

    # ---------- Neutrality ----------
    def enforce_neutrality(self):
        if self.rho is None:
            self.rho = np.zeros(len(self.mind.physics.roots), dtype=float)
        rho_mean = self.rho.mean() if self.rho.size else 0.0
        rho_tilde = self.rho - rho_mean
        # force exact neutrality
        drift = rho_tilde.sum()
        if abs(drift) > 1e-12:
            rho_tilde -= drift / max(1, rho_tilde.size)
        self.rho_tilde = rho_tilde
        self._last_neutrality = float(abs(self.rho_tilde.sum()))
        return self.rho_tilde

    # ---------- Stabilizer / Syndrome ----------
    def clamp_temperatures(self, data: dict) -> dict:
        """Clamp per-node temperature in-place-safely for stability."""
        try:
            t = float(data.get("temperature", 0.0))
            if t > self._clamp_temp:
                data["temperature"] = self._clamp_temp
        except Exception:
            pass
        return data

    def project_density(self, rho_vec: np.ndarray) -> np.ndarray:
        """Optional smoothing/projection on ρ to mitigate spikes."""
        try:
            if rho_vec is None or rho_vec.size == 0:
                return rho_vec
            # lightweight moving-average via Laplacian regularization term
            L = self.get_graph_laplacian()
            if hasattr(L, 'shape') and L.shape[0] == rho_vec.size:
                return rho_vec - 0.1 * _L_dot(L, rho_vec)
            return rho_vec
        except Exception:
            return rho_vec

    def residual_sentinel(self, residual: float) -> bool:
        """Return True if residual indicates unstable solve."""
        try:
            return float(residual) > float(self._residual_threshold)
        except Exception:
            return False

    # ---------- Laplacian ----------
    def _build_sparse_laplacian(self):
        X = self.mind.physics.roots  # (n, d)
        n = len(X)
        rows, cols, vals = [], [], []
        # k-NN edges within radius r_graph
        dists, idxs = self.mind.physics.roots_kdtree.query(X, k=min(self.k_graph, n))
        for i, (ds, js) in enumerate(zip(dists, idxs)):
            for d, j in zip(ds[1:], js[1:]):  # skip self
                if d < self.r_graph:
                    w = 1.0 / (d + self.eps)
                    rows += [i, j]; cols += [j, i]; vals += [-w, -w]
        A = coo_matrix((vals, (rows, cols)), shape=(n, n)).tocsr()
        deg = -np.array(A.sum(axis=1)).ravel()
        L = diags(deg) + A
        return L

    def get_graph_laplacian(self):
        if self._lap_cache is not None:
            return self._lap_cache
        n = len(self.mind.physics.roots)
        if n == 0:
            self._lap_cache = coo_matrix((0,0)).tocsr() if _HAS_SCIPY else np.zeros((0,0))
            return self._lap_cache

        if self.solver_kind == "sparse" and _HAS_SCIPY:
            L = self._build_sparse_laplacian()
            # regularize
            L = (L + self.reg * diags(np.ones(L.shape[0])))
            self._lap_cache = L.tocsr()
        else:
            # Dense fallback (small n only)
            L = np.zeros((n, n), dtype=float)
            roots = self.mind.physics.roots
            thr = self.r_graph
            for i in range(n):
                for j in range(i+1, n):
                    d = np.linalg.norm(roots[i] - roots[j])
                    if d < thr:
                        w = 1.0 / (d + self.eps)
                        L[i, j] = L[j, i] = -w
                        L[i, i] += w; L[j, j] += w
            L += self.reg * np.eye(n)
            self._lap_cache = L
        return self._lap_cache

    # ---------- Solvers ----------
    def _cg_dense(self, A, b, tol=1e-6, max_iter=2000):
        x = np.zeros_like(b)
        r = b - A @ x
        z = r / (np.diag(A) + 1e-12)
        p = z.copy()
        rz = float(r @ z)
        for _ in range(max_iter):
            Ap = A @ p
            denom = float(p @ Ap) + 1e-12
            alpha = rz / denom
            x += alpha * p
            r -= alpha * Ap
            if np.linalg.norm(r) < tol: break
            z = r / (np.diag(A) + 1e-12)
            rz_new = float(r @ z)
            beta = rz_new / (rz + 1e-12)
            p = z + beta * p
            rz = rz_new
        return x

    def solve_laplacian_system(self, rho_tilde):
        # Pre-solve hygiene: mild projection of sources
        if rho_tilde is not None:
            rho_tilde = self.project_density(rho_tilde)
        L = self.get_graph_laplacian()
        if self.solver_kind == "sparse" and _HAS_SCIPY:
            # SciPy CG with zero initial guess; handle SciPy versions with rtol/atol vs tol
            try:
                phi, info = scipy_cg(L, rho_tilde, x0=np.zeros_like(rho_tilde), atol=1e-8, rtol=1e-6, maxiter=4000)
            except TypeError:
                # Older SciPy expects 'tol' and may not support 'atol'
                try:
                    phi, info = scipy_cg(L, rho_tilde, x0=np.zeros_like(rho_tilde), tol=1e-6, maxiter=4000)
                except TypeError:
                    # Last resort: minimal args
                    phi, info = scipy_cg(L, rho_tilde)
            if info != 0 and _CURV_CONSOLE:
                _CURV_CONSOLE.log(f"[CURVATURE] sparse CG info={info}")
        else:
            phi = self._cg_dense(L, rho_tilde)
        # Mean-center the static solution
        phi -= phi.mean() if phi.size else 0.0

        # A-only: Everywhen wave mode (retarded component blended with static)
        ew_mode = os.getenv("E8_EW_MODE", "off").lower()
        if ew_mode == "wave":
            try:
                # Lazy init on owning mind for cross-component reuse/telemetry
                if not hasattr(self.mind, "everywhen_wave") or self.mind.everywhen_wave is None:
                    n_roots = len(self.mind.physics.roots) if hasattr(self.mind, 'physics') and hasattr(self.mind.physics, 'roots') else len(phi)
                    self.mind.everywhen_wave = PhotonicEverywhenWave(self.mind, n_roots)
                # Advance wave using current sources (mean-zero)
                phi_wave = self.mind.everywhen_wave.step(rho_tilde if rho_tilde is not None else np.zeros_like(phi))
                # Blend fields for downstream curvature/intensity display
                phi_use = self.mind.everywhen_wave.blend(phi)
                # Telemetry via metrics_log if available
                try:
                    metrics_log("everywhen_wave", {
                        "event": "ew.wave",
                        "nu": float(self.mind.everywhen_wave.nu),
                        "c_eff": float(self.mind.everywhen_wave.c_eff),
                        "damping": float(self.mind.everywhen_wave.damping),
                        "lam_max_est": float(self.mind.everywhen_wave._lam_max_est),
                    })
                    # concise console line for quick tuning
                    try:
                        # Best-effort energy proxy if mantle provides it
                        energy = 0.0
                        try:
                            if hasattr(self.mind, 'field_mantle') and self.mind.field_mantle:
                                st = self.mind.field_mantle.get_shell_field_state(8) or {}
                                energy = float(st.get('energy_density', 0.0))
                        except Exception:
                            energy = 0.0
                        if hasattr(self.mind, 'console') and self.mind.console:
                            self.mind.console.log(
                                f"[EW] c_eff={self.mind.everywhen_wave.c_eff:.3f} zeta={self.mind.everywhen_wave.damping:.3f} nu={self.mind.everywhen_wave.nu:.5f} lam_max≈{self.mind.everywhen_wave._lam_max_est:.3f} energy={energy:.3f}"
                            )
                    except Exception:
                        pass
                except Exception:
                    pass
            except Exception:
                # On any failure, fall back to static
                phi_use = phi
        else:
            phi_use = phi

        # Store chosen potential
        self.phi = phi_use

        # residual
        if self.solver_kind == "sparse" and _HAS_SCIPY:
            res = float(np.linalg.norm((_L_dot(L, phi)) - rho_tilde))
        else:
            res = float(np.linalg.norm(_L_dot(L, phi) - rho_tilde))
        self._last_residual = res

        # Residual sentinel: gently damp wave coupling if unstable
        try:
            if os.getenv("E8_EW_MODE", "off").lower() == "wave" and self.residual_sentinel(res):
                if hasattr(self.mind, 'everywhen_wave') and self.mind.everywhen_wave:
                    w = self.mind.everywhen_wave
                    w.damping = float(min(0.99, w.damping + self._damp_on_residual))
                    if hasattr(self.mind, 'console') and self.mind.console:
                        self.mind.console.log(f"[Stabilizer] residual={res:.3f} -> increase damping to {w.damping:.3f}")
        except Exception:
            pass
        return self.phi

    def compute_curvature_intensity(self):
        if self.phi is None:
            self.kappa = np.zeros(len(self.mind.physics.roots), dtype=float); return self.kappa
        L = self.get_graph_laplacian()
        if self.solver_kind == "sparse" and _HAS_SCIPY:
            k_lin = _L_dot(L, self.phi)
        else:
            k_lin = _L_dot(L, self.phi)
        k_disp = np.tanh(self.alpha_display * k_lin)
        self.kappa = k_disp
        return k_disp

    # expose diagnostics
    @property
    def last_residual(self): return self._last_residual
    @property
    def last_neutrality(self): return self._last_neutrality


class BlackHoleSourceManager:
    """Prevents double-counting: remove pre-BH mass, re-add condensed remnant mass."""
    def __init__(self, curvature_manager: MemoryCurvatureFieldManager):
        self.curv = curvature_manager
        self.bh_active = False
        self._removed = {}  # node_id -> mass_contribution

    def _get_node_record(self, nid):
        """Return node data dict for nid across graph backends (NX-like or mock)."""
        graph = self.curv.mind.memory.graph_db.graph
        # 1) NetworkX-style mapping?
        if hasattr(graph, "nodes"):
            nodes_attr = graph.nodes
            # nodes_attr is a method in the mock, a mapping in NX
            if callable(nodes_attr):
                for _nid, data in nodes_attr(data=True):
                    if _nid == nid:
                        return data or {}
            else:
                try:
                    return nodes_attr[nid]
                except Exception:
                    pass
        # 2) Fallback to MockMemoryManager store
        return getattr(self.curv.mind.memory, "nodes_data", {}).get(nid, {})

    def _has_node(self, nid):
        graph = self.curv.mind.memory.graph_db.graph
        try:
            return (nid in graph) or (nid in getattr(self.curv.mind.memory, "nodes_data", {}))
        except Exception:
            return nid in getattr(self.curv.mind.memory, "nodes_data", {})

    def _splat_norm(self, vec8d, total, k):
        dists, idxs = self.curv.mind.physics.roots_kdtree.query(vec8d.reshape(1, -1), k=min(k, len(self.curv.mind.physics.roots)))
        ds = dists[0]; js = idxs[0]
        ks = 1.0 / np.sqrt(ds * ds + self.curv.eps * self.curv.eps)
        ks_sum = ks.sum() + 1e-12
        self.curv.rho[js] += total * (ks / ks_sum)

    def _desplat_norm(self, vec8d, total, k):
        dists, idxs = self.curv.mind.physics.roots_kdtree.query(vec8d.reshape(1, -1), k=min(k, len(self.curv.mind.physics.roots)))
        ds = dists[0]; js = idxs[0]
        ks = 1.0 / np.sqrt(ds * ds + self.curv.eps * self.curv.eps)
        ks_sum = ks.sum() + 1e-12
        self.curv.rho[js] -= total * (ks / ks_sum)

    def begin_black_hole_event(self, cluster_ids):
        if self.bh_active or self.curv.rho is None: return
        self.bh_active = True
        self._removed.clear()
        graph = self.curv.mind.memory.graph_db.graph
        stepnum = getattr(self.curv.mind, "step", getattr(self.curv.mind, "stepnum", 0))
        total_removed = 0.0

        for nid in cluster_ids:
            if not self._has_node(nid):
                continue
            data = self._get_node_record(nid)  # ← instead of graph.nodes[nid]
            mass = self.curv._memory_weight(data, stepnum)
            self._removed[nid] = mass
            total_removed += mass

            emb = self.curv.mind.memory.main_vectors.get(nid)
            if emb is None: continue
            vec8d = self.curv.mind.memory.sdm._get_vec8d(emb)
            if vec8d is None: continue
            self._desplat_norm(vec8d, mass, self.curv.k_splat)

        if _CURV_CONSOLE:
            _CURV_CONSOLE.log(f"[BH-SOURCE] Removed {total_removed:.3f} from {len(self._removed)} nodes")

    def finalize_black_hole_remnant(self, remnant_id, condensed_mass):
        if not self.bh_active or self.curv.rho is None:
            return
        emb = self.curv.mind.memory.main_vectors.get(remnant_id)

        vec8d = None
        if emb is not None:
            vec8d = self.curv.mind.memory.sdm._get_vec8d(emb)

        if vec8d is None and self._removed:
            # compute barycenter of removed embeddings
            acc, cnt = np.zeros(8, dtype=float), 0
            for nid in self._removed:
                e = self.curv.mind.memory.main_vectors.get(nid)
                if e is None:
                    continue
                v = self.curv.mind.memory.sdm._get_vec8d(e)
                if v is None:
                    continue
                acc += v; cnt += 1
            if cnt > 0:
                vec8d = acc / (np.linalg.norm(acc) + 1e-18)

        if vec8d is not None:
            self._splat_norm(vec8d, condensed_mass, max(4, self.curv.k_splat // 2))

        self.bh_active = False
        self._removed.clear()
        if _CURV_CONSOLE:
            _CURV_CONSOLE.log(f"[BH-SOURCE] Added remnant mass {condensed_mass:.3f} at {remnant_id}")


class E8Mind:
    # Type hints for dynamic attributes
    _train_calls_this_tick: int
    _train_calls_history: deque
    _throttle_initialized: bool
    TRAIN_STEPS_PER_TICK: int
    TRAIN_GLOBAL_RATE: int
    blueprint: List[Dict[str, Any]]
    step_num: int
    
    def _safe_compute_state_dim(self) -> int:
        """
        Calculates the total dimension of the state vector by summing the
        fixed dimensions of its constituent parts.
        """
        # Mood vector size from its class definition
        mood_size = len(MoodEngine(console=None).mood_vector)
    
        # Goal vector size (based on the 4 initial goals)
        goal_size = 4
    
        # Shell attention vector size from its class definition
        shell_att_size = ShellAttention().out_dim
    
        # Dynamics vector size (as constructed in _build_state_vector)
        dynamics_size = 5
    
        return mood_size + goal_size + shell_att_size + dynamics_size


    def _path(self, rel: str) -> str:
        return get_path(rel, self.run_id)

    async def _log_panel(self, title: str, content: str, style: str = "blue"):
        """Async: Render a Rich Panel for high-signal events with console_lock.

        Args:
            title: Panel title (may include emoji and rich markup)
            content: Body text (rich markup allowed)
            style: border/title color name or hex (e.g., "magenta", "#5F9EA0")
        """
        if getattr(self, 'quiet', False):
            try:
                summary = sanitize_line(content, 160)
                self.console.log(f"[panel quiet] {title}: {summary}")
            except Exception:
                pass
            return
        try:
            async with self.console_lock:
                self.console.print(Panel(
                    content,
                    title=f"[bold {style}]{title}[/]" if style else f"[bold]{title}[/]",
                    border_style=style or "white",
                    expand=False,
                ))
        except Exception:
            # Fallback to simple log if panel fails for any reason
            try:
                self.console.log(f"[{title}] {content}")
            except Exception:
                pass

    def _log_panel_nowait(self, title: str, content: str, style: str = "blue"):
        """Schedule _log_panel without awaiting (safe for sync call sites)."""
        if getattr(self, 'quiet', False):
            try:
                summary = sanitize_line(content, 160)
                self.console.log(f"[panel quiet] {title}: {summary}")
            except Exception:
                pass
            return
        try:
            loop = asyncio.get_running_loop()
        except Exception:
            loop = None
        if loop and loop.is_running():
            try:
                asyncio.create_task(self._log_panel(title, content, style))
                return
            except Exception:
                pass
        # Fallback: best-effort direct print without lock
        try:
            self.console.print(Panel(content, title=f"[bold {style}]{title}[/]", border_style=style, expand=False))
        except Exception:
            try:
                self.console.log(f"[{title}] {content}")
            except Exception:
                pass

    async def _broadcast_curvature_frame(self) -> None:
        """Build and broadcast a lightweight curvature sample frame to SSE/WS clients.

        Payload shape:
        {
          "type": "curvature",
          "ts": <unix_ms>,
          "min": float, "max": float,
          "samples": [ {"i": int, "k": float}, ... ]
        }
        """
        try:
            curv = getattr(self, 'curvature_field', None)
            phys = getattr(self, 'physics', None)
            if curv is None or phys is None or not hasattr(phys, 'roots'):
                return
            # Ensure we have a current kappa vector
            if curv.kappa is None:
                try:
                    # Trigger a minimal recompute path
                    curv.build_memory_density_field()
                    curv.enforce_neutrality()
                    curv.solve_laplacian_system(curv.rho_tilde)
                    curv.compute_curvature_intensity()
                except Exception:
                    return
            kappa = np.asarray(curv.kappa).ravel()
            if kappa.size == 0:
                return
            # Subsample deterministically for UI (cap ~240 points)
            n = int(kappa.size)
            cap = int(os.getenv("E8_CURVATURE_STREAM_CAP", "240"))
            if n <= cap:
                idx = np.arange(n, dtype=int)
            else:
                step = max(1, n // cap)
                idx = np.arange(0, n, step, dtype=int)[:cap]
            samp = [{"i": int(i), "k": float(kappa[i])} for i in idx]
            payload = {
                "type": "curvature",
                "ts": int(time.time() * 1000),
                "min": float(np.min(kappa)),
                "max": float(np.max(kappa)),
                "samples": samp,
            }
            text = json.dumps(payload, cls=NumpyEncoder, ensure_ascii=False)
            # SSE push
            try:
                sse_clients = getattr(self, 'sse_clients', None)
                if sse_clients:
                    dead = set()
                    for q in list(sse_clients):
                        try:
                            q.put_nowait(text)
                        except asyncio.QueueFull:
                            dead.add(q)
                    for q in dead:
                        try:
                            sse_clients.discard(q)
                        except Exception:
                            pass
            except Exception:
                pass
            # WS push
            try:
                await self._ws_broadcast_text(text)
            except Exception:
                pass
        except Exception:
            pass

    async def _run_curvature_stream(self, interval_ms: int = None) -> None:
        """Background task: periodically emit curvature frames."""
        try:
            if interval_ms is None:
                try:
                    interval_ms = int(os.getenv("E8_CURVATURE_STREAM_MS", "350"))
                except Exception:
                    interval_ms = 350
            dt = max(50, int(interval_ms)) / 1000.0
            while True:
                await asyncio.sleep(dt)
                # Only bother when someone is listening
                if (getattr(self, 'sse_clients', None) or getattr(self, 'ws_clients', None)):
                    await self._broadcast_curvature_frame()
        except asyncio.CancelledError:
            return
        except Exception:
            # Don't crash the app if streamer fails; it will restart on next boot
            try:
                self.console.log("[Curvature] stream task terminated")
            except Exception:
                pass

    async def _proactive_synthesis_from_geodesics(self):
        """Trigger synthesis along geodesics in high-curvature wells using 8D proximity."""
        import numpy as _np
        if str(os.getenv("E8_GEODESIC_ENABLE", "1")) == "0":
            return
        mantle = getattr(self, "fluid_mantle", None)
        if mantle is None or not hasattr(mantle, "find_high_curvature_regions"):
            return

        topn = int(os.getenv("E8_GEODESIC_TOPN", "1"))
        hotspots = mantle.find_high_curvature_regions(top_n=max(1, topn))
        if not hotspots:
            return
        h8, curv = hotspots[0]

        k = int(os.getenv("E8_GEODESIC_K", "5"))
        cand = []

        # Preferred: proximity engine with shell/8D support
        pe = getattr(self, "proximity_engine", None)
        if pe and hasattr(pe, "find_similar_in_shell"):
            try:
                cand = pe.find_similar_in_shell(h8, dim=8, k=k) or []
            except Exception:
                cand = []

        # Fallback: KD-tree or cosine scan
        if not cand:
            try:
                # light cosine scan over up to 200 nodes to stay cheap
                ids = list(self.memory.main_vectors.keys())
                if len(ids) > 200:
                    ids = ids[:200]
                def _cos(a, b):
                    a = _np.asarray(a); b = _np.asarray(b)
                    return float(_np.dot(a, b) / ((_np.linalg.norm(a) * _np.linalg.norm(b)) + 1e-9))
                for nid in ids:
                    v = self.memory.main_vectors.get(nid)
                    if v is None:
                        continue
                    try:
                        v8p = self.memory.project_to_dim8(v)
                    except Exception:
                        v8p = None
                    if v8p is None or v8p.shape[0] != 8:
                        continue
                    s = _cos(h8, _np.asarray(v8p))
                    cand.append((nid, s))
                cand.sort(key=lambda x: x[1], reverse=True)
                cand = cand[:k]
            except Exception:
                return

        if len(cand) < 2:
            return

        parent_a_id = cand[0][0]
        parent_b_id = cand[1][0]
        if parent_a_id == parent_b_id:
            # try the next candidate
            for nid, _ in cand[2:]:
                if nid != parent_a_id:
                    parent_b_id = nid
                    break
            if parent_a_id == parent_b_id:
                return

        a = self.memory.graph_db.get_node(parent_a_id)
        b = self.memory.graph_db.get_node(parent_b_id)
        if not (a and b):
            return

        try:
            self.console.jsonl(evt="geodesic.pick", step=int(self.step_num), k=k, a=str(parent_a_id), b=str(parent_b_id), curv=float(curv))
        except Exception:
            pass

        await self._run_proactive_insight_synthesis(forced_parents=(a, b))

    def _build_state_vector(self) -> np.ndarray:
        """Constructs the current state vector from all relevant cognitive modules."""
        try:
            mood_vec = np.array(list(self.mood.mood_vector.values()), dtype=np.float32)
        except Exception:
            # Fallback: simple neutral mood vector
            mood_vec = np.zeros(len(MoodEngine(console=None).mood_vector), dtype=np.float32)

        try:
            if getattr(self.goal_field, 'is_initialized', False) and getattr(self.goal_field, 'goals', None):
                goal_activations = np.array([g.get("activation", 0.0) for g in self.goal_field.goals.values()], dtype=np.float32)
            else:
                goal_activations = np.zeros(4, dtype=np.float32)
        except Exception:
            goal_activations = np.zeros(4, dtype=np.float32)

        try:
            shell_att_vec = self.shell_attention.build(self)
        except Exception:
            shell_att_vec = np.zeros(ShellAttention().out_dim, dtype=np.float32)

        try:
            dynamics_vec = np.array([
                getattr(self, '_bh_ma50', 0.0),
                float(getattr(self, 'black_hole_pressure', 0.0) - getattr(self, '_prev_bh', 0.0)),
                float(np.linalg.norm(getattr(self, '_prev_action', np.zeros(1)))),
                0.0,
                0.0
            ], dtype=np.float32)
        except Exception:
            dynamics_vec = np.zeros(5, dtype=np.float32)

        return np.concatenate([
            mood_vec,
            goal_activations,
            shell_att_vec,
            dynamics_vec
        ])

    def e8_project_to_3(self, e8_coords: np.ndarray) -> list:
        """
        Project 8D E8 coordinates to 3D using the quasicrystal projection matrix.
        """
        try:
            if not hasattr(self.physics, 'projection_matrix') or self.physics.projection_matrix is None:
                # Generate projection matrix if not exists
                self.physics.generate_quasicrystal_blueprint()

            # Project 8D -> 3D
            coords_3d = e8_coords @ self.physics.projection_matrix

            # Normalize to unit sphere and scale
            norm = np.linalg.norm(coords_3d)
            if norm > 1e-6:
                coords_3d = coords_3d / norm

            return coords_3d.tolist()

        except Exception as e:
            try:
                self.console.log(f"[E8_PROJ] Error projecting to 3D: {e}")
            except Exception:
                pass
            # Fallback: use first 3 coordinates
            return e8_coords[:3].tolist() if len(e8_coords) >= 3 else [0.0, 0.0, 0.0]

    def get_current_system_state(self) -> Dict[str, Any]:
        """Extract current system state for M20 processing"""
        state = {
            'active_memories': [],
            'processing_queue': [],
            'insight_rate': 0.0
        }
        try:
            if hasattr(self, 'memory') and hasattr(self.memory, 'get_recent_memories'):
                try:
                    state['active_memories'] = self.memory.get_recent_memories(100)
                except Exception:
                    state['active_memories'] = []
            if hasattr(self, 'scheduler'):
                queue_obj = getattr(self.scheduler, 'queue', None)
                if queue_obj is not None:
                    try:
                        state['processing_queue'] = list(queue_obj)
                    except Exception:
                        try:
                            state['processing_queue'] = list(getattr(queue_obj, 'queue', []))
                        except Exception:
                            state['processing_queue'] = []
            if hasattr(self, 'insight_agent'):
                rewards = getattr(self.insight_agent, 'recent_rewards', [])
                if rewards:
                    recent = list(rewards)[-10:] if hasattr(rewards, '__iter__') else []
                    if recent:
                        state['insight_rate'] = float(np.mean(recent))
        except Exception as exc:
            try:
                self.console.log(f"[M20] Failed to get system state: {exc}")
            except Exception:
                pass
        return state

    def apply_manifold_action(self, action_vec):
        try:
            for lay in ACTION_LAYOUT:
                dim = lay["dim"]
                b0, blen, ai = lay["biv_start"], lay["biv_len"], lay["angle_idx"]
                bcoef = np.asarray(action_vec[b0:b0+blen], dtype=float)
                # Normalize bivector coefficients to a unit direction (avoid norm clamps)
                bn = np.linalg.norm(bcoef)
                if bn > 1e-12:
                    bcoef = (bcoef / bn).astype(np.float32)
                else:
                    bcoef = bcoef.astype(np.float32)
                # Angle gating: map raw angle into bounded range via tanh
                ang_raw = float(action_vec[ai]) if ai < len(action_vec) else 0.0
                ang = float(E8_MAX_SPIN_ANGLE * math.tanh(ang_raw))
                shell = self.dimensional_shells.get(dim)
                if shell is not None and hasattr(shell, "spin_with_bivector"):
                    # Schedule geometry operations in EventHorizonScheduler so async path isn't blocked
                    try:
                        loop = asyncio.get_running_loop()
                        # non-blocking schedule
                        loop.create_task(self.ehs.submit('SHELL_SPIN', dim=dim, bcoef=bcoef, angle=float(ang)))
                        loop.create_task(self.ehs.submit('UPDATE_SHELL_INDEX', dim=dim))
                    except RuntimeError:
                        # No running loop: run in background thread
                        import threading
                        threading.Thread(target=lambda: asyncio.run(self.ehs.submit('SHELL_SPIN', dim=dim, bcoef=bcoef, angle=float(ang))), daemon=True).start()
                        threading.Thread(target=lambda: asyncio.run(self.ehs.submit('UPDATE_SHELL_INDEX', dim=dim)), daemon=True).start()

                    try:
                        if hasattr(self, 'macro_manager') and self.macro_manager is not None:
                            self.macro_manager.on_action_executed(action_vec)
                    except Exception:
                        pass
        except Exception as e:
            try:
                self.console.log(f"[bold red]Error in apply_manifold_action: {e}[/bold red]")
            except Exception:
                pass
    
    async def get_embedding(self, text: str) -> np.ndarray:
        return np.zeros(1536)  # Placeholder
    
    def _init_training_throttle(self):
        """Initialize RL training throttle counters.
        Env vars:
          E8_TRAIN_STEPS_PER_TICK: max world-model train batches per cognitive step (default 1)
          E8_TRAIN_GLOBAL_RATE: max total train calls per N steps (optional)
        """
        if getattr(self, "_throttle_initialized", False):
            return
        self.TRAIN_STEPS_PER_TICK = int(os.getenv("E8_TRAIN_STEPS_PER_TICK", "1") or 1)
        self.TRAIN_GLOBAL_RATE = int(os.getenv("E8_TRAIN_GLOBAL_RATE", "0") or 0)  # 0 disables
        self._train_calls_this_tick = 0
        self._train_calls_history = deque(maxlen=TRAINING_HISTORY_MAXLEN)
        self._throttle_initialized = True

    def _reset_tick_train_counter(self):
        """Reset per-tick train counter (kept as class method)."""
        try:
            self._train_calls_this_tick = 0
        except Exception:
            # Ensure attribute exists even if throttle not initialized
            try:
                self._train_calls_this_tick = 0
            except Exception:
                pass
    
    async def seed_domain_if_empty(self):
        """
        If memory is empty, add one bootstrap concept using the semantic domain.
        Controlled by E8_SEED_DOMAIN flag.
        """
        try:
            # Count existing vectors across dimensional shells
            total = 0
            ds = getattr(self, "dimensional_shells", {}) or {}
            for shell in ds.values():
                try:
                    mat, _ = shell.get_all_vectors_as_matrix()
                    total += (len(mat) if mat is not None else 0)
                except Exception:
                    pass
            if total > 0:
                return  # already has content
            if not E8_SEED_DOMAIN:
                return
            label = E8_SEED_LABEL.strip() or f"domain:{getattr(self, 'semantic_domain', 'experience')}"
            entry = {
                "type": "seed",
                "label": label,
                "metaphor": "bootstrap seed",
                "rating": 0.6,
                "step": getattr(self, "step_num", 0),
                "source": "seed"
            }
            try:
                await self.memory.add_entry(entry)
                self.console.log(f"[seed] Inserted initial concept: {label}")
            except Exception as _e:
                self.console.log(f"[seed] Failed to insert seed concept: {_e}")
        except Exception as e:
            try:
                self.console.log(f"[seed] Error during seed check: {e}")
            except Exception:
                pass

    def _vae_ingest(self, vec):
        """Ingest embeddings into VAE training buffer."""
        if not getattr(self, "autoencoder", None) or vec is None:
            return
        try:
            v = np.asarray(vec, dtype=np.float32).reshape(1, -1)
            self._vae_buf.append(v)
            if len(self._vae_buf) > self._vae_buf_cap:
                self._vae_buf = self._vae_buf[-self._vae_buf_cap:]
        except Exception:
            pass

    def _vae_train_if_ready(self, stepnum: int):
        """Train VAE on cadence if buffer is ready."""
        if not getattr(self, "autoencoder", None):
            return
        if self._vae_train_every <= 0 or (stepnum % self._vae_train_every) != 0:
            return
        n = len(self._vae_buf)
        if n == 0 or (n < self._vae_min and not self._vae_partial):
            return
        take = min(self._vae_batch, n)
        batch = np.concatenate(self._vae_buf[-take:], axis=0).astype(np.float32)
        try:
            # Torch or NumPy path (the class you have supports both)
            try:
                import torch
                losses = self.autoencoder.train_on_batch(torch.from_numpy(batch))
            except Exception:
                losses = self.autoencoder.train_on_batch(batch)
            self._last_vae_losses = losses
            if E8_VAE_TELEM:
                # Enhanced VAE logging with new metrics
                if E8_VAE_ENHANCED_LOGGING and "current_beta" in losses:
                    self.console.log(
                        f"🧠 VAE ENHANCED — loss={losses.get('total_loss',0):.4f} "
                        f"recon={losses.get('recon_loss',0):.4f} kld={losses.get('kld_loss',0):.4f} "
                        f"β={losses.get('current_beta',0):.3f} step={losses.get('step_count',0)}"
                    )
                else:
                    # Fallback to standard logging
                    self.console.log(
                        f"🧠 VAE TRAINED — loss={losses.get('total_loss',0):.4f} "
                        f"recon={losses.get('recon_loss',0):.4f} kld={losses.get('kld_loss',0):.4f}"
                    )
        except Exception as e:
            self.console.log(f"[VAE][ERR] train: {e}")

    def _vae_project_vec(self, vec):
        """Return a denoised/latent vector using whatever API the class exposes."""
        ae = getattr(self, "autoencoder", None)
        if ae is None:
            return vec
        v = np.asarray(vec, dtype=np.float32).reshape(1, -1)
        # Try common method names in your codebase
        for meth in ("project", "project_to_dim"):
            fn = getattr(ae, meth, None)
            if fn:
                try:
                    out = fn(v)
                    return out.reshape(-1)
                except Exception:
                    pass
        # Fallback: decode(encode(mu)) if available
        try:
            if hasattr(ae, "encode") and hasattr(ae, "decode"):
                import torch
                mu, _ = ae.encode(torch.from_numpy(v).float().to(getattr(ae, "device", "cpu")))  # type: ignore
                x_hat = ae.decode(mu).detach().cpu().numpy()
                return x_hat.reshape(-1)
        except Exception:
            pass
        return vec

    def _vae_rerank_scores(self, query_vec, cand_vecs, scores, weight=0.3):
        """Rerank scores using VAE latent similarity."""
        if not getattr(self, "autoencoder", None):
            return scores
        try:
            Q = self._vae_project_vec(query_vec).reshape(1, -1)
            C = np.vstack([self._vae_project_vec(c) for c in cand_vecs]).astype(np.float32)
            Qn = Q / (np.linalg.norm(Q, axis=1, keepdims=True) + 1e-9)
            Cn = C / (np.linalg.norm(C, axis=1, keepdims=True) + 1e-9)
            lat = (Cn @ Qn.T).reshape(-1)
            return (1 - weight) * scores + weight * lat
        except Exception:
            return scores

    def shell_radius(self, shell_dim: int) -> float:
        """
        Calculate shell radius based on dimensional shell.
        Higher dimensions get larger radii for proper spacing.
        """
        try:
            # Base radius increases with shell dimension
            base_radius = 1.0 + (shell_dim - 8) * 0.5

            # Add some variation based on shell activity
            if hasattr(self, 'dimensional_shells') and shell_dim in self.dimensional_shells:
                shell = self.dimensional_shells[shell_dim]
                if hasattr(shell, 'vectors') and shell.vectors:
                    activity_factor = len(shell.vectors) * 0.1
                    base_radius += activity_factor

            return max(base_radius, 0.5)  # Minimum radius

        except Exception as e:
            self.console.log(f"[E8_PROJ] Error calculating shell radius: {e}")
            return 1.0

    # Backwards-compatibility alias for older call sites
    def shellRadius(self, shell_dim: int) -> float:
        """Compatibility wrapper for older code that calls shellRadius()."""
        try:
            return self.shell_radius(shell_dim)
        except Exception:
            # If something goes wrong, return a safe default
            try:
                return float(getattr(self, 'default_shell_radius', 1.0))
            except Exception:
                return 1.0

    def e8_register_point(self, e8_coords: np.ndarray, node_id: str, shell_dim: int = 8) -> dict:
        """
        Register an E8 point for tetra visualization.
        Projects 8D coordinates to 3D and calculates shell-based radial placement.
        """
        try:
            # Ensure we have 8D coordinates
            if e8_coords.shape[0] != 8:
                # Pad or truncate to 8D if necessary
                coords_8d = np.zeros(8, dtype=np.float32)
                size_to_copy = min(len(e8_coords), 8)
                coords_8d[:size_to_copy] = e8_coords[:size_to_copy]
            else:
                coords_8d = e8_coords.astype(np.float32)

            # Find nearest E8 root for blueprint location
            root_index = self.physics.find_nearest_root_index(coords_8d)
            if root_index is None:
                root_index = 0

            # Project to 3D using the physics quasicrystal projection
            coords_3d = self.e8_project_to_3(coords_8d)

            # Calculate shell radius for radial placement
            shell_radius = self.shell_radius(shell_dim)

            # Calculate rotor axis for spinning dynamics
            rotor_axis = self.calculate_rotor_axis(coords_8d)

            tetra_data = {
                "node_id": node_id,
                "e8_coords": coords_8d.tolist(),
                "coords_3d": coords_3d,
                "blueprint_location_id": root_index,
                "shell_dim": shell_dim,
                "shell_radius": shell_radius,
                "rotor_axis": rotor_axis,
                "timestamp": time.time()
            }

            return tetra_data

        except Exception as e:
            self.console.log(f"[E8_PROJ] Error registering point {node_id}: {e}")
            return {
                "node_id": node_id,
                "e8_coords": e8_coords.tolist() if hasattr(e8_coords, 'tolist') else list(e8_coords),
                "coords_3d": [0.0, 0.0, 0.0],
                "blueprint_location_id": 0,
                "shell_dim": shell_dim,
                "shell_radius": 1.0,
                "rotor_axis": [0.0, 1.0, 0.0],
                "timestamp": time.time()
            }

    def _snap_to_lattice(self, vector: np.ndarray, dim: int) -> np.ndarray:
        try:
            q = (os.getenv("E8_QUANTIZER", "e8") or "e8").lower()
            cell = float(os.getenv("E8_CELL", "0.10"))
        except Exception:
            q, cell = "e8", 0.10
        # Override hook: may be a callable or a string designating forced mode
        override = getattr(self, "_quantizer_override", None)
        if override is not None:
            if callable(override):
                try:
                    return np.asarray(override(vector), dtype=np.float32)
                except Exception as e:
                    try:
                        self.console.log(f"[Quantizer] callable override error: {e}; reverting to mode '{q}'")
                    except Exception:
                        pass
            elif isinstance(override, str):
                q = override.lower().strip() or q  # force mode
        v = vector.astype(np.float32, copy=False)
        if q == "none":
            return v
        if q == "cubic":
            return (np.round(v / cell) * cell).astype(np.float32)
        if q == "random":
            try:
                seed = int(os.getenv("GLOBAL_SEED", "1337"))
            except Exception:
                seed = 1337
            rng = np.random.default_rng(seed)
            jitter = rng.normal(0.0, cell, size=v.shape).astype(np.float32)
            w = v + jitter
            return (np.round(w / cell) * cell).astype(np.float32)

        kdtree = getattr(self, 'shell_kdtree_indices', {}).get(dim)
        if kdtree is None:
            return v

        _, nearest_index_arr = kdtree.query(vector.reshape(1, -1), k=1)

        try:
            scalar_index = nearest_index_arr.item()
            return self.shell_lattices[dim][scalar_index]
        except (ValueError, IndexError):
            return vector

    def _update_cognitive_modules(self, step: int):
        """Updates all core cognitive modules that evolve over time."""
        try:
            self.mood.update()
        except Exception:
            pass
        try:
            self.subconscious.decay(step)
        except Exception:
            pass
        try:
            self.goal_field.decay()
            self.goal_field.update_from_mood(self.mood.mood_vector)
        except Exception:
            pass
        try:
            self.memory.diffuse_field()
        except Exception:
            pass
        try:
            self._update_black_hole_pressure()
        except Exception:
            pass
        try:
            self.memory.decay_locks()
        except Exception:
            pass
        try:
            self.scheduler.tick(step)
        except Exception:
            pass

    def _train_autoencoder_if_ready(self, autoencoder_train_buffer: list, batch_size: int) -> list:
        """Trains the VAE on a batch of new embeddings if the buffer is full."""
        if TORCH_AVAILABLE and getattr(self, 'autoencoder', None) and getattr(self.memory, 'pending_embeddings', None):
            autoencoder_train_buffer.extend(self.memory.pending_embeddings)
            self.memory.pending_embeddings.clear()

            if len(autoencoder_train_buffer) >= batch_size:
                batch_np = np.array(autoencoder_train_buffer[:batch_size])
                autoencoder_train_buffer = autoencoder_train_buffer[batch_size:]

                try:
                    losses = self.autoencoder.train_on_batch(torch.from_numpy(batch_np).float())
                    content = (
                        f"Loss: [bold yellow]{losses['total_loss']:.4f}[/] | "
                        f"Recon: {losses['recon_loss']:.4f} | KLD: {losses['kld_loss']:.4f}"
                    )
                    self.console.print(Panel(content, title="[bold #5F9EA0]🧠 VAE TRAINED[/]", border_style="#5F9EA0"))
                except Exception as e:
                    self.console.log(f"[bold red]VAE Training Error: {e}[/bold red]")
        return autoencoder_train_buffer

    async def _update_synaptic_plasticity(self):
        """Periodically update the connectivity potential of all nodes."""
        try:
            self.console.print(Panel(
                "Reinforcing successful nodes and applying decay to memory connections.",
                title="[bold #5F9EA0]🧠 SYNAPTIC PLASTICITY[/]",
                border_style="#5F9EA0"
            ))
        except Exception:
            pass
        try:
            # --- 1. Get Global Modulators from AI state ---
            current_goal = getattr(self, 'current_goal_name', 'STABILITY')
            mood = getattr(self, 'mood', None)
            global_modulator = 1.0
            if current_goal == "NOVELTY":
                global_modulator *= 1.15
            elif current_goal == "STABILITY":
                global_modulator *= 0.85

            mood_entropy = float(mood.mood_vector.get('entropy', 0.5)) if mood and hasattr(mood, 'mood_vector') else 0.5
            global_modulator *= (1.0 + (mood_entropy - 0.5) * 0.2)

            # --- 2. Reinforce Successful Nodes ---
            successful_nodes = [
                node_id for node_id, data in self.memory.graph_db.graph.nodes(data=True)
                if data.get("type") in ["insight_synthesis", "explorer_insight", "meta_reflection"]
                and data.get("step", 0) > (getattr(self, 'step_num', 0) - 500)
                and data.get("rating", 0) > 0.9
            ]

            max_delta = float(os.getenv('E8_PLASTICITY_MAX_DELTA', '0.15'))
            for node_id in successful_nodes:
                node = self.memory.graph_db.get_node(node_id)
                if node:
                    before = node.get('connectivity_potential', self.memory.INITIAL_POTENTIAL)
                    try:
                        update_node_potential(node, 0.9, global_modulator)
                    except Exception:
                        pass
                    after = node.get('connectivity_potential', before)
                    if after - before > max_delta:
                        node['connectivity_potential'] = before + max_delta

            # --- 3. Apply Decay to a Sample of Other Nodes ---
            DECAY_RATE = 0.005
            all_nodes = list(self.memory.graph_db.graph.nodes(data=True))
            if len(all_nodes) > 100:
                nodes_to_decay = random.sample(all_nodes, 100)
            else:
                nodes_to_decay = all_nodes

            for node_id, node_data in nodes_to_decay:
                if node_id not in successful_nodes:
                    current_potential = node_data.get('connectivity_potential', self.memory.INITIAL_POTENTIAL)
                    decay_amount = (DECAY_RATE * (2.0 - global_modulator))
                    if decay_amount > max_delta:
                        decay_amount = max_delta
                    new_potential = max(0.0, current_potential - decay_amount)
                    node_data['connectivity_potential'] = new_potential
        except Exception:
            # Do not let plasticity update crash main loop
            try:
                self.console.log("[Plasticity] update failed")
            except Exception:
                pass
    
    def __init__(self, semantic_domain_val, run_id, llm_client_instance, client_model, embedding_model_name, embed_adapter, embed_in_dim, console: Console, is_embed_placeholder: bool):
        self.console = console
        self.run_id = run_id
        self.is_embed_placeholder = is_embed_placeholder
        try:
            self.llm_provider = globals().get('LLM_PROVIDER', os.getenv('E8_PROVIDER', 'stub'))
        except Exception:
            self.llm_provider = os.getenv('E8_PROVIDER', 'stub')
        try:
            quiet_val = str(os.getenv('E8_QUIET', '0')).strip().lower()
            self.quiet = quiet_val in ('1', 'true', 'yes', 'on')
        except Exception:
            self.quiet = False

        # Initialize Journey Logger for unified tracking
        self.journey_logger = JourneyLogger(
            log_file=get_path(E8_JOURNEY_LOGGER_FILE, run_id),
            buffer_size=E8_JOURNEY_LOGGER_BUFFER_SIZE
        )

        self.state_dim = self._safe_compute_state_dim()
        self.metrics = MetricsManager(self.run_id)
        self._quantizer_override = None
        self._last_cb_arm_idx = None
        self.snapshot_every = int(os.getenv('E8_SNAPSHOT_EVERY', str(CONSOLE_EXPORT_EVERY_STEPS)))
        self._last_hourly_metrics_ts = time.time()
        self.novelty_weight = 0.35
        self._coh_roll = deque(maxlen=200)
        # Task slots and per-step fire ledger to avoid overlaps & duplicate fires
        try:
            import types as _types  # local alias for SimpleNamespace
            self.slots = _types.SimpleNamespace(
                teacher=_TaskSlot("teacher"),
                explorer=_TaskSlot("explorer"),
                dream=_TaskSlot("dream"),
                insight=_TaskSlot("insight"),
                decay=_TaskSlot("decay"),
                subnarr=_TaskSlot("subnarr"),
                snapshot=_TaskSlot("snapshot"),
            )
        except Exception:
            # Fallback if types is unavailable for some reason
            class _Slots: pass
            self.slots = _Slots()
            self.slots.teacher = _TaskSlot("teacher")
            self.slots.explorer = _TaskSlot("explorer")
            self.slots.dream = _TaskSlot("dream")
            self.slots.insight = _TaskSlot("insight")
            self.slots.decay = _TaskSlot("decay")
            self.slots.subnarr = _TaskSlot("subnarr")
            self.slots.snapshot = _TaskSlot("snapshot")

        # Per-step fired keys (teacher/explorer/dream/insight/etc.) to guard duplicate scheduling
        if not hasattr(self, "_fired_step"):
            self._fired_step = {}

        # Cap concurrent validators to avoid fan-out overloads
        try:
            self._validator_sem = asyncio.Semaphore(int(os.getenv("E8_MAX_CONCURRENT_VALIDATORS", "3")))
        except Exception:
            self._validator_sem = asyncio.Semaphore(3)


        self.bandit = ContextBandit(
            arms=[
                {"angle_vec": 0.15, "shell_mix": (0.2,0.3,0.5), "k": 24, "diffusion_sigma": 1.2, "hub_penalty": 0.0},
                {"angle_vec": 0.25, "shell_mix": (0.4,0.2,0.4), "k": 32, "diffusion_sigma": 1.5, "hub_penalty": 0.03},
                {"angle_vec": 0.10, "shell_mix": (0.1,0.2,0.7), "k": 40, "diffusion_sigma": 1.0, "hub_penalty": 0.06},
            ],
            state_dim=self.state_dim,
            path_json=get_path("bandit_state.json", self.run_id),
        )
        self._last_cb_reward = 0.0

        self.console.rule(f"[bold cyan]Initializing E8 Mind | Run ID: {run_id}[/]")
        os.makedirs(os.path.join(RUNTIME_DIR, self.run_id), exist_ok=True)

        self.proximity_log_path = get_path("logs/proximity_alerts.ndjson", self.run_id)
        os.makedirs(os.path.dirname(self.proximity_log_path), exist_ok=True)
        self._prox_lock = asyncio.Lock()
        # Lock used to serialize insight-cycle execution
        try:
            self.insight_cycle_lock = asyncio.Lock()
        except Exception:
            # Fallback: ensure attribute exists even if asyncio not available
            self.insight_cycle_lock = None

        # Console logging lock (many code paths use it with 'async with').
        # Use an asyncio.Lock where possible so 'async with self.console_lock' works.
        try:
            self.console_lock = asyncio.Lock()
        except Exception:
            # If asyncio is unavailable, fall back to a simple threading.Lock but
            # wrap it in an async-compatible context manager so 'async with' won't fail.
            try:
                _thread_lock = threading.Lock()

                class _AsyncWrapper:
                    """Wrap a threading.Lock to behave as an async context manager."""
                    def __init__(self, lock):
                        self._lock = lock

                    async def __aenter__(self):
                        # Acquire lock in threadpool to avoid blocking event loop
                        loop = asyncio.get_event_loop()
                        await loop.run_in_executor(None, self._lock.acquire)
                        return self

                    async def __aexit__(self, exc_type, exc, tb):
                        loop = asyncio.get_event_loop()
                        await loop.run_in_executor(None, self._lock.release)

                self.console_lock = _AsyncWrapper(_thread_lock)
            except Exception:
                # Last resort: ensure attribute exists to avoid AttributeError
                self.console_lock = None

        self.market_enabled = bool(int(os.getenv("E8_MARKET_FEED_ENABLED", "0")))
        self.market_symbols = [s.strip().upper() for s in os.getenv("MARKET_SYMBOLS", "AAPL,MSFT,SPY").split(",") if s.strip()]
        finnhub_key = os.getenv("FINNHUB_KEY", "")
        self.market = None
        if self.market_enabled and finnhub_key and 'websockets' in globals() and globals()['websockets'] is not None:
            try:
                self.market = MarketFeed(self.market_symbols, finnhub_key, self._on_market_tick, self._on_market_bar)
                self.market.start()
                self.console.log(f"[INIT] Market feed started for symbols: {self.market_symbols}")
            except Exception as e:
                self.console.log(f"[yellow][INIT] Market feed failed: {e}[/yellow]")
                self.market = None
        self.market_last: Dict[str, float] = {}
        self.market_last_bar: Dict[Tuple[str, str], Bar] = {}

        self.probe = Probe(run_id)
        set_asyncio_exception_logger(self.probe)

        self.llm_client = llm_client_instance
        self._recent_texts = deque(maxlen=500)
        self._recent_norms = deque(maxlen=500)
        self._anti_repeat_enabled = True

        self.local_llm_client: Optional[OllamaClient] = None
        self.local_llm_model = 'phi3:mini-4k'
        self.client_model = client_model
        self.embedding_model = embedding_model_name
        self.semantic_domain = semantic_domain_val
        self.llm_pool = AsyncLLMPool(self, worker_count=max(1, LOCAL_GEN_WORKERS))
        self._insight_cycle_pending = False
        self._last_insight_cycle_step = -1
        self._last_proximity_log_step = -1000
        self._last_proximity_log_distance = None
        # Proximity telemetry buffers for frontend compatibility (M230)
        try:
            max_events = int(os.getenv("E8_PROX_MAX_EVENTS", "30") or 30)
        except Exception:
            max_events = 30
        self._proximity_events = deque(maxlen=max_events)
        self._last_proximity_event = None
        self._rating_model_stats: Dict[str, Dict[str, float]] = {}
        try:
            self._rating_model_stats = safe_json_read(get_path("rating_model_stats.json", self.run_id)) or {}
        except Exception:
            self._rating_model_stats = {}
        try:
            self._rating_model_stats = safe_json_read(get_path("rating_model_stats.json", self.run_id)) or {}
        except Exception:
            self._rating_model_stats = {}
        try:
            self._rating_model_stats = safe_json_read(get_path("rating_model_stats.json", self.run_id)) or {}
        except Exception:
            self._rating_model_stats = {}
        self.embed_adapter = embed_adapter
        self.embed_in_dim = embed_in_dim

        # Profile name and semantics/prompts initialization (ensure E8Mind instances always have them)
        try:
            self.profile_name = os.getenv("MIND_PROFILE", getattr(self, 'profile_name', 'default'))
        except Exception:
            self.profile_name = getattr(self, 'profile_name', 'default')
        try:
            sem_plugin, prompts = load_profile(self.profile_name)
            self.prompts = getattr(self, 'prompts', prompts)

            # Instantiate the semantics class and attach the mind instance to it
            self.semantics = sem_plugin.PLUGIN()
            if hasattr(self.semantics, 'attach_mind'):
                self.semantics.attach_mind(self)
        except Exception as e:
            try:
                self.console.log(f"[INIT] Failed to load profile semantics/prompts: {e}; using fallback instances")
            except Exception:
                pass
            if getattr(self, 'semantics', None) is None:
                # Inline minimal fallback semantics (avoid referencing load_profile-local class)
                class __LocalFallbackSem:
                    name = "default"
                    base_domain = getattr(self, 'semantic_domain', None) or SEMANTIC_DOMAIN
                    def persona_prefix(self, mood):
                        intensity = (mood or {}).get('intensity', 0.5)
                        entropy = (mood or {}).get('entropy', 0.5)
                        coherence = (mood or {}).get('coherence', 0.5)
                        if entropy > 0.7 and intensity > 0.6:
                            return "You are feeling chaotic, fragmented, and electric."
                        elif coherence > 0.75:
                            return "You are feeling exceptionally clear, logical, and focused."
                        elif intensity < 0.3:
                            return "You are feeling calm, quiet, and introspective."
                        else:
                            return "You are in a balanced and considered state of mind."
                    def pre_embed(self, t):
                        base = getattr(self, "base_domain", None)
                        if base and isinstance(t, str):
                            return f"{base}: {t}"
                        return t
                    def post_embed(self, v):
                        try:
                            n = float(np.linalg.norm(v))
                            return v / n if n > 1e-9 else v
                        except Exception:
                            return v
                    def rerank(self, c):
                        return c
                self.semantics = __LocalFallbackSem()
            if getattr(self, 'prompts', None) is None:
                class _P:
                    def render(self, *a, **k):
                        return ""
                self.prompts = _P()

        self.physics = E8Physics(self.console)
        self.e8_physics = self.physics

        # Ensure shells/lattice indices exist even if not yet populated. Some
        # code paths call into `self.shell_kdtree_indices` or `self.shell_lattices`
        # before they are populated; initialize empty containers to avoid
        # AttributeError and let later code populate them when available.
        try:
            if not hasattr(self, 'shell_lattices') or self.shell_lattices is None:
                self.shell_lattices = {}
        except Exception:
            self.shell_lattices = {}
        try:
            if not hasattr(self, 'shell_kdtree_indices') or self.shell_kdtree_indices is None:
                self.shell_kdtree_indices = {}
        except Exception:
            self.shell_kdtree_indices = {}

        try:
            self.fabric = E8BoundaryFabric(self.physics)
            self.fabric.layout_2d()
            safe_json_write(self._path("boundary_fabric.json"), self.fabric.to_json())
        except Exception as e:
            self.console.log(f"[INIT] Boundary fabric failed: {e}")
            self.fabric = None

        self.blueprint = self.physics.generate_quasicrystal_blueprint()
        safe_json_write(self._path("quasicrystal_blueprint.json"), self.blueprint)
        self.blueprint_kdtree = KDTree([[p['x'], p['y']] for p in self.blueprint])

        self.action_dim = ACTION_SIZE_NO_LOCK

        if TORCH_AVAILABLE:
            self.agent = SACAgent(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                hidden_sizes=[256, 256],
                actor_lr=3e-4,
                critic_lr=3e-4,
                alpha_lr=3e-4,
                gamma=0.99,
                tau=0.005,
                batch_size=256,
                buffer_size=int(1e6),
                start_steps=1000,
                updates_per_step=1,
                path=get_path("sac_agent", self.run_id),
            )
            self.world_model = StateVAEWorldModel(
                input_dim=self.state_dim,
                action_dim=self.action_dim,
                latent_dim=32,
            )
        else:
            self.agent = None
            self.world_model = None

        self.mood = MoodEngine(self.console)
        self.subconscious = SubconsciousLayer(self.get_embedding, self.llm_pool, self.console, mind=self)
        self.goal_field = GoalField(self.get_embedding, self.console, mind=self)
        self.drives = DriveSystem()
        self.dimensional_shells = {dim: DimensionalShell(dim, self) for dim in DIMENSIONAL_SHELL_SIZES}
        self.proximity_engine = ProximityEngine(shell_dims=DIMENSIONAL_SHELL_SIZES, mind_instance=self, console=self.console)
        self.memory = MemoryManager(self)
        
        # Initialize Memory-Curvature Field Managers (M23)
        try:
            self.curvature_field = MemoryCurvatureFieldManager(self)
            self.black_hole_source_manager = BlackHoleSourceManager(self.curvature_field)
            self.console.log("[INIT] Memory-Curvature field system initialized")
        except Exception as e:
            self.console.log(f"[INIT] Curvature field initialization failed: {e}")
            self.curvature_field = None
            self.black_hole_source_manager = None
            
        # Initialize Systematic Physics Validator for physics-inspired constraints (M24)
        try:
            self.systematic_physics = SystematicPhysicsValidator(self)
            self.console.log("[INIT] Systematic physics validator initialized")
        except Exception as e:
            self.console.log(f"[INIT] Systematic physics validator failed: {e}")
            self.systematic_physics = None
        
        # EventHorizonScheduler: offload blocking geometric operations to worker threads
        try:
            # Number of geo workers configurable via E8_GEO_WORKERS
            n_workers = int(os.getenv('E8_GEO_WORKERS', '2') or 2)
            self.ehs = EventHorizonScheduler(self, n_workers=n_workers)
            self.ehs.start()
        except Exception:
            # Ensure attribute exists for compatibility even if scheduler fails
            self.ehs = None
        # Create a thin Manifold wrapper to own geometry/physics/pubsub and expose
        # a minimal interface for modules (boundary, emit, geom_submit).
        try:
            self.manifold = Manifold(
                physics=getattr(self, 'physics', None),
                loop=None,
                pubsub=getattr(self, 'pubsub', None),
                ehs=self.ehs,
                curvature_params={
                    'black_hole_pressure': getattr(self, 'black_hole_pressure', 0.0),
                    'horizon_visuals': getattr(self, 'horizon_visuals', None)
                }
            )
        except Exception:
            self.manifold = Manifold()
        
        # Initialize Layer 1 Bootstrap: Deterministic Embedding System
        try:
            self.embedding_bootstrap = EmbeddingBootstrap(
                mind_instance=self,
                embedding_dim=EMBED_DIM
            )
            self.console.log("[INIT] Embedding bootstrap system initialized")
        except Exception as e:
            self.console.log(f"[INIT] Embedding bootstrap failed: {e}")
            self.embedding_bootstrap = None
        
        self._autoencoder_train_buffer = []
        
        # --- VAE init & config ---
        self.autoencoder = None
        self._vae_buf = []
        self._vae_buf_cap = E8_VAE_BUFFER_SIZE
        self._vae_batch = E8_VAE_BATCH
        self._vae_min = E8_VAE_MIN_BUFFER
        self._vae_train_every = E8_VAE_TRAIN_EVERY
        self._vae_partial = E8_VAE_TRAIN_PARTIAL
        self._last_vae_losses = None

        if E8_VAE_ENABLE:
            try:
                # infer input dim (or set explicitly)
                embed_dim = int(getattr(self.memory, "embedding_dim", EMBED_DIM))
                layers_env = E8_VAE_LAYERS
                layer_sizes = [int(x) for x in layers_env.split(",")]
                if layer_sizes[0] != embed_dim:
                    self.console.log(f"[VAE][WARN] E8_VAE_LAYERS first dim {layer_sizes[0]} != embed_dim {embed_dim}; fixing.")
                    layer_sizes[0] = embed_dim
                latent = E8_VAE_LATENT
                beta = E8_VAE_BETA
                lr = E8_VAE_LR

                # Instantiate the class already in M24
                self.autoencoder = VariationalAutoencoder(layer_sizes, latent_dim=latent, lr=lr, beta=beta, console=self.console)
                # Store VAE beta for BH Console telemetry
                self._last_vae_beta = beta
                self.console.log(f"[INIT][VAE] ready (layers={layer_sizes}, latent={latent}, beta={beta})")
            except Exception as e:
                self.console.log(f"[INIT][VAE][ERR] {e}")
                self.autoencoder = None
        else:
            try:
                # Fallback to original autoencoder if VAE disabled
                self.autoencoder = Autoencoder(
                    input_dim=EMBED_DIM,
                    hidden_dims=[512, 256, 128],
                    latent_dim=64,
                    lr=1e-3,
                    batch_size=64,
                    path=get_path("autoencoder", self.run_id),
                )
            except Exception:
                self.autoencoder = None
        # Boot-time rotor validation: verify rotor algebra properties early
        try:
            # Prefer a rotor generator object if present
            rg = getattr(self, 'rotor_generator', None)
            validator_owner = rg if rg is not None else self
            if hasattr(validator_owner, 'validate_rotor_properties'):
                try:
                    # Attempt to obtain/generate a small test rotor
                    test_rotor = None
                    if rg is not None and hasattr(rg, 'random_rotor'):
                        try:
                            test_rotor = rg.random_rotor()
                        except Exception:
                            test_rotor = None
                    if test_rotor is None:
                        # fallback to identity-like rotor
                        try:
                            test_rotor = rg.identity() if (rg is not None and hasattr(rg, 'identity')) else 1
                        except Exception:
                            test_rotor = 1

                    val = validator_owner.validate_rotor_properties(test_rotor)
                    try:
                        self.console.log(f"[ROTOR_BOOT] Validation: norm_invariant={val.get('norm_invariant')} double_cover={val.get('double_cover')}")
                    except Exception:
                        pass
                    # Emit simple gauges if metrics present
                    try:
                        if hasattr(self, 'metrics') and hasattr(self.metrics, 'gauge'):
                            self.metrics.gauge('rotor.norm_invariant', 1.0 if val.get('norm_invariant') else 0.0)
                            self.metrics.gauge('rotor.double_cover', 1.0 if val.get('double_cover') else 0.0)
                    except Exception:
                        pass
                except Exception as e:
                    try:
                        self.console.log(f"[ROTOR_BOOT] Rotor validation failed: {e}")
                    except Exception:
                        pass
            else:
                try:
                    self.console.log("[ROTOR_BOOT] Rotor validation API not available; skipping.")
                except Exception:
                    pass
        except Exception:
            try:
                self.console.log("[ROTOR_BOOT] Unexpected error during rotor boot validation; continuing.")
            except Exception:
                pass
        self._macro_manager_async_warning = False
        
        # Initialize training throttle early to avoid race conditions
        self._init_training_throttle()
        
        self.novelty_scorer = NoveltyScorer(self.memory, self.llm_pool, self.console)
        self.insight_agent = InsightAgent(self.llm_pool, self.novelty_scorer, self.console)
        
        # Initialize unified rating system
        self.unified_rating = UnifiedRatingSystem(self.console)
        # Set global reference for fallback compatibility
        global unified_rating_system
        unified_rating_system = self.unified_rating
        self.console.log("[INIT] Unified rating system initialized")

        # Initialize teacher configuration tracking
        import hashlib
        import json
        self.teacher_config = {
            "rating_floor": float(os.getenv("E8_TEACHER_RATING_FLOOR", "0.5")),
            "rating_floor_original": float(os.getenv("E8_TEACHER_RATING_FLOOR", "0.5")),
            "rating_floor_schedule": {
                "enabled": bool(int(os.getenv("E8_TEACHER_FLOOR_SCHEDULE", "1"))),
                "temp_floor": 0.52,  # Temporary higher floor as per review
                "schedule_steps": int(os.getenv("E8_TEACHER_FLOOR_SCHEDULE_STEPS", "500")),
                "current_step": 0
            },
            "model": client_model,
            "semantic_domain": semantic_domain_val,
            "timestamp": time.time()
        }
        # Generate teacher config hash for tracking
        config_str = json.dumps(self.teacher_config, sort_keys=True)
        self.teacher_config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]
        self.teacher_id = f"teacher_{self.teacher_config_hash}_{int(time.time())}"
        self.console.log(f"[INIT] Teacher system initialized - ID: {self.teacher_id}")
        
        # Teacher event log for tracking scores with config
        self.teacher_log = []

        self.new_node_id_queue = deque(maxlen=500)
        self.shell_attention = ShellAttention(out_dim=32, keep_k=3)
        self.arbiter_gate = ArbiterGate()
        self.curriculum = AutoTaskManager(self.console)
        self.dream_engine = DreamEngine(self.memory, self)
        self.dream_replay_service = DreamReplayService(self, batch=int(os.getenv('E8_REPLAY_BATCH','32')), steps=int(os.getenv('E8_REPLAY_STEPS','20')))
        self.narrative_streamer = NarrativeStreamer(self.memory, self.llm_pool, self.run_id)
        self.synthetic_env = SyntheticEnvironment(self.llm_pool, self)
        self.domain_tint = DomainTintEngine(self.semantic_domain, self.llm_pool)
    # Validator will be initialized later with logging and gating awareness
    # (see dedicated Validator initialization block below)
        self.ingestion_pipeline = DataIngestionPipeline(self)
        self.scheduler = CognitiveScheduler(self)
        self.potential_evaluator = StatePotentialEvaluator(self.dimensional_shells, self.goal_field)

        try:
            self.macro_manager = MacroManager(
                state_dim=self.state_dim,
                action_dim=self.action_dim,
                hidden_sizes=[128, 64],
                lr=1e-3,
                batch_size=32,
                path=get_path("macro_manager", self.run_id),
                layout=ACTION_LAYOUT,
            )
        except Exception as e:
            self.macro_manager = None

        self.max_action = 0.1

        self._bh_window = deque(maxlen=50)
        self._bh_recent = deque(maxlen=100)
        self._bh_ma50 = 0.0
        self._prev_bh = 0.0
        self._low_bh_streak = 0
        self._prev_action = np.zeros(self.action_dim, dtype=np.float32)

        # --- BH cadence state (new) ---
        self._bh_last_event_step   = -1
        self._bh_cooldown_dynamic  = int(BH_COOLDOWN_INIT)
        self._bh_event_intervals   = deque(maxlen=16)
        self._bh_degraded_ema      = 0.0
        self._bh_threshold_dynamic = float(BH_PRESSURE_THRESHOLD)  # start from static knob
        self._bh_last_log_step     = -50  # throttle logging
        self._bh_last_logged_pressure = 0.0
        self._bh_last_log_time = 0.0  # for time-based throttling

        if TORCH_AVAILABLE:
            self.qeng = QuantumEngine(self.physics, QuantumConfig(seed=GLOBAL_SEED), self.console)
            self.ceng = ClassicalEngine(self.physics, ClassicalConfig(seed=GLOBAL_SEED), self.console)
            self.anchors = MultiAnchorField(self.physics)
        else:
            # Quantum/classical engines and anchor field are disabled when torch is not present
            self.qeng = None
            self.ceng = None
            self.anchors = None

        # Wavey bridge is optional; only create if the wavey integration is available.
        # Keep it None when Wavey is not desired/available so the rest of the code can skip calls.
        try:
            self.wavey_bridge = WaveyE8Bridge(embed_dim=EMBED_DIM, seed=GLOBAL_SEED) if WAVEY_AVAILABLE else None
        except Exception:
            self.wavey_bridge = None
        self._wavey_bias_last = None
        self.valence = SymmetryValenceEngine(self.physics)
        self.ego_gate = EgoGate(self.valence, min_delta=-0.01)
        self.holo, self.emap, self.slice_stack = HoloEncoder(self.fabric, feat_dim=8), EntropyMap(self.fabric), SliceStack()
        self.judge, self.adapter, self.decoder = Judge(self.llm_client, self.get_embedding, self.emap, self.holo), OnlineAdapter(), ConstrainedDecoder(self.llm_client, self.fabric)
        self.topology_monitor = TopologyMonitor()

        self.step_num, self.max_steps, self.trace = 0, 0, []
        self.prev_node_index: Optional[int] = None
        self.visit = np.zeros(self.physics.roots.shape[0], dtype=np.int32)
        self.ego_summary, self.teacher_question, self.explorer_last_answer, self.last_teacher_question = "Nascent state.", None, "", ""
        self.current_task_embedding = np.zeros(EMBED_DIM)
        self.gravitational_lock_target: Optional[Tuple[str, str]] = None
        self.teacher_log, self.explorer_log, self.subconscious_event_log, self.black_hole_log = [], [], [], []
        self.black_hole_pressure, self._bh_cooldown_until, self._bh_inflight = 0.0, -1, False
        self._last_dream_at, self._last_dream_seed_hash, self._last_dream_step = 0.0, None, -1
        self._progress_lock, self._last_progress_step, self.sigma_q, self.last_policy_state = asyncio.Lock(), -1, 1.25, {}
        self.bardo_until, self._last_region = -1, None
        self.sse_clients = set()
        # Concurrency: snapshot rotation & SSE throttling
        self.snapshot_rotation_lock = asyncio.Lock()
        self._last_sse_sent_ts = 0.0
        self._sse_min_interval_ms = int(os.getenv("E8_SSE_MIN_INTERVAL_MS", "200"))
        try:
            field_mantle = HyperdimensionalFieldMantle(
                mind=self,
                core_dimensions=8,
                max_dimensions=248,
                lattice_points=getattr(self.e8_physics, 'roots', [])
            )
            self.field_mantle = field_mantle
            self.fluid_mantle = field_mantle
            self.valence_engine = DynamicValenceEngine(self.e8_physics, self.fluid_mantle)
            self.energy_navigator = EnergyLandscapeNavigator(self.fluid_mantle, self.e8_physics)
            self.topology_engine = AdaptiveTopologyEngine(self.e8_physics)
            self.recursive_architect = RecursiveArchitectureEngine(self.insight_agent, self.console)
            self.m20_metrics = {
                'field_steps': 0,
                'valence_updates': 0,
                'energy_explorations': 0,
                'topology_adaptations': 0,
                'recursive_implementations': 0,
                'spacetime_curvature': 0.0,
                'poynting_flux': 0.0,
                'field_energy': 0.0,
                'field_pressure_proxy': 0.0
            }
            self.dynamic_valence = self.valence_engine
            self.energy_landscape_navigator = self.energy_navigator
            self.architecture_engine = self.recursive_architect
            self.console.log("[M20] Electromagnetic field mantle initialized successfully")
        except Exception as e:
            self.console.log(f"[M20] Warning: Failed to initialize M20 components: {e}")
            self.field_mantle = None
            self.fluid_mantle = None
            self.valence_engine = None
            self.energy_navigator = None
            self.topology_engine = None
            self.recursive_architect = None
            self.dynamic_valence = None
            self.energy_landscape_navigator = None
            self.architecture_engine = None
            self.m20_metrics = {
                'field_steps': 0,
                'valence_updates': 0,
                'energy_explorations': 0,
                'topology_adaptations': 0,
                'recursive_implementations': 0,
                'spacetime_curvature': 0.0,
                'poynting_flux': 0.0,
                'field_energy': 0.0,
                'field_pressure_proxy': 0.0
            }

        # Validator initialization
        self.validator = None
        try:
            # Assuming HypothesisValidator is defined in this file or accessible directly
            from e8_mind_server_M24 import HypothesisValidator # Adjust import path if HypothesisValidator is in a different module
            self.validator = HypothesisValidator(self)
            # Log comprehensive validator configuration
            config = AppConfig.from_env()
            self.console.log(f"[VALIDATOR] Initialized (writeback={AppConfig.VALIDATOR_WRITEBACK_ENABLED})")
            self.console.log(f"[VALIDATOR] Auto-validate insights: {config.auto_validate_insights}")
            self.console.log(f"[VALIDATOR] Minimum rating threshold: {config.validator_min_rating}")
        except Exception as e:
            self.console.log(f"[VALIDATOR][WARN] not available: {e}")

        self.console.log("[bold green]E8 Mind initialization complete. Ready for cognitive cycle.[/bold green]")

        # Optional: speaker audit
        if os.getenv("E8_SPEAKER_AUDIT", "0").lower() in ("1","true","yes"):
            try:
                speaker_funcs = {
                    "TEACHER": getattr(self, "_teacher_ask_new_question", None),
                    "EXPLORER": getattr(self, "_explorer_answer_pending_question", None),
                    "VALIDATOR": getattr(self, "validate_insight", None),
                    "SUBCONSCIOUS": getattr(self, "_generate_subconscious_narrative", None),
                    "NARRATIVE": getattr(self, "generate_narrative_summary", None),
                    # Back-compat key and new preferred name both map to the same method
                    "WORMHOLE_ALERT": getattr(self, "_show_wormhole_alert", None),
                    "RAY_ALERT": getattr(self, "_show_ray_alert", None),
                    "SNAPSHOT": getattr(self, "snapshot", None),
                }
                ok = [k for k,v in speaker_funcs.items() if callable(v)]
                missing = [k for k,v in speaker_funcs.items() if not callable(v)]
                self.console.log(f"[SPEAKER-AUDIT] available={ok} missing={missing}")
            except Exception as _e:
                try:
                    self.console.log(f"[SPEAKER-AUDIT] error: {_e}")
                except Exception:
                    pass



        # Try to enable horizons if requested (moved into instance method to avoid class-scope self usage)
        try:
            self._maybe_enable_horizons()
        except Exception:
            pass

    # Add/enhance this method in the E8Mind class (incorporates witness check and golden ratio scaling)
    async def _trigger_entanglement_event(self, source_node_id: str, event_type: str):
        """
        Triggers non-local entanglement effect using quantum-inspired witness check and golden ratio scaling.
        Simulates cognitive "spooky action" with distance-independent correlations, validated by brain research.
        """
        source_data = self.memory.graph_db.get_node(source_node_id)
        if not source_data:
            return

        source_location = source_data.get('blueprint_location_id')
        if source_location is None:
            return

        target_location = self.physics.get_symmetric_counterpart(source_location)
        if target_location == -1:
            return

        target_node_id = self.memory.find_latest_node_at_blueprint(target_location)
        if target_node_id and target_node_id != source_node_id:
            target_data = self.memory.graph_db.get_node(target_node_id)
            if not target_data:
                return

            # Research integration: Simplified entanglement witness (distance-independent correlation)
            # Map ratings in [0,1] to spins in [-1,1], then use product as correlation proxy.
            source_rating = float(source_data.get('rating', 0.5))
            target_rating = float(target_data.get('rating', 0.5))
            sigma_s = 2.0 * source_rating - 1.0
            sigma_t = 2.0 * target_rating - 1.0
            correlation = sigma_s * sigma_t  # positive if aligned, negative if anti-aligned
            entangle_thresh = float(os.getenv("E8_ENTANGLE_THRESH", "0.1"))  # Threshold on |correlation|
            if abs(correlation) < entangle_thresh:
                try:
                    self.console.log(
                        f"[ENTANGLE] Witness check: |corr|={abs(correlation):.3f} < {entangle_thresh} (ratings {source_rating:.2f},{target_rating:.2f}); skipping."
                    )
                except Exception:
                    pass
                return

            self.console.print(Panel(
                f"Entanglement Event Triggered by '{event_type.upper()}'!\n"
                f"Source: '{source_data.get('label')}' (Location {source_location})\n"
                f"Entangled With: '{target_data.get('label')}' (Location {target_location})\n"
                f"Correlation: {correlation:.3f}",
                title="[bold purple]QUANTUM ENTANGLEMENT[/]", border_style="purple"
            ))

            # Research integration: Golden ratio scaling for nudge amounts (from E8 spectra)
            phi = (1 + np.sqrt(5)) / 2  # ≈1.618
            temp_amount = 0.35 * phi  # Scaled temperature spike (photon entanglement inspired)
            rating_boost = 0.1 * phi   # Scaled potential nudge
            edge_weight = 0.1 * phi    # Scaled edge weight

            # Apply effects with photon-inspired propagation (distance-independent)
            self.memory.spike_temperature(target_node_id, amount=temp_amount)
            update_node_potential(target_data, rating=target_data.get('rating', 0.5) + rating_boost)

            # Add weak 'entangled' edge with research-aligned metadata
            self.memory.graph_db.add_edge(source_node_id, target_node_id, type="entangled", weight=edge_weight)

            # Telemetry logging for research validation (integrates with your existing metrics_log)
            try:
                metrics_log("entanglement_event", {
                    "event_type": event_type,
                    "source_id": source_node_id,
                    "target_id": target_node_id,
                    "source_rating": source_rating,
                    "target_rating": target_rating,
                    "correlation": correlation,
                    "abs_corr": abs(correlation),
                    "threshold": entangle_thresh,
                    "phi_scale": phi
                })
            except Exception:
                pass

    async def _spawn_validator(self, node_id: str):
        """Serialize validator runs under a semaphore and provide clear logs."""
        try:
            sem = getattr(self, "_validator_sem", None)
            if sem is None:
                sem = asyncio.Semaphore(int(os.getenv("E8_MAX_CONCURRENT_VALIDATORS", "3")))
                self._validator_sem = sem
        except Exception:
            sem = asyncio.Semaphore(3)
            self._validator_sem = sem
        async with sem:
            try:
                print(f"[VALIDATOR] start {node_id}")
                # Get node data for prompt variables
                node_data = self.memory.graph_db.get_node(node_id)
                if node_data:
                    raw_text = node_data.get('metaphor', node_data.get('label', ''))
                    rating = node_data.get('rating', 0.0)
                    epoch = getattr(self, 'epoch', 0)
                    
                    # Sanitize text using semantics if available
                    if hasattr(self, "semantics") and hasattr(self.semantics, "sanitize_for_validation"):
                        safe_text = self.semantics.sanitize_for_validation(raw_text)
                    else:
                        safe_text = raw_text
                    
                    # Build prompt vars expected by prompts.yaml
                    prompt_vars = {
                        "insight_text": safe_text,
                        "insight_rating": f"{rating:.3f}" if rating is not None else "null",
                        "node_id": node_id,
                        "epoch": epoch,
                    }
                    
                    await self.validator.validate_insight(node_id, prompt_vars=prompt_vars)
                else:
                    await self.validator.validate_insight(node_id)
            except Exception as e:
                print(f"[VALIDATOR] {node_id} crashed: {e}")

    def _maybe_validate_new_insight(self, node_id: str, kind: str, rating: float | None):
        if not self.validator or not AppConfig.VALIDATOR_WRITEBACK_ENABLED:
            return
        # Only validate insights of the correct types
        insight_types_to_validate = {"insight_synthesis", "explorer_insight", "meta_reflection"}
        if kind not in insight_types_to_validate:
            return
        # Only validate interesting insights - use centralized config
        config = AppConfig.from_env()
        min_rating = config.validator_min_rating
        if rating is not None and rating < min_rating:
            return
        # Fire and forget (async) if coroutine; otherwise run inline or via a thread
        try:
            coro = getattr(self.validator, "validate_insight", None)
            if coro is None:
                return
            if asyncio.iscoroutinefunction(coro):
                asyncio.create_task(coro(node_id))
            else:
                # optional: offload to thread to avoid blocking the tick
                threading.Thread(target=coro, args=(node_id,), daemon=True).start()
            self.console.log(f"[VALIDATOR] queued {kind} → {node_id}")
        except Exception as e:
            self.console.log(f"[VALIDATOR][ERR] {e}")
    
    def _maybe_enable_horizons(self):
        """Enable horizon visualizations if the feature flag is set and field_mantle exists."""
        if os.getenv("E8_USE_HORIZONS", "0") != "1":
            return
        if not getattr(self, "field_mantle", None):
            return
        try:
            if not hasattr(self.physics, "blueprint_points_3d"):
                self.physics.blueprint_points_3d = getattr(self.physics, "projection_points_3d", np.zeros((0,3)))
            if not hasattr(self.physics, "tetra_edges"):
                self.physics.tetra_edges = getattr(self.physics, "blueprint_tetra_edges", [])
            self.field_mantle.enable_horizons(self.physics, getattr(self, 'shells', {}), console=self.console)
        except Exception as e:
            try:
                if self.console:
                    self.console.log(f"[Horizon] init failed: {e}")
            except Exception:
                pass

    def register_proximity_alert(self, distance: float):
        try:
            value = _safe_number(distance, 0.0, 0.0, None)
            # Avoid negative zeros in prints by forcing +0.0
            if abs(value) < 1e-12:
                value = 0.0
            self.last_proximity_distance = value
            
            # Update EMA-based alert distance if proximity engine available
            if hasattr(self, 'proximity_engine') and self.proximity_engine:
                self.proximity_engine.update_alert_distance_ema(value, component="proximity_alert", reason="distance_observation")
            
        except Exception:
            self.last_proximity_distance = float(getattr(self, 'last_proximity_distance', 0.0))
            return

        step = int(getattr(self, 'step_num', 0) or 0)
        last_step = getattr(self, '_last_proximity_log_step', -1000)
        last_value = getattr(self, '_last_proximity_log_distance', None)
        gap = getattr(self, '_proximity_log_gap', 5)  # Show every 5 steps instead of 25
        delta = getattr(self, '_proximity_log_delta', 0.01)  # More sensitive to small changes

        should_log = False
        if last_value is None:
            should_log = True
        else:
            if abs(value - last_value) >= delta:
                should_log = True
            elif gap >= 0 and (step - last_step) >= gap:
                should_log = True

        if should_log and hasattr(self, "console") and self.console:
            # Enhanced logging with EMA threshold info
            current_threshold = getattr(self.proximity_engine, 'alert_distance_ema', 'N/A') if hasattr(self, 'proximity_engine') and self.proximity_engine else 'N/A'
            self.console.log(f"[PROXIMITY] Alert distance={value:.4f} threshold_ema={current_threshold}")
            self._last_proximity_log_distance = value
            self._last_proximity_log_step = step

    def _normalize_rating_by_model(self, value: float) -> float:
        """Improved rating normalization using unified rating system"""
        try:
            rating = float(value)
        except Exception:
            return 0.5
            
        # Use unified rating system if available
        if hasattr(self, 'unified_rating') and self.unified_rating is not None:
            model_key = (self.client_model or "unknown").lower()
            return self.unified_rating.calibrate_rating(rating, model_key)
        
        # Fallback to simple clipping if unified system not available
        return float(np.clip(rating, 0.0, 1.0))


    # --- Throttled RL Training Support ---
    def _schedule_async_task(self, coro):
        """Utility to schedule async coroutines without disrupting sync call sites."""
        if coro is None:
            return
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            try:
                asyncio.run(coro)
            except Exception:
                pass
            return
        try:
            loop.create_task(coro)
        except Exception:
            try:
                asyncio.run(coro)
            except Exception:
                pass

    def cognitive_step_m20(self, dt: float = 0.1):
        """Enhanced M20 cognitive step with field spacetime dynamics"""
        try:
            # EmergenceController integration
            if hasattr(self, 'emergence_controller') and self.emergence_controller is not None:
                try:
                    # Update annealing progress
                    self.emergence_controller._update_annealing()

                    # Update emergence metrics
                    self.emergence_controller.update_metrics()

                    # Apply safeguards
                    self.emergence_controller.apply_safeguards()

                except Exception as ec_err:
                    try:
                        self.console.log(f"[M20] EmergenceController integration failed: {ec_err}")
                    except Exception:
                        pass

            mantle = getattr(self, 'fluid_mantle', None)
            metrics = getattr(self, 'm20_metrics', None)

            def _increment_metric(key: str):
                if isinstance(metrics, dict):
                    metrics[key] = metrics.get(key, 0) + 1

            consciousness_data = self.extract_consciousness_data()
            if mantle is not None:
                try:
                    mantle.flow_step(consciousness_data, dt)
                    _increment_metric('field_steps')
                    if isinstance(metrics, dict) and hasattr(mantle, 'get_shell_energy_summary'):
                        summary = mantle.get_shell_energy_summary()
                        metrics['field_energy'] = float(summary.get('total_energy', metrics.get('field_energy', 0.0)))
                        metrics['poynting_flux'] = float(summary.get('total_flux', metrics.get('poynting_flux', 0.0)))
                except Exception as flow_err:
                    try:
                        self.console.log(f"[M20] Field mantle step failed: {flow_err}")
                    except Exception:
                        pass

            self.standard_cognitive_processing()
            
            # Systematic Physics Validation Integration (M24)
            if hasattr(self, 'systematic_physics') and self.systematic_physics is not None:
                try:
                    # Track field energy and compute inflation rate
                    phi = getattr(self.curvature_field, 'kappa', None) if hasattr(self, 'curvature_field') else None
                    L = self.curvature_field.get_graph_laplacian() if hasattr(self, 'curvature_field') else None
                    
                    field_energy = self.systematic_physics.compute_field_energy(phi, L)
                    inflation_rate = self.systematic_physics.compute_inflation_rate()
                    
                    # Validate CFL constraint for wave mechanics
                    if hasattr(self, 'everywhen_wave') and hasattr(self.everywhen_wave, 'c_eff'):
                        cfl_validation = self.systematic_physics.validate_cfl_constraint(self.everywhen_wave.c_eff, L)
                        if cfl_validation['violation']:
                            self.systematic_physics.log(f"CFL violation: c_eff={cfl_validation['c_eff']:.3f} > c_max={cfl_validation['c_max_stable']:.3f}", "warning")
                    
                    # Auto-tune parameters if enabled
                    if self.systematic_physics.auto_tune_enabled:
                        tuned_params = self.systematic_physics.auto_tune_parameters(L)
                        if tuned_params and hasattr(self, 'everywhen_wave'):
                            # Apply auto-tuned parameters
                            if 'c_eff' in tuned_params:
                                self.everywhen_wave.c_eff = tuned_params['c_eff']
                                self.everywhen_wave.nu = min((tuned_params['c_eff'] ** 2) * 0.9, 1.5 / max(tuned_params.get('lambda_max', 1.0), 1e-9))
                            if 'damping' in tuned_params:
                                self.everywhen_wave.damping = tuned_params['damping']
                    
                    # Compute and track dimensionless invariants every 10 steps
                    if self.step_num % 10 == 0:
                        c_eff = getattr(self.everywhen_wave, 'c_eff', 0.9) if hasattr(self, 'everywhen_wave') else 0.9
                        beta = getattr(self.everywhen_wave, 'damping', 0.08) if hasattr(self, 'everywhen_wave') else 0.08
                        invariants = self.systematic_physics.compute_dimensionless_invariants(phi, L, c_eff, beta)
                        
                        # Log significant invariant changes
                        if invariants.get('Q2', 0) > 10.0:  # Stability regime warning
                            self.systematic_physics.log(f"Q2 stability indicator high: {invariants['Q2']:.3f} (suggests potential instability)", "warning")
                        
                        # Emit metrics
                        try:
                            metrics_log("systematic_physics.invariants", {
                                "step": self.step_num,
                                "Q1": invariants.get('Q1', 0),
                                "Q2": invariants.get('Q2', 0), 
                                "Q3": invariants.get('Q3', 1),
                                "field_energy": field_energy,
                                "inflation_rate": inflation_rate
                            })
                        except Exception:
                            pass
                    
                    # Control inflation epochs based on system state
                    if self.step_num % 25 == 0:  # Check every 25 steps
                        # Estimate novelty rate and validator acceptance
                        recent_insights = module_fallback_get_recent_insights(self, 10)
                        novelty_rate = len([r for r in recent_insights if r.get('rating', 0) > 0.7]) / max(len(recent_insights), 1)
                        validator_acceptance = len([r for r in recent_insights if r.get('rating', 0) > 0.5]) / max(len(recent_insights), 1)
                        
                        epoch_control = self.systematic_physics.control_inflation_epoch(novelty_rate, validator_acceptance)
                        
                        # Apply Hubble friction to field mantle if available
                        if hasattr(self, 'fluid_mantle') and 'friction_factor' in epoch_control:
                            try:
                                # Adjust damping parameters based on Hubble friction
                                friction = epoch_control['friction_factor']
                                if hasattr(self.fluid_mantle, 'gauge_damping'):
                                    self.fluid_mantle.gauge_damping *= friction
                            except Exception:
                                pass
                    
                    # Validate commutation relations every 100 steps
                    if self.step_num % 100 == 0 and phi is not None and len(phi) > 0:
                        try:
                            # Test commutation with current field state
                            commutation_result = self.systematic_physics.validate_project_compact_commutation(phi[:min(8, len(phi))])
                            if not commutation_result['valid']:
                                self.systematic_physics.log(f"Commutation violation: error={commutation_result['error']:.6f} > ε={self.systematic_physics.commutation_epsilon:.6f}", "warning")
                            
                            # Emit commutation metrics
                            try:
                                metrics_log("systematic_physics.commutation", {
                                    "step": self.step_num,
                                    "valid": commutation_result['valid'],
                                    "error": commutation_result['error'],
                                    "coherent": commutation_result.get('physics_interpretation', {}).get('coherent', False)
                                })
                            except Exception:
                                pass
                        except Exception:
                            pass
                    
                except Exception as phys_err:
                    try:
                        self.console.log(f"[SystematicPhysics] Validation step failed: {phys_err}")
                    except Exception:
                        pass
            
            system_state = self.get_current_system_state()

            active_memories = []
            if hasattr(self, 'memory') and hasattr(self.memory, 'get_active_memories'):
                try:
                    active_memories = self.memory.get_active_memories()
                except Exception:
                    active_memories = []
            if active_memories and mantle is not None and hasattr(mantle, 'transport_memories_along_geodesics'):
                try:
                    transported = mantle.transport_memories_along_geodesics(active_memories, dt)
                    self.update_memory_positions(transported or [])
                except Exception as transport_err:
                    try:
                        self.console.log(f"[M20] Geodesic transport failed: {transport_err}")
                    except Exception:
                        pass

            if getattr(self, 'valence_engine', None) is not None:
                try:
                    cognitive_load = self.valence_engine.assess_cognitive_load(system_state)
                    # Use a safe getattr fallback in case the instance doesn't expose the method
                    recent_insights_fn = getattr(self, 'get_recent_insights', None)
                    if recent_insights_fn is None:
                        recent_insights = module_fallback_get_recent_insights(self, 20)
                    else:
                        recent_insights = recent_insights_fn(20)
                    novelty_pressure = self.valence_engine.assess_novelty_pressure(recent_insights)
                    active_nodes = self.get_active_cognitive_nodes()
                    for node_id in active_nodes[:10]:
                        try:
                            self.valence_engine.alter_node_valence(node_id, cognitive_load, novelty_pressure)
                            _increment_metric('valence_updates')
                        except Exception as val_err:
                            try:
                                self.console.log(f"[M20] Valence update failed for {node_id}: {val_err}")
                            except Exception:
                                pass
                except Exception as assess_err:
                    try:
                        self.console.log(f"[M20] Valence assessment failed: {assess_err}")
                    except Exception:
                        pass

            if getattr(self, 'energy_navigator', None) is not None and active_memories:
                try:
                    if float(np.random.random()) < 0.1:
                        current_state = self.get_current_cognitive_state()
                        if current_state is not None:
                            novel_associations = self.energy_navigator.navigate_to_novel_associations(current_state, exploration_budget=3)
                            self.process_novel_associations(novel_associations)
                            _increment_metric('energy_explorations')
                except Exception as energy_err:
                    try:
                        self.console.log(f"[M20] Energy landscape navigation failed: {energy_err}")
                    except Exception:
                        pass

            if getattr(self, 'topology_engine', None) is not None:
                try:
                    recent_data_fn = getattr(self, 'get_recent_cognitive_data', None)
                    if recent_data_fn is None:
                        recent_data = module_fallback_get_recent_cognitive_data(self)
                    else:
                        recent_data = recent_data_fn()
                    if recent_data:
                        complexity = self.topology_engine.assess_pattern_complexity(recent_data)
                        threshold = getattr(self.topology_engine, 'complexity_threshold', 0.0)
                        if complexity > threshold:
                            new_topology = self.topology_engine.generate_topology_variations(complexity)
                            if new_topology is not None:
                                self.apply_topology_update(new_topology)
                                _increment_metric('topology_adaptations')
                except Exception as topo_err:
                    try:
                        self.console.log(f"[M20] Topology adaptation failed: {topo_err}")
                    except Exception:
                        pass

            if getattr(self, 'recursive_architect', None) is not None:
                try:
                    if float(np.random.random()) < 0.05:
                        tasks = self.get_pending_architecture_tasks()
                        if tasks:
                            plan = self.recursive_architect.generate_architecture_plan(tasks)
                            if plan:
                                self.recursive_architect.execute_plan(plan)
                                _increment_metric('recursive_implementations')
                except Exception as arch_err:
                    try:
                        self.console.log(f"[M20] Recursive architect failed: {arch_err}")
                    except Exception:
                        pass
        except Exception as exc:
            try:
                self.console.log(f"[M20] cognitive_step_m20 failed: {exc}")
                self.standard_cognitive_processing()
            except Exception:
                self.standard_cognitive_processing()
        else:
            if getattr(self, 'm20_metrics', None) is not None:
                try:
                    self.m20_metrics['field_steps'] = self.m20_metrics.get('field_steps', 0)
                except Exception:
                    pass

            # Integrate EmbeddingBootstrap for deterministic embeddings
            if hasattr(self, 'embedding_bootstrap') and self.embedding_bootstrap is not None:
                try:
                    # Process recent cognitive content for embedding
                    recent_content = self.get_recent_cognitive_content()
                    if recent_content:
                        # Collect embeddings for batch processing
                        content_embeddings = {}
                        
                        # Generate embeddings for recent content
                        for content_item in recent_content[:10]:  # Process up to 10 recent items
                            try:
                                embedding = self.embedding_bootstrap.encode_text(str(content_item))
                                if embedding is not None and len(embedding) > 0:
                                    content_embeddings[str(content_item)] = embedding
                                    self.console.log(f"[BOOTSTRAP] Generated embedding for: {str(content_item)[:50]}...")
                            except Exception as embed_err:
                                try:
                                    self.console.log(f"[BOOTSTRAP] Embedding generation failed: {embed_err}")
                                except Exception:
                                    pass
                        
                        # Apply graph hygiene if we have embeddings
                        if content_embeddings:
                            try:
                                # Apply deduplication and k-NN edge generation
                                cleaned_embeddings, knn_edges = self.embedding_bootstrap.apply_graph_hygiene(
                                    content_embeddings, 
                                    dedup_threshold=0.9,  # Slightly more aggressive deduplication
                                    knn_k=3  # Fewer edges for bootstrap content
                                )
                                
                                # Store cleaned embeddings
                                for content_text, embedding in cleaned_embeddings.items():
                                    try:
                                        self.store_deterministic_embedding(content_text, embedding)
                                        _increment_metric('embedding_bootstraps')
                                    except Exception as store_err:
                                        self.console.log(f"[BOOTSTRAP] Failed to store embedding: {store_err}")
                                
                                # Log hygiene results
                                self.console.log(f"[BOOTSTRAP] Hygiene applied: {len(content_embeddings)} ? {len(cleaned_embeddings)} embeddings, {len(knn_edges)} edges")
                                
                                # Log comprehensive bootstrap cycle metrics
                                try:
                                    metrics_log("bootstrap.cycle_complete", {
                                        "event": "bootstrap_cycle",
                                        "content_processed": len(recent_content),
                                        "embeddings_generated": len(content_embeddings),
                                        "embeddings_after_hygiene": len(cleaned_embeddings),
                                        "edges_generated": len(knn_edges),
                                        "dedup_ratio": len(cleaned_embeddings) / len(content_embeddings) if content_embeddings else 0.0,
                                        "avg_embedding_norm": float(np.mean([np.linalg.norm(vec) for vec in cleaned_embeddings.values()])) if cleaned_embeddings else 0.0
                                    })
                                except Exception:
                                    pass
                                
                            except Exception as hygiene_err:
                                self.console.log(f"[BOOTSTRAP] Graph hygiene failed: {hygiene_err}")
                                # Fallback: store all embeddings without hygiene
                                for content_text, embedding in content_embeddings.items():
                                    try:
                                        self.store_deterministic_embedding(content_text, embedding)
                                        _increment_metric('embedding_bootstraps')
                                    except Exception as store_err:
                                        self.console.log(f"[BOOTSTRAP] Fallback store failed: {store_err}")
                        else:
                            self.console.log("[BOOTSTRAP] No embeddings generated for recent content")
                            
                except Exception as bootstrap_err:
                    try:
                        self.console.log(f"[BOOTSTRAP] Embedding bootstrap integration failed: {bootstrap_err}")
                    except Exception:
                        pass

    def get_active_cognitive_nodes(self) -> List[str]:
        """Get list of active cognitive node IDs"""
        nodes: List[str] = []
        try:
            if hasattr(self, 'memory') and hasattr(self.memory, 'get_recent_memories'):
                memories = self.memory.get_recent_memories(20)
            else:
                memories = []
            for memory in memories or []:
                node_id = None
                if hasattr(memory, 'node_id') and memory.node_id:
                    node_id = memory.node_id
                elif hasattr(memory, 'id') and memory.id:
                    node_id = memory.id
                if node_id:
                    nodes.append(str(node_id))
        except Exception as exc:
            try:
                self.console.log(f"[M20] Failed to get active nodes: {exc}")
            except Exception:
                pass
        return list(dict.fromkeys(nodes))

    def get_pending_architecture_tasks(self) -> List[Any]:
        """Placeholder for fetching architecture improvement tasks.

        Returns an empty list by default. A future implementation may query
        a curriculum manager or auto-tasking system (e.g., self.curriculum).
        """
        try:
            # If a curriculum manager exists, try to pull pending tasks
            curriculum = getattr(self, 'curriculum', None) or getattr(self, 'auto_task_manager', None)
            if curriculum is not None and hasattr(curriculum, 'get_pending_tasks'):
                try:
                    tasks = curriculum.get_pending_tasks()
                    return list(tasks or [])
                except Exception:
                    pass
        except Exception:
            pass
        return []

    def extract_consciousness_data(self) -> List[Any]:
        """Extract consciousness data for fluid dynamics"""
        consciousness_data: List[Any] = []
        try:
            if hasattr(self, 'memory') and hasattr(self.memory, 'get_recent_memories'):
                memories = self.memory.get_recent_memories(50)
            else:
                memories = []
            for memory in memories or []:
                position_vec = None
                if hasattr(memory, 'vector') and memory.vector is not None:
                    try:
                        position_vec = np.array(memory.vector, dtype=float)[:8]
                    except Exception:
                        position_vec = None
                elif hasattr(memory, 'embedding') and memory.embedding is not None:
                    try:
                        position_vec = np.array(memory.embedding, dtype=float)[:8]
                    except Exception:
                        position_vec = None
                if position_vec is None:
                    continue
                data_point = type('ConsciousnessData', (), {})()
                data_point.position = position_vec
                if hasattr(memory, 'rating') and memory.rating is not None:
                    data_point.strength = float(memory.rating)
                elif hasattr(memory, 'novelty') and memory.novelty is not None:
                    data_point.strength = float(memory.novelty)
                else:
                    data_point.strength = 0.5
                consciousness_data.append(data_point)
        except Exception as exc:
            try:
                self.console.log(f"[M20] Failed to extract consciousness data: {exc}")
            except Exception:
                pass
        return consciousness_data

    def standard_cognitive_processing(self):
        """Execute the canonical M19 cognitive loop for one tick."""
        step = int(getattr(self, 'step_num', 0))
        try:
            self._update_cognitive_modules(step)
        except Exception as exc:
            try:
                self.console.log(f"[M19] Cognitive module update failed: {exc}")
            except Exception:
                pass
        buffer = getattr(self, '_autoencoder_train_buffer', [])
        batch_size = getattr(self, '_autoencoder_batch_size', 64)
        try:
            updated_buffer = self._train_autoencoder_if_ready(buffer, batch_size)
            if updated_buffer is not None:
                self._autoencoder_train_buffer = updated_buffer
        except Exception as exc:
            try:
                self.console.log(f"[M19] Autoencoder training step failed: {exc}")
            except Exception:
                pass
        try:
            self._update_emergent_metrics()
        except Exception:
            pass
        if step % 50 == 0:
            self._schedule_async_task(self._update_synaptic_plasticity())
        if step % 100 == 0 and getattr(self, 'bandit', None) is not None:
            try:
                self.bandit.log_health(step=step)
            except Exception as exc:
                try:
                    self.console.log(f"[Health] Bandit health logging failed: {exc}")
                except Exception:
                    pass
        # Reset LLM degraded flag periodically to allow recovery
        if step % 100 == 0:
            try:
                self.llm_pool.maybe_reset_degraded()
            except Exception:
                pass
        if step % 500 == 0 and step > 0:
            try:
                removed = self.memory.dedup_nodes_by_eps()
                added = self.memory.ensure_quasicrystal_edges()
                self.console.log(f"[Geometry] dedup_removed={removed}, knn_edges_added={added}")
            except Exception as exc:
                try:
                    self.console.log(f"[Geometry] hygiene skipped: {exc}")
                except Exception:
                    pass
        if hasattr(self, 'hrl'):
            try:
                self.hrl.maybe_update(step)
            except Exception:
                pass

        current_state = self._build_state_vector()
        if hasattr(self, '_wm_lazy_init') and getattr(self, 'world_model', None) is None:
            try:
                self._wm_lazy_init(current_state.size)
            except Exception as exc:
                try:
                    self.console.log(f"[WM] Lazy init failed: {exc}")
                except Exception:
                    pass
        if getattr(self, 'world_model', None) is not None and self.world_model.is_ready():
            try:
                self.world_model.imagine_with_policy(current_state, self.agent, horizon=8)
            except Exception:
                pass

        arm_index = None
        if getattr(self, 'bandit', None) is not None:
            try:
                arm_index = self.bandit.pull(context=current_state)
                active_arm = self.bandit.arms[arm_index]
                self.sigma_q = active_arm.get('diffusion_sigma', self.sigma_q)
            except Exception as exc:
                try:
                    self.console.log(f"[Bandit] Selection failed: {exc}")
                except Exception:
                    pass
                arm_index = None

        action = np.zeros(self.action_dim, dtype=np.float32)
        if getattr(self, 'agent', None) is not None:
            try:
                action = self.agent.select_action(current_state)
            except Exception as exc:
                try:
                    self.console.log(f"[Agent] Action selection failed: {exc}")
                except Exception:
                    pass

        if getattr(self, 'macro_manager', None) is not None and getattr(self, 'world_model', None) is not None:
            try:
                maybe = self.macro_manager.select_action(current_state, self)
                if maybe is not None:
                    blended = np.asarray(maybe, dtype=np.float32)
                    action = 0.7 * action + 0.3 * blended
            except Exception as exc:
                try:
                    self.console.log(f"[MacroManager] Action proposal failed: {exc}")
                except Exception:
                    pass

        try:
            # Pass raw action to manifold; individual shells will normalize/gate bivector+angle
            self.apply_manifold_action(action)
        except Exception as exc:
            try:
                self.console.log(f"[Manifold] Action application failed: {exc}")
            except Exception:
                pass

        prev_idx = self.prev_node_index or random.randrange(self.physics.roots.shape[0])
        # Optional Wavey integration: only call if bridge exists and Wavey is available
        try:
            if getattr(self, 'wavey_bridge', None) is not None and WAVEY_AVAILABLE:
                wavey_out = integrate_one_cycle(self, self.wavey_bridge)
                if isinstance(wavey_out, dict):
                    self._update_anchors_from_wavey(wavey_out)
        except Exception as e:
            try:
                self.console.log(f"[dim][Wavey] integration skipped/failed: {e}[/dim]")
            except Exception:
                try: self.console.log(f"[Wavey] integration skipped/failed: {e}")
                except Exception: pass

        # Quantum engine step: only run if qeng and anchors exist
        try:
            if getattr(self, 'qeng', None) is not None and getattr(self, 'anchors', None) is not None:
                V = None
                try:
                    V = self.anchors.potential()
                except Exception:
                    V = None
                # Build Hamiltonian using provided potential (V may be None -> handled by build_hamiltonian)
                if hasattr(self.qeng, 'build_hamiltonian'):
                    try:
                        self.qeng.build_hamiltonian(V=V)
                    except Exception as bex:
                        try:
                            self.console.log(f"[dim][QENG] build_hamiltonian failed: {bex}[/dim]")
                        except Exception:
                            try: self.console.log(f"[QENG] build_hamiltonian failed: {bex}")
                            except Exception: pass
                # Continue with adaptive step and measurement if methods exist
                if hasattr(self.qeng, 'step_adaptive'):
                    try: self.qeng.step_adaptive()
                    except Exception:
                        pass
                if hasattr(self.qeng, 'measure_hybrid'):
                    try:
                        current_node_index = self.qeng.measure_hybrid(prev_idx, sigma=self.sigma_q)[0]
                        self.prev_node_index = current_node_index
                    except Exception as exc:
                        try:
                            self.console.log(f"[dim][QENG] measurement failed: {exc}[/dim]")
                        except Exception:
                            try: self.console.log(f"[QENG] measurement failed: {exc}")
                            except Exception: pass
        except Exception as exc:
            try:
                self.console.log(f"[dim][QENG] Quantum step failed: {exc}[/dim]")
            except Exception:
                try:
                    self.console.log(f"[QENG] Quantum step failed: {exc}")
                except Exception:
                    pass

        next_state = self._build_state_vector()
        try:
            base_reward = self.potential_evaluator.calculate_potential_and_get_reward()
        except Exception:
            base_reward = 0.0

        final_reward = base_reward
        if hasattr(self, 'hrl'):
            try:
                final_reward = self.hrl.shape_reward(current_state, next_state, base_reward)
            except Exception:
                final_reward = base_reward

        if hasattr(self, 'causal'):
            try:
                self.causal.update_on_step(self, clamped_action, final_reward)
            except Exception:
                pass

    def get_recent_cognitive_content(self) -> List[str]:
        """Get recent cognitive content for embedding bootstrap processing"""
        content: List[str] = []
        try:
            # Get recent memories
            if hasattr(self, 'memory') and hasattr(self.memory, 'get_recent_memories'):
                memories = self.memory.get_recent_memories(10)
                for memory in memories or []:
                    # Extract text content from memory
                    text_content = None
                    if hasattr(memory, 'text') and memory.text:
                        text_content = str(memory.text)
                    elif hasattr(memory, 'content') and memory.content:
                        text_content = str(memory.content)
                    elif hasattr(memory, 'description') and memory.description:
                        text_content = str(memory.description)

                    if text_content and len(text_content.strip()) > 10:  # Minimum content length
                        content.append(text_content.strip())

            # Get recent insights
            if hasattr(self, 'insight_agent') and hasattr(self.insight_agent, 'recent_insights'):
                insights = list(self.insight_agent.recent_insights)[-5:]  # Last 5 insights
                for insight in insights:
                    if isinstance(insight, dict) and 'text' in insight:
                        content.append(str(insight['text']))
                    elif hasattr(insight, 'text'):
                        content.append(str(insight.text))

        except Exception as exc:
            try:
                self.console.log(f"[M20] Failed to get recent cognitive content: {exc}")
            except Exception:
                pass

        return content

    def store_deterministic_embedding(self, content: str, embedding: np.ndarray):
        """Store deterministic embedding in the memory system"""
        try:
            if not hasattr(self, 'memory') or self.memory is None:
                return

            # Create embedding entry
            embedding_entry = {
                'content': content,
                'embedding': embedding,
                'timestamp': time.time(),
                'type': 'deterministic_embedding',
                'bootstrap_generated': True
            }

            # Store in memory system
            if hasattr(self.memory, 'add_embedding_entry'):
                self.memory.add_embedding_entry(embedding_entry)
            elif hasattr(self.memory, 'store_embedding'):
                self.memory.store_embedding(content, embedding)
            else:
                # Fallback: store as regular memory entry
                memory_entry = type('MemoryEntry', (), {})()
                memory_entry.text = content
                memory_entry.embedding = embedding
                memory_entry.timestamp = time.time()
                memory_entry.type = 'embedding'

                if hasattr(self.memory, 'add_entry'):
                    self.memory.add_entry(memory_entry)

            return content
        except Exception as exc:
            try:
                self.console.log(f"[M20] Failed to store deterministic embedding: {exc}")
            except Exception:
                pass

        return None

    # --- Moved methods: ensure console export and cognitive cycle ---
    def _ensure_console_export_state(self):
        if not hasattr(self, "_console_export_inited"):
            base = get_path("logs/console", self.run_id)
            os.makedirs(base, exist_ok=True)
            self.console_export_dir = base
            self._console_last_export_len = 0
            self._console_chunk_index = 0
            self._console_export_inited = True

    def _export_console_chunk(self, end_step: int, final: bool = False) -> None:
        self._ensure_console_export_state()
        try:
            text_all = self.console.export_text()

            if len(text_all) < self._console_last_export_len:
                self._console_last_export_len = 0
                self._console_chunk_index += 1

            new_text = text_all[self._console_last_export_len:]

            if not new_text and not final:
                return

            start_step = self._console_chunk_index * CONSOLE_EXPORT_EVERY_STEPS
            end_inclusive = end_step
            base = f"console_{start_step:06d}-{end_inclusive:06d}"

            if CONSOLE_EXPORT_FORMAT in ("text", "both"):
                with open(os.path.join(self.console_export_dir, base + ".txt"), "w", encoding="utf-8") as f:
                    f.write(new_text)

            if CONSOLE_EXPORT_FORMAT in ("json", "both"):
                payload = {
                    "run_id": self.run_id,
                    "chunk_index": self._console_chunk_index,
                    "start_step": start_step,
                    "end_step": end_inclusive,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "content": new_text
                }
                with open(os.path.join(self.console_export_dir, base + ".json"), "w", encoding="utf-8") as f:
                    json.dump(payload, f, ensure_ascii=False)

            self._console_last_export_len = len(text_all)
            self._console_chunk_index += 1
        except Exception as e:
            self.console.log(f"[ConsoleExport] Failed: {e}")

    async def _project_self_into_memory(self):
        """
        One-time at start: read this script and inject sections into memory as concepts.
        """
        try:
            src_path = os.path.abspath(globals().get('__file__', 'e8_mind_server_M20.py'))
            with open(src_path, "r", encoding="utf-8") as f:
                code_txt = f.read()
        except Exception as e:
            self.console.log(f"[SelfProject] failed to read source: {e}")
            return

        try:
            splitter = re.compile(r"(?m)^(class\s+\w+\s*:|def\s+\w+\s*\(|if\s+__name__\s*==\s*['\"]__main__['\"]\s*:)");
            idxs = [m.start() for m in splitter.finditer(code_txt)]
            idxs = [0] + idxs + [len(code_txt)]
            sections = []
            for a, b in zip(idxs[:-1], idxs[1:]):
                chunk = code_txt[a:b].strip()
                if not chunk:
                    continue
                first = chunk.splitlines()[0].strip()
                label = sanitize_line(first[:72]) if 'sanitize_line' in globals() else first[:72]
                excerpt = "\n".join(chunk.splitlines()[:40])
                sections.append((label, excerpt))
            if not sections:
                head = "\n".join(code_txt.splitlines()[:80])
                sections = [("source: e8_mind_server", head)]
        except Exception as e:
            self.console.log(f"[SelfProject] split failed: {e}")
            return

        try:
            root_id = await self.memory.add_entry({
                "type": "self_code",
                "label": "E8 Mind � current source",
                "metaphor": "The mind reading its own blueprint.",
                "rating": 0.9,
                "step": int(getattr(self, "step_num", 0))
            })
        except Exception as e:
            self.console.log(f"[SelfProject] root insert failed: {e}")
            return

        inserted = []
        for label, excerpt in sections[:40]:
            try:
                emb = await self.get_embedding(excerpt)
            except Exception:
                emb = None
            try:
                node_id = await self.memory.add_entry({
                    "type": "self_code_section",
                    "label": label,
                    "metaphor": excerpt,
                    "embedding": emb,
                    "rating": 0.7,
                    "temperature": 0.2,
                    "step": int(getattr(self, "step_num", 0))
                }, parent_ids=[root_id])
                inserted.append(node_id)
            except Exception as e:
                self.console.log(f"[SelfProject] section insert failed: {e}")

        try:
            if 'bump_temps' in globals():
                bump_temps(self.memory, inserted, amount=0.6)
        except Exception as e:
            self.console.log(f"[SelfProject] temp bump failed: {e}")
        self.console.log(f"[SelfProject] projected {len(inserted)} code sections into memory.")

    async def run_cognitive_cycle(self, max_steps=297600, mode='quantum'):
        """Start the integrated cognitive loop."""
        self._ensure_console_export_state()
        self.console.rule(f"[bold magenta]Starting Integrated Cognitive Cycle | Mode: {mode.upper()}[/bold magenta]")

        await self.llm_pool.start()
        await self.goal_field.initialize_goals()

        for name, config in DATA_SOURCES.items():
            self.ingestion_pipeline.add_source(name, config)

        if E8_INGEST:
            await self.ingestion_pipeline.start()

        if self.market:
            if globals().get("E8_MARKET_FEED_ENABLED", False):
                await self.market.start()

        self.max_steps = max_steps

        with Progress(
            SpinnerColumn(style="green"),
            "[progress.description]{task.description}",
            BarColumn(),
            "[progress.percentage]{task.percentage:>3.0f}%",
            "Step", TextColumn("{task.completed}/{task.total}"),
            "Concepts:", TextColumn("[bold magenta]{task.fields[concept_count]}[/bold magenta]"),
            TimeElapsedColumn(),
            console=self.console, transient=True
        ) as progress:
            task = progress.add_task(f"Thinking ({mode})", total=max_steps, concept_count=0)
            for step in range(max_steps):
                self.step_num = step
                # Reset per-tick training counter
                if step == 0 or getattr(self, '_train_calls_this_tick', None) is None:
                    self._init_training_throttle()
                self._reset_tick_train_counter()

                if step == 0:
                    if E8_SELF_PROJECT:
                        await self._project_self_into_memory()

                self.cognitive_step_m20()

                concept_count = self.memory.graph_db.graph.number_of_nodes()
                progress.update(task, advance=1, concept_count=concept_count)
                # Every 100 steps: emit concise summary
                if (step % 100) == 0 and step > 0:
                    try:
                        bh_hist = getattr(self, 'black_hole_log', [])
                        bh_vals = [e.get('pressure') for e in bh_hist[-50:] if isinstance(e, dict) and 'pressure' in e]
                        bh_avg = float(sum(bh_vals)/len(bh_vals)) if bh_vals else float(getattr(self, 'black_hole_pressure', 0.0))
                        explorer_events = sum(1 for e in getattr(self, 'subconscious_event_log', []) if e.get('type') == 'teacher_explorer' and e.get('step', -1) > step-100)
                        validator_events = sum(1 for e in getattr(self, 'subconscious_event_log', []) if e.get('type') == 'validator' and e.get('step', -1) > step-100)
                        fails = sum(1 for e in getattr(self, 'subconscious_event_log', []) if e.get('type') == 'validator' and e.get('data', {}).get('verdict') == 'fail' and e.get('step', -1) > step-100)
                        summary = {"explorer": explorer_events, "validator": validator_events, "fail": fails, "bh_avg": round(bh_avg,3)}
                        
                        # Add systematic physics diagnostics to summary
                        if hasattr(self, 'systematic_physics') and self.systematic_physics is not None:
                            try:
                                physics_report = self.systematic_physics.generate_diagnostic_report()
                                summary.update({
                                    "physics_violations": physics_report.get('stability_violations', 0) + physics_report.get('mass_violations', 0) + physics_report.get('commutation_violations', 0),
                                    "inflation_active": physics_report.get('inflation_epoch_active', False),
                                    "Q1_stability": round(physics_report.get('Q1_stability', 0), 4),
                                    "field_energy": round(physics_report.get('field_energy_current', 0), 4)
                                })
                            except Exception:
                                pass
                        
                        self.console.log(f"[Summary@{step}] {summary}")
                        metrics_log("cycle.summary", {"event": "insight", "step": step, **summary})
                        
                        # Display hypothesis dashboard at configurable intervals
                        if HYPOTHESIS_DASHBOARD_INTERVAL > 0 and step % HYPOTHESIS_DASHBOARD_INTERVAL == 0 and step > 0:
                            self.display_hypothesis_dashboard()
                            
                    except Exception:
                        pass
                await asyncio.sleep(0.01)
                # Finalize this step: update metrics and push telemetry to SSE/WS
                try:
                    self.complete_cognitive_cycle()
                except Exception:
                    pass

            self.console.log("\nCognitive cycle complete.")
            self._export_console_chunk(self.step_num, final=True)

    def create_emergence_controller(self, summary_hook=None, http=None):
        """Create and attach an EmergenceController to this mind instance and return it."""
        try:
            # Import required modules
            import aiohttp
            web = aiohttp.web
        except ImportError:
            web = None

        # Create EmergenceController instance
        host = os.getenv("E8_EC_HOST", "localhost")
        port = int(os.getenv("E8_EC_PORT", "7871"))  # Different port from main server

        # Set initial mode from environment if specified
        initial_mode = os.getenv("E8_EMERGENCE_MODE", "assisted")
        if initial_mode not in ['assisted', 'self-emergent']:
            initial_mode = "assisted"

        ec = EmergenceController(
            mind_instance=self,
            host=host,
            port=port
        )

        # Set initial mode if specified
        if initial_mode != "assisted":  # Default is assisted
            try:
                ec.set_emergence_mode(initial_mode)
            except Exception:
                pass

        # Assign to mind instance
        self.emergence_controller = ec
        return ec

    def get_current_system_state(self) -> Dict[str, Any]:
        """Extract current system state for M20 processing"""
        state = {
            'active_memories': [],
            'processing_queue': [],
            'insight_rate': 0.0
        }
        try:
            if hasattr(self, 'memory') and hasattr(self.memory, 'get_recent_memories'):
                try:
                    state['active_memories'] = self.memory.get_recent_memories(100)
                except Exception:
                    state['active_memories'] = []
            if hasattr(self, 'scheduler'):
                queue_obj = getattr(self.scheduler, 'queue', None)
                if queue_obj is not None:
                    try:
                        state['processing_queue'] = list(queue_obj)
                    except Exception:
                        try:
                            state['processing_queue'] = list(getattr(queue_obj, 'queue', []))
                        except Exception:
                            state['processing_queue'] = []
            if hasattr(self, 'insight_agent'):
                rewards = getattr(self.insight_agent, 'recent_rewards', [])
                if rewards:
                    recent = list(rewards)[-10:] if hasattr(rewards, '__iter__') else []
                    if recent:
                        state['insight_rate'] = float(np.mean(recent))
        except Exception as exc:
            try:
                self.console.log(f"[M20] Failed to get system state: {exc}")
            except Exception:
                pass
        return state

    def extract_consciousness_data(self) -> List[Any]:
        """Extract consciousness data for fluid dynamics"""
        consciousness_data: List[Any] = []
        try:
            if hasattr(self, 'memory') and hasattr(self.memory, 'get_recent_memories'):
                memories = self.memory.get_recent_memories(50)
            else:
                memories = []
            for memory in memories or []:
                position_vec = None
                if hasattr(memory, 'vector') and memory.vector is not None:
                    try:
                        position_vec = np.array(memory.vector, dtype=float)[:8]
                    except Exception:
                        position_vec = None
                elif hasattr(memory, 'embedding') and memory.embedding is not None:
                    try:
                        position_vec = np.array(memory.embedding, dtype=float)[:8]
                    except Exception:
                        position_vec = None
                if position_vec is None:
                    continue
                data_point = type('ConsciousnessData', (), {})()
                data_point.position = position_vec
                if hasattr(memory, 'rating') and memory.rating is not None:
                    data_point.strength = float(memory.rating)
                elif hasattr(memory, 'novelty') and memory.novelty is not None:
                    data_point.strength = float(memory.novelty)
                else:
                    data_point.strength = 0.5
                consciousness_data.append(data_point)
        except Exception as exc:
            try:
                self.console.log(f"[M20] Failed to extract consciousness data: {exc}")
            except Exception:
                pass
        return consciousness_data

    async def seed_domain_if_empty(self):
        """
        If memory is empty, add one bootstrap concept using the semantic domain.
        Controlled by E8_SEED_DOMAIN flag.
        """
        try:
            # Count existing vectors across dimensional shells
            total = 0
            ds = getattr(self, "dimensional_shells", {}) or {}
            for shell in ds.values():
                try:
                    mat, _ = shell.get_all_vectors_as_matrix()
                    total += (len(mat) if mat is not None else 0)
                except Exception:
                    pass
            if total > 0:
                return  # already has content
            if not E8_SEED_DOMAIN:
                return
            label = E8_SEED_LABEL.strip() or f"domain:{getattr(self, 'semantic_domain', 'experience')}"
            entry = {
                "type": "seed",
                "label": label,
                "metaphor": "bootstrap seed",
                "rating": 0.6,
                "step": getattr(self, "step_num", 0),
                "source": "seed"
            }
            try:
                await self.memory.add_entry(entry)
                self.console.log(f"[seed] Inserted initial concept: {label}")
            except Exception as _e:
                self.console.log(f"[seed] Failed to insert seed concept: {_e}")
        except Exception as e:
            try:
                self.console.log(f"[seed] Error during seed check: {e}")
            except Exception:
                pass


    def get_current_cognitive_state(self) -> Optional[Any]:
        """Get current cognitive state for energy navigation"""
        try:
            state = type('CognitiveState', (), {})()
            memories = []
            if hasattr(self, 'memory') and hasattr(self.memory, 'get_recent_memories'):
                try:
                    memories = self.memory.get_recent_memories(10)
                except Exception:
                    memories = []
            positions: List[np.ndarray] = []
            for memory in memories or []:
                vec = None
                if hasattr(memory, 'vector') and memory.vector is not None:
                    vec = np.array(memory.vector, dtype=float)[:8]
                elif hasattr(memory, 'embedding') and memory.embedding is not None:
                    vec = np.array(memory.embedding, dtype=float)[:8]
                if vec is not None:
                    positions.append(vec)
            if positions:
                try:
                    stack = np.stack(positions, axis=0)
                    state.position = np.mean(stack, axis=0)
                except Exception:
                    state.position = np.zeros(8, dtype=float)
            else:
                state.position = np.zeros(8, dtype=float)
            return state
        except Exception as exc:
            try:
                self.console.log(f"[M20] Failed to get cognitive state: {exc}")
            except Exception:
                pass
            return None

    def get_recent_insights(self, count: int = 20) -> List[Dict[str, Any]]:
        """Get recent insights for novelty assessment"""
        insights: List[Dict[str, Any]] = []
        try:
            source = getattr(self.insight_agent, 'recent_insights', []) if hasattr(self, 'insight_agent') else []
            if source:
                subset = list(source)[-count:] if hasattr(source, '__iter__') else []
                for insight in subset:
                    if isinstance(insight, dict) and 'rating' in insight:
                        insights.append({'rating': float(insight['rating'])})
                    elif hasattr(insight, 'rating'):
                        try:
                            insights.append({'rating': float(insight.rating)})
                        except Exception:
                            pass
        except Exception as exc:
            try:
                self.console.log(f"[M20] Failed to get recent insights: {exc}")
            except Exception:
                pass
        return insights

    def get_recent_cognitive_data(self) -> List[np.ndarray]:
        """Get recent cognitive data for complexity assessment"""
        data: List[np.ndarray] = []
        try:
            memories = []
            if hasattr(self, 'memory') and hasattr(self.memory, 'get_recent_memories'):
                try:
                    memories = self.memory.get_recent_memories(30)
                except Exception:
                    memories = []
            for memory in memories or []:
                vec = None
                if hasattr(memory, 'embedding') and memory.embedding is not None:
                    vec = memory.embedding
                elif hasattr(memory, 'vector') and memory.vector is not None:
                    vec = memory.vector
                if vec is not None:
                    try:
                        data.append(np.array(vec, dtype=float))
                    except Exception:
                        pass
        except Exception as exc:
            try:
                self.console.log(f"[M20] Failed to get recent cognitive data: {exc}")
            except Exception:
                pass
        return data

    def _fallback_get_recent_insights(self, count: int = 20) -> List[Dict[str, Any]]:
        """Fallback recent insights collector when instance method is missing."""
        insights = []
        try:
            src = getattr(self, 'insight_agent', None)
            if src is not None and hasattr(src, 'recent_insights'):
                subset = list(src.recent_insights)[-count:]
                for item in subset:
                    if isinstance(item, dict) and 'rating' in item:
                        insights.append({'rating': float(item.get('rating', 0.0))})
                    elif hasattr(item, 'rating'):
                        try:
                            insights.append({'rating': float(item.rating)})
                        except Exception:
                            pass
        except Exception:
            pass
        return insights

    def _fallback_get_recent_cognitive_data(self) -> List[np.ndarray]:
        """Fallback cognitive data collector when instance method is missing."""
        data = []
        try:
            if hasattr(self, 'memory') and hasattr(self.memory, 'get_recent_memories'):
                memories = self.memory.get_recent_memories(20)
            else:
                memories = []
            for mem in memories or []:
                vec = None
                if hasattr(mem, 'embedding') and mem.embedding is not None:
                    vec = mem.embedding
                elif hasattr(mem, 'vector') and mem.vector is not None:
                    vec = mem.vector
                if vec is not None:
                    try:
                        data.append(np.array(vec, dtype=float))
                    except Exception:
                        pass
        except Exception:
            pass
        return data

    def get_active_hypotheses(self, max_count: int = 10) -> List[Dict[str, Any]]:
        """Get currently active/pending hypotheses"""
        hypotheses = []
        try:
            G = self.memory.graph_db.graph
            for node_id, data in G.nodes(data=True):
                if data.get("type") == "explorer_insight" and data.get("metaphor"):
                    # Check if this hypothesis has been validated
                    validation_status = data.get("validation_status", {})
                    verdict = validation_status.get("type", "unknown")
                    
                    hypothesis_info = {
                        "node_id": node_id,
                        "label": data.get("label", "Unknown"),
                        "hypothesis": data.get("metaphor", ""),
                        "rating": data.get("rating", 0.0),
                        "step": data.get("step", 0),
                        "status": verdict,
                        "uncertainty": None,
                        "created": data.get("timestamp", "Unknown")
                    }
                    
                    # Extract uncertainty from explorer schema if available
                    explorer_schema = data.get("explorer_schema", {})
                    if isinstance(explorer_schema, dict) and "uncertainty" in explorer_schema:
                        hypothesis_info["uncertainty"] = explorer_schema["uncertainty"]
                    
                    hypotheses.append(hypothesis_info)
            
            # Sort by step (most recent first) and limit
            hypotheses.sort(key=lambda x: x["step"], reverse=True)
            return hypotheses[:max_count]
            
        except Exception as e:
            self.console.log(f"[HypothesisTracker] Error getting active hypotheses: {e}")
            return []

    def display_hypothesis_dashboard(self):
        """Display a dashboard of current hypothesis status"""
        try:
            hypotheses = self.get_active_hypotheses(8)
            if not hypotheses:
                self.console.print("[dim]No active hypotheses found.[/]")
                return
                
            # Count by status
            status_counts = {}
            for h in hypotheses:
                status = h["status"]
                status_counts[status] = status_counts.get(status, 0) + 1
            
            # Display header
            self.console.print(f"\n[bold blue]📊 HYPOTHESIS DASHBOARD[/] ({len(hypotheses)} active)")
            self.console.print(f"[dim]Status Distribution: {dict(status_counts)}[/]\n")
            
            # Display each hypothesis
            for i, h in enumerate(hypotheses[:5], 1):  # Show top 5
                status_colors = {"pass": "green", "fail": "red", "unknown": "yellow", "computationally_testable": "green"}
                status_color = status_colors.get(h["status"], "white")
                
                hypothesis_preview = h["hypothesis"][:80] + "..." if len(h["hypothesis"]) > 80 else h["hypothesis"]
                
                self.console.print(f"{i}. [cyan]{h['label']}[/] [{status_color}][{h['status'].upper()}][/]")
                self.console.print(f"   [dim]Hypothesis:[/] {hypothesis_preview}")
                self.console.print(f"   [dim]Rating: {h['rating']:.3f} | Step: {h['step']} | Node: {h['node_id'][:8]}...[/]")
                if h["uncertainty"] is not None:
                    self.console.print(f"   [dim]Uncertainty: {h['uncertainty']:.2f}[/]")
                self.console.print("")
                
        except Exception as e:
            self.console.log(f"[HypothesisTracker] Error displaying dashboard: {e}")

    def process_novel_associations(self, associations: Iterable[Dict[str, Any]]):
        """Process discovered novel associations"""
        try:
            for assoc in associations or []:
                if isinstance(assoc, dict) and 'novelty_score' in assoc:
                    score = float(assoc.get('novelty_score', 0.0))
                    if score > 0.8 and hasattr(self, 'console'):
                        self.console.log(f"[M20] High-novelty association discovered: {score:.3f}")
        except Exception as exc:
            try:
                self.console.log(f"[M20] Failed to process novel associations: {exc}")
            except Exception:
                pass

    def update_memory_positions(self, transported_memories: Iterable[Any]):
        """Update memory positions after geodesic transport"""
        try:
            for memory in transported_memories or []:
                if hasattr(memory, 'position') and hasattr(memory, 'effective_dimension'):
                    pass
        except Exception as exc:
            try:
                self.console.log(f"[M20] Failed to update memory positions: {exc}")
            except Exception:
                pass

    def update_m20_metrics(self):
        """Update M20 performance metrics"""
        metrics = getattr(self, 'm20_metrics', None)
        mantle = getattr(self, 'fluid_mantle', None)
        if not isinstance(metrics, dict) or mantle is None:
            return
        try:
            metric_tensor = getattr(mantle, 'metric_tensor', None)
            if metric_tensor is not None:
                tensor = np.asarray(metric_tensor, dtype=float)
                eye = np.eye(tensor.shape[0], dtype=float)
                deviation = tensor - eye
                curv_raw = float(np.trace(np.abs(deviation)))
                # Guard curvature metric
                # curv_cap = float(os.getenv('E8_CURV_GLOBAL_CAP', '6.0'))  # COMMENTED OUT - removing 6.0 curvature limiter
                # metrics['spacetime_curvature'] = _safe_number(curv_raw, 0.0, 0.0, curv_cap)  # COMMENTED OUT - removing 6.0 curvature limiter
                metrics['spacetime_curvature'] = _safe_number(curv_raw, 0.0, 0.0, None)  # Removed cap limiter
            if hasattr(mantle, 'get_shell_energy_summary'):
                try:
                    summary = mantle.get_shell_energy_summary()
                    metrics['field_energy'] = float(summary.get('total_energy', metrics.get('field_energy', 0.0)))
                    pf_raw = float(summary.get('total_flux', metrics.get('poynting_flux', 0.0)))
                    flux_cap = float(os.getenv('E8_FLUX_GLOBAL_CAP', '100.0'))
                    metrics['poynting_flux'] = _safe_number(pf_raw, 0.0, 0.0, flux_cap)
                except Exception:
                    pass
            else:
                velocity_field = getattr(mantle, 'velocity_field', {})
                if isinstance(velocity_field, dict) and velocity_field:
                    try:
                        norms = [np.linalg.norm(np.asarray(vec, dtype=float)) for vec in velocity_field.values()]
                        pf_raw = float(np.mean(norms)) if norms else 0.0
                        flux_cap = float(os.getenv('E8_FLUX_GLOBAL_CAP', '100.0'))
                        metrics['poynting_flux'] = _safe_number(pf_raw, 0.0, 0.0, flux_cap)
                    except Exception:
                        metrics['poynting_flux'] = metrics.get('poynting_flux', 0.0)
            if hasattr(mantle, 'compute_pressure_proxy'):
                try:
                    metrics['field_pressure_proxy'] = float(np.clip(mantle.compute_pressure_proxy(), 0.0, 2.0))
                except Exception:
                    pass

            # Derive a smoothed consciousness flow rate for frontend visuals.
            # Apply EMA with anti-windup toward a target range to avoid integrator runaway.
            try:
                pf = _safe_number(metrics.get('poynting_flux', 0.0), 0.0, 0.0, float(os.getenv('E8_FLUX_GLOBAL_CAP', '100.0')))
                flow_instant = float(np.clip(pf, 0.0, 2.0))
                prev_flow = float(metrics.get('consciousness_flow_rate', 0.0))
                rho = float(os.getenv('E8_FLUX_RHO', '0.9'))
                aw_gain = float(os.getenv('E8_FLUX_AW_GAIN', '0.1'))
                target = float(os.getenv('E8_FLUX_TARGET_STD', '1.0'))
                # EMA update
                flow = rho * prev_flow + (1.0 - rho) * flow_instant
                # anti-windup: gently pull flow back inside [-target, target]
                clipped = float(np.clip(flow, -target, target))
                flow -= (flow - clipped) * aw_gain
                metrics['consciousness_flow_rate'] = flow
                try:
                    log_mod = int(os.getenv('E8_FLUX_LOG_MOD', '50'))
                    step_num = int(getattr(self, 'step_num', 0) or 0)
                    if log_mod > 0 and (step_num % log_mod == 0):
                        metrics_log('flux.flow', {
                            'pf': pf,
                            'instant': flow_instant,
                            'flow': flow,
                            'target': target,
                            'rho': rho,
                            'aw_gain': aw_gain,
                            'event': 'flux.flow'
                        })
                except Exception:
                    pass
            except Exception:
                pass
            payload = dict(metrics)
            payload['step'] = int(getattr(self, 'step_num', 0))
            payload['timestamp'] = time.time()

            # Add emergence controller metrics
            if hasattr(self, 'emergence_controller') and self.emergence_controller is not None:
                try:
                    ec_metrics = self.emergence_controller.get_emergence_metrics()
                    payload.update({
                        'emergence_mode': ec_metrics.get('emergence_mode', 'unknown'),
                        'field_energy_total': ec_metrics.get('field_energy_total', 0.0),
                        'ray_success_rate': ec_metrics.get('ray_success_rate', 0.0),
                        'mean_ray_latency': ec_metrics.get('mean_ray_latency', 0.0),
                        'rotor_activity': ec_metrics.get('rotor_activity', 0.0),
                        'rotor_pressure': ec_metrics.get('rotor_pressure', 0.0),
                        'path_cache_size': ec_metrics.get('path_cache_size', 0),
                        'coupling_rotor_to_source_gain': self.emergence_controller.couplings.get('rotor_to_source_gain', 1.0),
                        'coupling_field_damping': self.emergence_controller.couplings.get('field_damping', 0.1),
                        'coupling_curvature_feedback': self.emergence_controller.couplings.get('curvature_feedback', 1.0),
                        'coupling_use_field_rays': self.emergence_controller.couplings.get('use_field_rays', False)
                    })

                    # Add annealing status if active
                    annealing = self.emergence_controller.get_annealing_status()
                    if annealing.get('active', False):
                        payload.update({
                            'annealing_progress': annealing.get('progress', 0.0),
                            'annealing_paused': annealing.get('paused', False),
                            'annealing_pause_reason': annealing.get('pause_reason', '')
                        })

                except Exception as ec_log_err:
                    try:
                        self.console.log(f"[M20] Failed to add emergence metrics: {ec_log_err}")
                    except Exception:
                        pass

            try:
                metrics_log('m20_metrics', payload)
            except Exception:
                pass
        except Exception as exc:
            try:
                self.console.log(f"[M20] Failed to update metrics: {exc}")
            except Exception:
                pass


    def standard_cognitive_processing(self):
        """Execute the canonical M19 cognitive loop for one tick."""
        step = int(getattr(self, 'step_num', 0))
        try:
            self._update_cognitive_modules(step)
        except Exception as exc:
            try:
                self.console.log(f"[M19] Cognitive module update failed: {exc}")
            except Exception:
                pass
        buffer = getattr(self, '_autoencoder_train_buffer', [])
        batch_size = getattr(self, '_autoencoder_batch_size', 64)
        try:
            updated_buffer = self._train_autoencoder_if_ready(buffer, batch_size)
            if updated_buffer is not None:
                self._autoencoder_train_buffer = updated_buffer
        except Exception as exc:
            try:
                self.console.log(f"[M19] Autoencoder training step failed: {exc}")
            except Exception:
                pass
        try:
            self._update_emergent_metrics()
        except Exception:
            pass
        if step % 50 == 0:
            self._schedule_async_task(self._update_synaptic_plasticity())
        if step % 100 == 0 and getattr(self, 'bandit', None) is not None:
            try:
                self.bandit.log_health(step=step)
            except Exception as exc:
                try:
                    self.console.log(f"[Health] Bandit health logging failed: {exc}")
                except Exception:
                    pass
        # Reset LLM degraded flag periodically to allow recovery
        if step % 100 == 0:
            try:
                self.llm_pool.maybe_reset_degraded()
            except Exception:
                pass
        if step % 500 == 0 and step > 0:
            try:
                removed = self.memory.dedup_nodes_by_eps()
                added = self.memory.ensure_quasicrystal_edges()
                self.console.log(f"[Geometry] dedup_removed={removed}, knn_edges_added={added}")
            except Exception as exc:
                try:
                    self.console.log(f"[Geometry] hygiene skipped: {exc}")
                except Exception:
                    pass
        if hasattr(self, 'hrl'):
            try:
                self.hrl.maybe_update(step)
            except Exception:
                pass

        current_state = self._build_state_vector()
        if hasattr(self, '_wm_lazy_init') and getattr(self, 'world_model', None) is None:
            try:
                self._wm_lazy_init(current_state.size)
            except Exception as exc:
                try:
                    self.console.log(f"[WM] Lazy init failed: {exc}")
                except Exception:
                    pass
        if getattr(self, 'world_model', None) is not None and self.world_model.is_ready():
            try:
                self.world_model.imagine_with_policy(current_state, self.agent, horizon=8)
            except Exception:
                pass

        arm_index = None
        if getattr(self, 'bandit', None) is not None:
            try:
                arm_index = self.bandit.pull(context=current_state)
                active_arm = self.bandit.arms[arm_index]
                self.sigma_q = active_arm.get('diffusion_sigma', self.sigma_q)
            except Exception as exc:
                try:
                    self.console.log(f"[Bandit] Selection failed: {exc}")
                except Exception:
                    pass
                arm_index = None

        action = np.zeros(self.action_dim, dtype=np.float32)
        if getattr(self, 'agent', None) is not None:
            try:
                action = self.agent.select_action(current_state)
            except Exception as exc:
                try:
                    self.console.log(f"[Agent] Action selection failed: {exc}")
                except Exception:
                    pass

        if getattr(self, 'macro_manager', None) is not None and getattr(self, 'world_model', None) is not None:
            try:
                maybe = self.macro_manager.select_action(current_state, self)
                if maybe is not None:
                    blended = np.asarray(maybe, dtype=np.float32)
                    action = 0.7 * action + 0.3 * blended
            except Exception as exc:
                try:
                    self.console.log(f"[MacroManager] Action proposal failed: {exc}")
                except Exception:
                    pass

        try:
            # Clamp the action before applying to the manifold to ensure a stable magnitude.
            try:
                clamp_norm = getattr(self, '_action_clamp_norm', 0.04)
                clamped_action = clamp_action(action, max_norm=clamp_norm)
            except Exception:
                # Fallback to raw action if clamping fails for any reason
                clamped_action = action
            self.apply_manifold_action(clamped_action)
        except Exception as exc:
            try:
                self.console.log(f"[Manifold] Action application failed: {exc}")
            except Exception:
                pass

        prev_idx = self.prev_node_index or random.randrange(self.physics.roots.shape[0])
        # Optional Wavey integration: only call if bridge exists and Wavey is available
        try:
            if getattr(self, 'wavey_bridge', None) is not None and WAVEY_AVAILABLE:
                wavey_out = integrate_one_cycle(self, self.wavey_bridge)
                if isinstance(wavey_out, dict):
                    self._update_anchors_from_wavey(wavey_out)
        except Exception as e:
            try: self.console.log(f"[Wavey] integration skipped/failed: {e}")
            except Exception: pass

        # Quantum engine step: only run if qeng and anchors exist
        try:
            if getattr(self, 'qeng', None) is not None and getattr(self, 'anchors', None) is not None:
                V = None
                try:
                    V = self.anchors.potential()
                except Exception:
                    V = None
                if hasattr(self.qeng, 'build_hamiltonian'):
                    try:
                        self.qeng.build_hamiltonian(V=V)
                    except Exception as bex:
                        try: self.console.log(f"[QENG] build_hamiltonian failed: {bex}")
                        except Exception: pass
                if hasattr(self.qeng, 'step_adaptive'):
                    try: self.qeng.step_adaptive()
                    except Exception:
                        pass
                if hasattr(self.qeng, 'measure_hybrid'):
                    try:
                        current_node_index = self.qeng.measure_hybrid(prev_idx, sigma=self.sigma_q)[0]
                        self.prev_node_index = current_node_index
                    except Exception as exc:
                        try: self.console.log(f"[QENG] measurement failed: {exc}")
                        except Exception: pass
        except Exception as exc:
            try:
                self.console.log(f"[QENG] Quantum step failed: {exc}")
            except Exception:
                pass

        next_state = self._build_state_vector()
        try:
            base_reward = self.potential_evaluator.calculate_potential_and_get_reward()
        except Exception:
            base_reward = 0.0

        final_reward = base_reward
        if hasattr(self, 'hrl'):
            try:
                final_reward = self.hrl.shape_reward(current_state, next_state, base_reward)
            except Exception:
                final_reward = base_reward

        if hasattr(self, 'causal'):
            try:
                self.causal.update_on_step(self, clamped_action, final_reward)
            except Exception:
                pass

        if getattr(self, 'world_model', None) is not None:
            try:
                self.world_model.observe(current_state, clamped_action, next_state, base_reward)
            except Exception:
                pass

        if getattr(self, 'agent', None) is not None:
            try:
                max_steps = max(0, int(getattr(self, 'max_steps', 0)))
                is_last = bool(max_steps and step >= max_steps - 1)
                self.agent.store(current_state, clamped_action, next_state, final_reward, is_last)
                if step > 1024:
                    self.agent.update()
            except Exception as exc:
                try:
                    self.console.log(f"[Agent] Update failed: {exc}")
                except Exception:
                    pass

        if getattr(self, 'bandit', None) is not None and arm_index is not None:
            try:
                self.bandit.update(arm_index, base_reward, current_state)
            except Exception as exc:
                try:
                    self.console.log(f"[Bandit] Update failed: {exc}")
                except Exception:
                    pass

        self._prev_action = clamped_action
        self._prev_bh = self.black_hole_pressure
        
        # VAE training on cadence
        try:
            self._vae_train_if_ready(step)
        except Exception as exc:
            try:
                self.console.log(f"[VAE] Training step failed: {exc}")
            except Exception:
                pass


    def complete_cognitive_cycle(self):
        """Finalize the cognitive step by flushing metrics and background tasks."""
        step = int(getattr(self, 'step_num', 0))
        metrics = getattr(self, 'm20_metrics', None)
        if isinstance(metrics, dict):
            try:
                metrics['memory_nodes'] = self.memory.graph_db.graph.number_of_nodes()
            except Exception:
                pass
        self.update_m20_metrics()
        try:
            self._schedule_async_task(self._sse_push_telemetry())
        except Exception:
            pass
        try:
            if CONSOLE_EXPORT_EVERY_STEPS > 0 and (step + 1) % CONSOLE_EXPORT_EVERY_STEPS == 0:
                self._export_console_chunk(step)
        except Exception as exc:
            try:
                self.console.log(f"[ConsoleExport] Failed during cycle completion: {exc}")
            except Exception:
                pass
        try:
            if hasattr(self, 'metrics') and isinstance(metrics, dict):
                flux_cap = float(os.getenv('E8_FLUX_GLOBAL_CAP', '100.0'))
                flow = _safe_number(metrics.get('poynting_flux', 0.0), 0.0, 0.0, flux_cap)
                self.metrics.gauge('m20.poynting_flux', flow)
        except Exception:
            pass


    async def step(self):
        """Main step method integrating M20 processing when available"""
        try:
            if getattr(self, 'cognitive_step_m20', None) and getattr(self, 'fluid_mantle', None) is not None:
                self.cognitive_step_m20()
            else:
                self.standard_cognitive_processing()
        except Exception as err:
            try:
                self.console.log(f"[STEP] Error: {err}")
            except Exception:
                pass

    def _reset_tick_train_counter(self):
        self._train_calls_this_tick = 0

    def _can_train_more(self):
        if not getattr(self, "_throttle_initialized", False):
            self._init_training_throttle()
        if self._train_calls_this_tick >= self.TRAIN_STEPS_PER_TICK:
            return False
        if self.TRAIN_GLOBAL_RATE > 0:
            # enforce average rate: allow only if last TRAIN_GLOBAL_RATE window not saturated
            current_step = getattr(self, 'step_num', 0)
            recent = [t for t in self._train_calls_history if t >= current_step - self.TRAIN_GLOBAL_RATE]
            # Heuristic: permit up to TRAIN_STEPS_PER_TICK * (TRAIN_GLOBAL_RATE//2) in sliding window
            limit = self.TRAIN_STEPS_PER_TICK * max(1, self.TRAIN_GLOBAL_RATE // 2)
            if len(recent) >= limit:
                return False
        return True

    def _record_train_call(self):
        self._train_calls_this_tick += 1
        current_step = getattr(self, 'step_num', 0)
        self._train_calls_history.append(current_step)
        # lightweight metric
        try:
            self.metrics.gauge("train.calls_per_tick", self._train_calls_this_tick)
        except Exception:
            pass

    # --- Blueprint / KDTree Guards ---
    def ensure_blueprint(self):
        """Ensure blueprint exists before any lookup; regenerate if missing/empty."""
        try:
            if not getattr(self, 'blueprint', None):
                self.blueprint = self.physics.generate_quasicrystal_blueprint()
        except Exception:
            try:
                self.console.log("[guard] Failed to ensure blueprint; using empty list")
            except Exception:
                pass
            self.blueprint = self.blueprint if getattr(self, 'blueprint', None) else []
        return self.blueprint

    def ensure_main_kdtree(self):
        """Ensure main_kdtree is built before queries; attempt rebuild if None."""
        try:
            if self.memory.main_kdtree is None:
                self.memory._rebuild_main_kdtree()
        except Exception:
            return None
        return self.memory.main_kdtree

    async def seed_domain_if_empty(self):
        """
        If memory is empty, add one bootstrap concept using the semantic domain.
        Controlled by E8_SEED_DOMAIN flag.
        """
        try:
            # Count existing vectors across dimensional shells
            total = 0
            ds = getattr(self, "dimensional_shells", {}) or {}
            for shell in ds.values():
                try:
                    mat, _ = shell.get_all_vectors_as_matrix()
                    total += (len(mat) if mat is not None else 0)
                except Exception:
                    pass
            if total > 0:
                return  # already has content
            if not E8_SEED_DOMAIN:
                return
            label = E8_SEED_LABEL.strip() or f"domain:{getattr(self, 'semantic_domain', 'experience')}"
            entry = {
                "type": "seed",
                "label": label,
                "metaphor": "bootstrap seed",
                "rating": 0.6,
                "step": getattr(self, "step_num", 0),
                "source": "seed"
            }
            try:
                await self.memory.add_entry(entry)
                self.console.log(f"[seed] Inserted initial concept: {label}")
            except Exception as _e:
                self.console.log(f"[seed] Failed to insert seed concept: {_e}")
        except Exception as e:
            try:
                self.console.log(f"[seed] Error during seed check: {e}")
            except Exception:
                pass

    # E8 Projection Utilities for Tetra Visualization
    def e8_register_point(self, e8_coords: np.ndarray, node_id: str, shell_dim: int = 8) -> dict:
        """
        Register an E8 point for tetra visualization.
        Projects 8D coordinates to 3D and calculates shell-based radial placement.
        """
        try:
            # Ensure we have 8D coordinates
            if e8_coords.shape[0] != 8:
                # Pad or truncate to 8D if necessary
                coords_8d = np.zeros(8, dtype=np.float32)
                size_to_copy = min(len(e8_coords), 8)
                coords_8d[:size_to_copy] = e8_coords[:size_to_copy]
            else:
                coords_8d = e8_coords.astype(np.float32)

            # Find nearest E8 root for blueprint location
            root_index = self.physics.find_nearest_root_index(coords_8d)
            if root_index is None:
                root_index = 0

            # Project to 3D using the physics quasicrystal projection
            coords_3d = self.e8_project_to_3(coords_8d)

            # Calculate shell radius for radial placement
            shell_radius = self.shell_radius(shell_dim)

            # Calculate rotor axis for spinning dynamics
            rotor_axis = self.calculate_rotor_axis(coords_8d)

            tetra_data = {
                "node_id": node_id,
                "e8_coords": coords_8d.tolist(),
                "coords_3d": coords_3d,
                "blueprint_location_id": root_index,
                "shell_dim": shell_dim,
                "shell_radius": shell_radius,
                "rotor_axis": rotor_axis,
                "timestamp": time.time()
            }

            return tetra_data

        except Exception as e:
            self.console.log(f"[E8_PROJ] Error registering point {node_id}: {e}")
            return {
                "node_id": node_id,
                "e8_coords": e8_coords.tolist() if hasattr(e8_coords, 'tolist') else list(e8_coords),
                "coords_3d": [0.0, 0.0, 0.0],
                "blueprint_location_id": 0,
                "shell_dim": shell_dim,
                "shell_radius": 1.0,
                "rotor_axis": [0.0, 1.0, 0.0],
                "timestamp": time.time()
            }

    def e8_project_to_3(self, e8_coords: np.ndarray) -> list:
        """
        Project 8D E8 coordinates to 3D using the quasicrystal projection matrix.
        """
        try:
            if not hasattr(self.physics, 'projection_matrix') or self.physics.projection_matrix is None:
                # Generate projection matrix if not exists
                self.physics.generate_quasicrystal_blueprint()

            # Project 8D -> 3D
            coords_3d = e8_coords @ self.physics.projection_matrix

            # Normalize to unit sphere and scale
            norm = np.linalg.norm(coords_3d)
            if norm > 1e-6:
                coords_3d = coords_3d / norm

            return coords_3d.tolist()

        except Exception as e:
            self.console.log(f"[E8_PROJ] Error projecting to 3D: {e}")
            # Fallback: use first 3 coordinates
            return e8_coords[:3].tolist() if len(e8_coords) >= 3 else [0.0, 0.0, 0.0]

    def e8_finalize_projection(self, tetra_registry: dict) -> dict:
        """
        Finalize the E8-to-3D projection by applying shell-based radial placement
        and ensuring proper 3D positioning for tetra visualization.
        """
        try:
            finalized_tetras = {}

            for node_id, tetra_data in tetra_registry.items():
                # Apply shell-based radial placement
                base_coords = np.array(tetra_data["coords_3d"])
                shell_radius = tetra_data["shell_radius"]

                # Scale by shell radius for proper spacing
                positioned_coords = base_coords * shell_radius

                # Add some jitter to prevent exact overlaps
                jitter = np.random.normal(0, 0.01, 3)
                final_coords = positioned_coords + jitter

                # Update tetra data
                tetra_data["final_coords_3d"] = final_coords.tolist()
                tetra_data["positioned"] = True

                finalized_tetras[node_id] = tetra_data

            self.console.log(f"[E8_PROJ] Finalized projection for {len(finalized_tetras)} tetras")
            return finalized_tetras

        except Exception as e:
            self.console.log(f"[E8_PROJ] Error finalizing projection: {e}")
            return tetra_registry

    def calculate_rotor_axis(self, e8_coords: np.ndarray) -> np.ndarray:
        """
        Calculate the rotor axis for a given set of E8 coordinates.
        """
        try:
            # Normalize the E8 coordinates
            norm = np.linalg.norm(e8_coords)
            if norm > 1e-6:
                normalized_coords = e8_coords / norm
            else:
                normalized_coords = e8_coords

            # Placeholder logic for rotor axis calculation
            rotor_axis = np.zeros(8)
            rotor_axis[:len(normalized_coords)] = normalized_coords

            return rotor_axis
        except Exception as e:
            try:
                self.console.log(f"[E8_CALC] Error calculating rotor axis: {e}")
            except Exception:
                pass
            return np.zeros(8)
        """
        Calculate rotor axis for tetra spinning based on E8 coordinates.
        """
        try:
            # Use the coordinate directions to determine spin axis
            # Normalize to get a unit vector for the axis
            axis = e8_coords[:3] / (np.linalg.norm(e8_coords[:3]) + 1e-6)

            # Ensure it's a valid axis (not zero)
            if np.linalg.norm(axis) < 1e-6:
                axis = np.array([0.0, 1.0, 0.0])  # Default Y-axis

            return axis.tolist()

        except Exception as e:
            self.console.log(f"[E8_PROJ] Error calculating rotor axis: {e}")
            return [0.0, 1.0, 0.0]

        self.probe = Probe(run_id)# --- [END PATCH] ---

    def _bump_edge_weights(self, node_id: str, verdict: str, adjustment_factor: float = 0.1):
        """
        Adjust edge weights, node ratings, and temperatures based on validator verdict.
        Called after insight validation to reinforce or penalize connections.
        
        Args:
            node_id: The insight node that was validated
            verdict: "pass", "reject", or "unknown"
            adjustment_factor: How much to adjust (default 0.1)
        """
        try:
            # Use unified forward-named flag with back-compat handled in AppConfig
            if not AppConfig.VALIDATOR_WRITEBACK_ENABLED:
                return
                
            G = self.memory.graph_db.graph
            if node_id not in G:
                return
                
            # Get node data
            node_data = G.nodes[node_id]
            current_rating = node_data.get('rating', 0.5)
            
            # Determine new rating based on verdict
            if verdict == "pass":
                new_rating = min(1.0, current_rating + adjustment_factor)
                boost_factor = 1.0 + adjustment_factor
                temp_adjust = -adjustment_factor * 0.5  # Cool down temperature
                change_reason = "validator_feedback"
            elif verdict == "reject":
                new_rating = max(0.0, current_rating - adjustment_factor)
                boost_factor = 1.0 - adjustment_factor
                temp_adjust = adjustment_factor * 0.5  # Heat up temperature
                change_reason = "validation_failure"
            else:  # unknown
                new_rating = current_rating
                boost_factor = 1.0
                temp_adjust = 0.0
                change_reason = "validation_unknown"
            
            # Validate rating change using RatingValidator
            if not RatingValidator.validate_rating_change(current_rating, new_rating, change_reason):
                self.console.log(f"[VALIDATOR] Rating change rejected for {node_id[:8]}: {current_rating:.3f} -> {new_rating:.3f}")
                new_rating = current_rating  # Revert to original rating
                boost_factor = 1.0
                temp_adjust = 0.0
            
            # Update node rating
            node_data['rating'] = new_rating
            
            # Track rating metadata if unified system available
            if hasattr(self, 'unified_rating') and self.unified_rating is not None:
                metadata = self.unified_rating.create_rating_metadata(
                    value=new_rating,
                    rating_type=RatingTypes.CONCEPT_QUALITY,
                    source=f"validator_{verdict}",
                    context=f"verdict={verdict}, factor={adjustment_factor}"
                )
                node_data['rating_metadata'] = metadata
            
            # Adjust edge weights to neighbors
            for neighbor_id in G.neighbors(node_id):
                edge_data = G.get_edge_data(node_id, neighbor_id, {})
                if edge_data:
                    current_weight = edge_data.get('weight', 1.0)
                    new_weight = current_weight * boost_factor
                    # Update edge data using NetworkX method
                    G[node_id][neighbor_id]['weight'] = max(0.1, min(5.0, new_weight))
            
            # Adjust temperature if available
            if 'temperature' in node_data:
                current_temp = node_data['temperature']
                node_data['temperature'] = max(0.1, min(2.0, current_temp + temp_adjust))
            
            # Log the adjustment
            self.console.log(f"[VALIDATOR] {verdict.upper()} for node {node_id[:8]}: rating={new_rating:.3f}, edges adjusted")
            
        except Exception as e:
            self.console.log(f"[VALIDATOR] Error in writeback: {e}")

    def apply_manifold_action(self, action_vec):
        try:
            for lay in ACTION_LAYOUT:
                dim = lay["dim"]
                b0, blen, ai = lay["biv_start"], lay["biv_len"], lay["angle_idx"]
                bcoef = action_vec[b0:b0+blen]
                ang = action_vec[ai] if ai < len(action_vec) else 0.0
                shell = self.dimensional_shells.get(dim)
                if shell is not None and hasattr(shell, "spin_with_bivector"):
                    # Schedule geometry operations via EventHorizonScheduler to
                    # keep the cognitive event stream (CES) purely async and
                    # single-writer for geometry. Use non-blocking scheduling
                    # where possible, fallback to background thread if no loop.
                    try:
                        loop = asyncio.get_running_loop()
                        # schedule spin and index update without awaiting
                        if getattr(self, 'ehs', None) is not None:
                            loop.create_task(self.ehs.submit('SHELL_SPIN', dim=dim, bcoef=bcoef, angle=float(ang)))
                            loop.create_task(self.ehs.submit('UPDATE_SHELL_INDEX', dim=dim))
                    except RuntimeError:
                        # No running loop: schedule in background threads
                        import threading
                        if getattr(self, 'ehs', None) is not None:
                            threading.Thread(target=lambda: asyncio.run(self.ehs.submit('SHELL_SPIN', dim=dim, bcoef=bcoef, angle=float(ang))), daemon=True).start()
                            threading.Thread(target=lambda: asyncio.run(self.ehs.submit('UPDATE_SHELL_INDEX', dim=dim)), daemon=True).start()

                    try:
                        if hasattr(self, 'macro_manager') and self.macro_manager is not None:
                            self.macro_manager.on_action_executed(action_vec)
                    except Exception:
                        pass
        except Exception as e:
            self.console.log(f"[bold red]Error in apply_manifold_action: {e}[/bold red]")

    def _snap_to_lattice(self, vector: np.ndarray, dim: int) -> np.ndarray:
        try:
            q = (os.getenv("E8_QUANTIZER", "e8") or "e8").lower()
            cell = float(os.getenv("E8_CELL", "0.10"))
        except Exception:
            q, cell = "e8", 0.10
        # Override hook: may be a callable or a string designating forced mode
        override = getattr(self, "_quantizer_override", None)
        if override is not None:
            if callable(override):
                try:
                    return np.asarray(override(vector), dtype=np.float32)
                except Exception as e:
                    try:
                        self.console.log(f"[Quantizer] callable override error: {e}; reverting to mode '{q}'")
                    except Exception:
                        pass
            elif isinstance(override, str):
                q = override.lower().strip() or q  # force mode
        v = vector.astype(np.float32, copy=False)
        if q == "none":
            return v
        if q == "cubic":
            return (np.round(v / cell) * cell).astype(np.float32)
        if q == "random":
            try:
                seed = int(os.getenv("GLOBAL_SEED", "1337"))
            except Exception:
                seed = 1337
            rng = np.random.default_rng(seed)
            jitter = rng.normal(0.0, cell, size=v.shape).astype(np.float32)
            w = v + jitter
            return (np.round(w / cell) * cell).astype(np.float32)

        kdtree = self.shell_kdtree_indices.get(dim)
        if kdtree is None:
            return v

        _, nearest_index_arr = kdtree.query(vector.reshape(1, -1), k=1)

        try:
            scalar_index = nearest_index_arr.item()
            return self.shell_lattices[dim][scalar_index]
        except (ValueError, IndexError):
            return vector

    def _local_teacher_question(self) -> str:
        """Generate a fallback teacher question when LLM is unavailable."""
        try:
            # try bias-aware pick first
            try:
                seeds = self._subconscious_seed_terms(k_terms=4, k_nodes=10)
                if len(seeds) >= 2:
                    a, b = random.sample(seeds, 2)
                    return random.choice([
                        f"How might {a} reshape {b}?",
                        f"What unexpected link ties {a} and {b}?",
                        f"Could {a} catalyze {b} in this system?",
                        f"What pattern links {a} with {b}?"
                    ])
            except Exception:
                pass
            # Get recent memory concepts
            recent_nodes = list(self.memory.graph_db.graph.nodes(data=True))[-10:]
            concepts = []
            for _, data in recent_nodes:
                label = data.get('label', '')
                if label and len(label.split()) <= 3:  # Short, meaningful labels
                    concepts.append(label)
            
            if len(concepts) >= 2:
                concept_a, concept_b = random.sample(concepts, 2)
                templates = [
                    f"How might {concept_a} relate to {concept_b}?",
                    f"What connects {concept_a} and {concept_b}?",
                    f"Could {concept_a} influence {concept_b}?",
                    f"What patterns link {concept_a} with {concept_b}?"
                ]
                return random.choice(templates)
            elif len(concepts) >= 1:
                concept = concepts[0]
                templates = [
                    f"What are the implications of {concept}?",
                    f"How does {concept} fit into our broader understanding?",
                    f"What questions does {concept} raise?",
                    f"What might emerge from {concept}?"
                ]
                return random.choice(templates)
            else:
                # Generic fallback questions
                fallback_questions = [
                    "What connection can we draw now?",
                    "What patterns are emerging?",
                    "What should we explore next?",
                    "How do these concepts relate?",
                    "What insights can we synthesize?"
                ]
                return random.choice(fallback_questions)
        except Exception:
            return "What connection can we draw now?"

    def _subconscious_seed_terms(self, k_terms: int = 6, k_nodes: int = 12) -> List[str]:
        """
        Pull creative seeds from:
          - subconscious bias vector (nearest nodes in latent space)
          - nouns from the latest subconscious narrative
        Returns a small list of distinct, high-signal terms.
        """
        terms: List[str] = []

        # 1) nearest nodes to subconscious bias
        try:
            bias = self.subconscious.get_bias() if hasattr(self, 'subconscious') else None
            if bias is not None and isinstance(bias, np.ndarray) and np.linalg.norm(bias) > 0:
                # Project to 8D shell if your proximity expects it; else use full emb
                vec8 = getattr(getattr(self, 'memory', None), 'sdm', None)
                vec8_fn = getattr(vec8, "_get_vec8d", None)
                qvec = vec8_fn(bias) if callable(vec8_fn) else bias
                try:
                    dim_hint = 8 if isinstance(qvec, np.ndarray) and qvec.shape[0] == 8 else None
                except Exception:
                    dim_hint = None
                if hasattr(self, 'proximity_engine') and qvec is not None:
                    if dim_hint is not None:
                        near = self.proximity_engine.find_similar_in_shell(qvec, dim=dim_hint, k=k_nodes)
                    else:
                        # Fallback: try 8D if available, otherwise skip
                        try:
                            near = self.proximity_engine.find_similar_in_shell(qvec, dim=8, k=k_nodes)
                        except Exception:
                            near = []
                    for node_id, _dist in near or []:
                        try:
                            nd = self.memory.graph_db.get_node(node_id)
                        except Exception:
                            nd = None
                        if not nd:
                            continue
                        lbl = (nd.get("label") or nd.get("title") or nd.get("metaphor") or "").strip()
                        if lbl:
                            terms.append(lbl)
        except Exception:
            pass

        # 2) nouns from latest subconscious narrative
        try:
            text = (getattr(self.subconscious, "narrative", "") or "")[:1500].lower()
            nouns = re.findall(r"\b[a-z]{3,}\b", text)
            # crude down-selection: keep rarer words that also appear in graph tokens
            graph_tokens: set[str] = set()
            try:
                nodes_tail = list(self.memory.graph_db.graph.nodes(data=True))[-200:]
            except Exception:
                nodes_tail = []
            for _, d in nodes_tail:
                if not isinstance(d, dict):
                    continue
                for f in ("label","metaphor","text","content","summary","title","description"):
                    v = d.get(f)
                    if v:
                        try:
                            graph_tokens.update(re.findall(r"[a-z0-9]{3,}", str(v).lower()))
                        except Exception:
                            continue
            nouns = [w for w in nouns if w in graph_tokens]
            terms.extend(nouns[:k_terms])
        except Exception:
            pass

        # de-dup and trim
        seen: set[str] = set()
        out: List[str] = []
        for t in terms:
            try:
                t = t.strip()
            except Exception:
                continue
            if t and t.lower() not in seen:
                out.append(t)
                seen.add(t.lower())
            if len(out) >= k_terms:
                break
        return out

    def _local_explorer_answer(self, question: str) -> str:
        """Generate a fallback explorer answer when LLM is unavailable."""
        try:
            # Extract key terms from the question
            terms = re.findall(r"[A-Za-z]{3,}", question.lower())
            key_terms = [t for t in terms if t not in {'what', 'how', 'why', 'the', 'and', 'are', 'can', 'might', 'could', 'does', 'will'}]
            
            # Get recent memory concepts that might relate to the question
            recent_nodes = list(self.memory.graph_db.graph.nodes(data=True))[-15:]
            related_concepts = []
            
            for _, data in recent_nodes:
                label = data.get('label', '').lower()
                metaphor = data.get('metaphor', '').lower()
                content = f"{label} {metaphor}"
                
                # Check if any question terms appear in this concept
                if any(term in content for term in key_terms):
                    related_concepts.append(data.get('label', ''))
            
            if related_concepts:
                concept = random.choice(related_concepts)
                templates = [
                    f"This suggests {concept} may play a key role in the relationship.",
                    f"The pattern seems to involve {concept} as a connecting element.",
                    f"Evidence points toward {concept} being central to this question.",
                    f"The data indicates {concept} might bridge these concepts."
                ]
                return random.choice(templates)
            else:
                # Generic analytical responses
                generic_responses = [
                    "The available evidence suggests multiple potential connections worth exploring.",
                    "This question reveals interesting patterns in the conceptual network.",
                    "The relationship appears complex and may require deeper investigation.",
                    "Current data indicates several possible interpretations of this connection.",
                    "The pattern suggests emergent properties that warrant further analysis."
                ]
                return random.choice(generic_responses)
        except Exception:
            return "The available evidence suggests multiple potential connections worth exploring."

    async def _teacher_ask_new_question(self):
        # Increase cold-start threshold so the Teacher waits for a larger memory graph
        # Default moved from 6 to 100 as requested (can still be overridden via E8_MIN_FOR_TEACHER env)
        min_for_teacher = max(int(os.getenv("E8_MIN_FOR_TEACHER", "100")), 1)
        current_nodes = self.memory.graph_db.graph.number_of_nodes()
        if current_nodes == 0:
            return
        # Ensure the shared lock exists (backwards-compatibility for older runs)
        try:
            if not hasattr(self, 'teacher_explorer_lock') or self.teacher_explorer_lock is None:
                self.teacher_explorer_lock = asyncio.Lock()
        except Exception:
            # As a last resort create a simple lock
            self.teacher_explorer_lock = asyncio.Lock()

        # Instrumentation: log entry to help diagnose silent failures
        try:
            self.console.log(f"[TEACHER] entering _teacher_ask_new_question step={getattr(self, 'step_num', '?')}")
        except Exception:
            pass

        async with self.teacher_explorer_lock:
            try:
                # Reset previous outcome early to avoid stale question usage
                self.teacher_question = None
                frontier_insights = []
                G = self.memory.graph_db.graph
                for node_id, data in G.nodes(data=True):
                    if data.get("type") == "explorer_insight" and not any(G.get_edge_data(node_id, n, {}).get("type") == "reflection_source" for n in G.neighbors(node_id)):
                        frontier_insights.append((node_id, data))
                frontier_insights.sort(key=lambda x: x[1].get("step", 0), reverse=True)
                top_goal_name, top_goal_desc = self.goal_field.get_top_goals(k=1)[0]
                self._teacher_question_context_ids = []
                
                # Initialize teacher_temp early so it's available in both branches
                base_temp = float(os.getenv("E8_TEACHER_TEMPERATURE", "0.75"))
                teacher_temp = min(1.25, base_temp + 0.08 * 0)  # Default sub_level to 0 for frontier branch
                
                if len(frontier_insights) > 1 and random.random() > 0.4:
                    id_A, data_A = frontier_insights[1]; id_B, data_B = frontier_insights[0]
                    self._teacher_question_context_ids = [id_A, id_B]
                    # Apply metric-causal attention light-cone gating to teacher context when possible
                    try:
                        origin_id = None
                        try:
                            recent = self.get_active_cognitive_nodes() or []
                            if recent:
                                origin_id = recent[0]
                        except Exception:
                            origin_id = None
                        if origin_id is None and hasattr(self.memory, 'last_accessed_node_id'):
                            origin_id = self.memory.last_accessed_node_id
                        shell_dim = None
                        if origin_id and hasattr(self, 'dimensional_shells'):
                            for d in DIMENSIONAL_SHELL_SIZES:
                                shell = self.dimensional_shells.get(d)
                                if not shell or not getattr(shell, 'vectors', None):
                                    continue
                                if origin_id in shell.vectors and (id_A in shell.vectors or id_B in shell.vectors):
                                    shell_dim = d
                                    break
                        if origin_id and shell_dim and hasattr(self, 'proximity_engine'):
                            candidates = [id_A, id_B]
                            gated = self.proximity_engine.filter_by_light_cone(origin_id, candidates, shell_dim)
                            if gated:
                                self._teacher_question_context_ids = gated[:2]
                            try:
                                metrics_log("teacher.light_cone", {"event": "teacher.cone", "origin": origin_id, "dim": shell_dim, "candidates": len(candidates), "kept": len(gated or [])})
                            except Exception:
                                pass
                    except Exception:
                        pass
                    prompt = (f"Goal: '{top_goal_desc}'.\nInsight A: '{data_A.get('metaphor', '')}'\nInsight B: '{data_B.get('metaphor', '')}'.\n\n"
                              "Ask one concise hypothesis-generating question (under 20 words) that would lead to a TESTABLE prediction "
                              "about the connection between A and B. The question MUST include key terms from either Insight A or B.")
                else:
                    # Get the last 20 recent node IDs from the reliable deque.
                    recent_node_ids = list(self.memory.recent_nodes)[-20:]
                    # Apply light-cone gating on recent nodes for causal grounding if possible
                    try:
                        origin_id = None
                        try:
                            recent = self.get_active_cognitive_nodes() or []
                            if recent:
                                origin_id = recent[0]
                        except Exception:
                            origin_id = None
                        if origin_id is None and hasattr(self.memory, 'last_accessed_node_id'):
                            origin_id = self.memory.last_accessed_node_id
                        shell_dim = None
                        if origin_id and hasattr(self, 'dimensional_shells'):
                            for d in DIMENSIONAL_SHELL_SIZES:
                                shell = self.dimensional_shells.get(d)
                                if not shell or not getattr(shell, 'vectors', None):
                                    continue
                                if origin_id in shell.vectors:
                                    candidate_ids_here = [cid for cid in recent_node_ids if cid in shell.vectors]
                                    if candidate_ids_here:
                                        shell_dim = d
                                        if hasattr(self, 'proximity_engine'):
                                            filtered_ids = self.proximity_engine.filter_by_light_cone(origin_id, candidate_ids_here, shell_dim)
                                            if filtered_ids:
                                                recent_node_ids = [nid for nid in recent_node_ids if nid in set(filtered_ids)]
                                            try:
                                                metrics_log("teacher.light_cone", {"event": "teacher.recent", "origin": origin_id, "dim": shell_dim, "candidates": len(candidate_ids_here), "kept": len(filtered_ids or [])})
                                            except Exception:
                                                pass
                                        break
                    except Exception:
                        pass
                    recent_nodes_data = [self.memory.graph_db.get_node(nid) for nid in recent_node_ids if self.memory.graph_db.get_node(nid)]
                    
                    # Create the snippet for the prompt from the last 4 valid concepts.
                    memory_snippet = "\n".join(
                        f"- {n.get('label', '')}: {n.get('metaphor', '')}" 
                        for n in recent_nodes_data[-4:] if n.get('label')
                    )
                    
                    # Create the prompt using the reliable snippet.
                    # Subconscious awareness dial (0..5)
                    sub_level = int(os.getenv("E8_TEACHER_SUB_AWARE", "0") or "0")
                    sub_level = 0 if sub_level < 0 else 5 if sub_level > 5 else sub_level
                    sub_strength_words = {0:"not at all",1:"slightly",2:"moderately",3:"strongly",4:"very strongly",5:"dominantly"}
                    sub_strength_word = sub_strength_words.get(sub_level, "slightly")
                    sub_hint_parts, sub_terms = [], []
                    if sub_level > 0:
                        try:
                            subcon_narr = (getattr(self.subconscious, "narrative", "") or "").strip()
                            if subcon_narr:
                                subcon_narr = subcon_narr.replace("\n", " ")[:160]
                                sub_hint_parts.append(f"- Subconscious: {subcon_narr}")
                                sub_terms.extend(re.findall(r"[A-Za-z0-9]{3,}", subcon_narr.lower()))
                        except Exception:
                            pass
                        try:
                            nodes = list(self.memory.graph_db.graph.nodes(data=True))[-400:]
                            dream_titles = [d.get("label","") for _, d in nodes if isinstance(d, dict) and d.get("type") == "dream"]
                            dream_titles = [t for t in dream_titles if t][:3]
                            if dream_titles:
                                sub_hint_parts.append(f"- Dreams: {', '.join(dream_titles)}")
                                for t in dream_titles:
                                    sub_terms.extend(re.findall(r"[A-Za-z0-9]{3,}", t.lower()))
                        except Exception:
                            pass
                    sub_hint_block = "\n".join(sub_hint_parts)
                    alpha = [0.00, 0.15, 0.30, 0.50, 0.70, 1.00][sub_level] if sub_level in (0,1,2,3,4,5) else 0.0
                    base_temp = float(os.getenv("E8_TEACHER_TEMPERATURE", "0.75"))
                    teacher_temp = min(1.25, base_temp + 0.08 * sub_level)

                    # Seed extractor + temperature boost knobs
                    seed_terms: List[str] = []
                    try:
                        _k = int(os.getenv("E8_TEACHER_SUB_K", "6"))
                        seed_terms = self._subconscious_seed_terms(k_terms=_k)
                    except Exception:
                        seed_terms = []
                    sub_gain = float(os.getenv("E8_TEACHER_SUB_GAIN", "0.12"))
                    use_temp = min(1.35, (teacher_temp if isinstance(teacher_temp, (int, float)) else 0.8) + (sub_gain if seed_terms else 0.0))
                    domain_hint = f"Favor at least one of: {', '.join(seed_terms[:6])}." if seed_terms else ""
                    if sub_level > 0 and sub_hint_block:
                        prompt = (
                            f"Goal: '{top_goal_desc}'.\n"
                            f"Recent thoughts:\n{memory_snippet}\n"
                            f"Subconscious hints ({sub_strength_word} consider these motifs):\n{sub_hint_block}\n\n"
                            f"{domain_hint}\n"
                            "Briefly imagine 1–2 plausible next states and ask ONE concise, profound HYPOTHESIS-GENERATING question (≤20 words) "
                            "that leads to testable predictions about the path forward. "
                            "The question MUST include a key term from 'Recent thoughts' OR an echoed motif from the hints "
                            "(do not quote verbatim)."
                        )
                    else:
                        prompt = (
                            f"Goal: '{top_goal_desc}'.\n"
                            f"Recent thoughts:\n{memory_snippet}\n\n"
                            f"{domain_hint}\n"
                            "Briefly imagine 1–2 plausible next states and ask ONE concise, profound HYPOTHESIS-GENERATING question (≤20 words) "
                            "that leads to testable predictions to advance the goal. The question MUST include at least one key term from 'Recent thoughts'."
                        )
                question = await self._async_call_llm_internal(
                    "", _prompt_key="teacher.question", _prompt_vars={"question": prompt},
                    max_tokens=80, temperature=use_temp if 'use_temp' in locals() else teacher_temp
                )

                # Build grounding/top_memory_terms from the reliable recent_nodes deque
                top_memory_terms = []
                try:
                    stopwords = {'the', 'and', 'for', 'with', 'this', 'that', 'are', 'was', 'what', 'how'}
                    # recent_nodes_data was built above from recent_nodes deque
                    for node in recent_nodes_data:
                        content = f"{node.get('label','')} {node.get('metaphor','')}"
                        terms = re.findall(r"[A-Za-z0-9]{3,}", content.lower())
                        for t in terms:
                            if t not in stopwords:
                                top_memory_terms.append(t)
                    top_memory_terms = list(dict.fromkeys(top_memory_terms))[:8]
                except Exception:
                    top_memory_terms = []

                # Determine if we should perform grounding check/regeneration
                # Option A: skip grounding checks when the graph is small to avoid
                # getting stuck in a regenerate/reject loop during early runs.
                try:
                    total_nodes = len(self.memory.graph_db.graph.nodes())
                except Exception:
                    total_nodes = 0
                small_graph = total_nodes < int(os.getenv('E8_TEACHER_SMALL_GRAPH_NODES', '25'))

                # If we have too few grounding terms, we should attempt regeneration
                regen_needed = len(top_memory_terms) < 3

                needs_grounding_check = (regen_needed or (len(top_memory_terms) > 2)) and (not small_graph)
                if needs_grounding_check:
                    memory_grounded = any(t.lower() in (question or "").lower() for t in top_memory_terms)
                    common_terms = ["ai", "artificial", "intelligence", "learning", "neural", "network", "data", "model", "algorithm", "system", "memory"]
                    common_grounded = any(t.lower() in (question or "").lower() for t in common_terms)
                    sub_grounded = False
                    if sub_level >= 3 and sub_terms:
                        k_sub = max(1, int(round(alpha * 4)))
                        allow = list(dict.fromkeys(sub_terms))[:k_sub]
                        sub_grounded = any(t in (question or '').lower() for t in allow)
                    grounded = memory_grounded or common_grounded or sub_grounded

                    if not grounded:
                        self.console.log("[TEACHER] Question lacks memory grounding, regenerating with constraints.")
                        constrained_prompt = prompt + f"\n\nTry to include one of these terms if possible: {', '.join(top_memory_terms[:4])}"
                        try:
                            question = await self._async_call_llm_internal(
                                "", _prompt_key="teacher.question", _prompt_vars={"question": constrained_prompt},
                                max_tokens=80, temperature=min(1.25, teacher_temp + 0.05)
                            )
                        except Exception:
                            question = question  # keep original

                        # Final checks after regeneration
                        final_memory_grounded = any(t.lower() in (question or "").lower() for t in top_memory_terms)
                        final_common_grounded = any(t.lower() in (question or "").lower() for t in common_terms)
                        # Fix missing final_sub_grounded variable
                        final_sub_grounded = False
                        if sub_level >= 3 and sub_terms:
                            k_sub = max(1, int(round(alpha * 4)))
                            allow = list(dict.fromkeys(sub_terms))[:k_sub]
                            final_sub_grounded = any(t in (question or '').lower() for t in allow)
                        has_reasonable_length = len((question or "").split()) >= 3

                        # Option B: accept reasonable questions even if they lack
                        # explicit grounding to avoid endless regeneration loops.
                        if not (final_memory_grounded or final_common_grounded or final_sub_grounded):
                            if has_reasonable_length:
                                self.console.log("[TEACHER] No grounding but reasonable length; accepting to avoid loop.")
                                # allow fallthrough which will accept this question after filtering
                            else:
                                self.console.log("[TEACHER] Regenerated question still lacks grounding and is too short, rejecting.")
                                self.teacher_question = None
                                return

                # Build graph_terms robustly by scanning multiple likely text fields
                # (label/metaphor/summary/text/content/etc). This is more tolerant to
                # different node schemas and increases the chance teacher grounding
                # finds a meaningful overlap with the generated question.
                fields_to_scan = ['label', 'metaphor', 'text', 'content', 'body', 'summary', 'note', 'title', 'description']
                tokens = []
                try:
                    for _, node_data in self.memory.graph_db.graph.nodes(data=True):
                        # Ensure node_data behaves like a mapping
                        if not isinstance(node_data, dict):
                            try:
                                node_data = dict(node_data)
                            except Exception:
                                continue
                        for f in fields_to_scan:
                            v = node_data.get(f)
                            if not v:
                                continue
                            try:
                                v_text = str(v).lower()
                            except Exception:
                                v_text = ''
                            # take alphanumeric tokens length>=3 to avoid stopwords/noise
                            tokens.extend(re.findall(r"[A-Za-z0-9]{3,}", v_text))
                except Exception:
                    tokens = []

                graph_terms = set(tokens)

                accepted = teacher_prompt_ok(
                    question, graph_terms,
                    total_graph_nodes=len(self.memory.graph_db.graph.nodes()),
                    telemetry_cb=lambda ev: metrics_log("teacher.prompt_filter", ev))
                if not accepted:
                    # Attempt a single relaxed regeneration with explicit term list
                    if seed_terms:
                        relaxed_prompt = prompt + f"\n\nRegenerate under 18 words and include ≥1 of: {', '.join(seed_terms[:6])}."
                    else:
                        regen_terms = ", ".join(top_memory_terms[:6])
                        relaxed_prompt = prompt + f"\n\nRegenerate a different concise question (<20 words) that includes at least one of: {regen_terms}" if regen_terms else prompt
                    try:
                        regen = await self._async_call_llm_internal(
                            "", _prompt_key="teacher.question", _prompt_vars={"question": relaxed_prompt},
                            max_tokens=40, temperature=min(1.25, (use_temp if 'use_temp' in locals() else teacher_temp) + 0.05)
                        )
                        if regen:
                            question = regen
                            accepted = teacher_prompt_ok(
                                question, graph_terms,
                                total_graph_nodes=len(self.memory.graph_db.graph.nodes()),
                                telemetry_cb=lambda ev: metrics_log("teacher.prompt_filter", ev))
                    except Exception:
                        pass

                if not accepted or (not question or question.startswith("[LLM")):
                    # Local fallback (integrated former M17 patch logic)
                    try:
                        q_local = self._local_teacher_question()
                        self.teacher_question = sanitize_line(q_local, max_chars=140)
                        metrics_log("teacher.fallback_used", {"step": self.step_num})
                    except Exception as e:
                        self.console.log(f"[TEACHER] fallback generation failed: {e}")
                        self.teacher_question = "What connection can we draw now?"
                        metrics_log("teacher.ultimate_fallback_used", {"step": self.step_num})
                else:
                    self.teacher_question = str(question).strip().replace('"', '')

                # Ensure teacher_log exists and append a concise record
                try:
                    if not hasattr(self, 'teacher_log') or self.teacher_log is None:
                        self.teacher_log = []
                    self.teacher_log.append({"step": self.step_num, "q": self.teacher_question})
                except Exception:
                    # Swallow errors here to avoid breaking the teacher flow
                    pass
                try:
                    self.console.log(f"[TEACHER] produced question: {self.teacher_question}")
                except Exception:
                    pass
                try:
                    if self.teacher_question: 
                        self.current_task_embedding = await self.get_embedding(self.teacher_question)
                except Exception as e:
                    self.console.log(f"[TEACHER] embedding generation failed: {e}")
                
                async with self.console_lock:
                    try:
                        self.console.print(Panel(
                            f"[bold white]{sanitize_block(self.teacher_question, 2, 240)}[/]",
                            title="[bold cyan]🧑‍🏫 TEACHER[/]", border_style="cyan"
                        ))
                    except Exception as e:
                        # Log the specific error to help debug Panel issues
                        self.console.log(f"[TEACHER] Rich Panel formatting failed: {e}")
                        # Enhanced fallback that maintains some visual structure
                        self.console.print("â”Œâ”€ ðŸŽ“ TEACHER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
                        for line in sanitize_block(self.teacher_question, 2, 240).split('\n'):
                            self.console.print(f"â”‚ {line[:55]:<55} â”‚")
                        self.console.print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â””")
            except Exception as e:
                self.console.log(f"[TEACHER] skipped (error): {e}")
                # Final fallback if exception occurred
                try:
                    q_local = self._local_teacher_question()
                    if q_local:
                        self.teacher_question = sanitize_line(q_local, max_chars=140)
                        metrics_log("teacher.exception_fallback_used", {"step": self.step_num, "reason": str(e)})
                        return
                except Exception as fallback_e:
                    self.console.log(f"[TEACHER] exception fallback also failed: {fallback_e}")
                self.teacher_question = "What connection can we draw now?"  # Ultimate fallback

    async def _explorer_answer_pending_question(self):

        # Instrumentation: log entry to help diagnose silent failures
        try:
            self.console.log(f"[TEACHER] entering _teacher_ask_new_question step={getattr(self, 'step_num', '?')}")
        except Exception:
            pass

        async with self.teacher_explorer_lock:
            try:
                # Reset previous outcome early to avoid stale question usage
                self.teacher_question = None
                frontier_insights = []
                G = self.memory.graph_db.graph
                for node_id, data in G.nodes(data=True):
                    if data.get("type") == "explorer_insight" and not any(G.get_edge_data(node_id, n, {}).get("type") == "reflection_source" for n in G.neighbors(node_id)):
                        frontier_insights.append((node_id, data))
                frontier_insights.sort(key=lambda x: x[1].get("step", 0), reverse=True)
                top_goal_name, top_goal_desc = self.goal_field.get_top_goals(k=1)[0]
                self._teacher_question_context_ids = []
                if len(frontier_insights) > 1 and random.random() > 0.4:
                    id_A, data_A = frontier_insights[1]; id_B, data_B = frontier_insights[0]
                    self._teacher_question_context_ids = [id_A, id_B]
                    prompt = (f"Goal: '{top_goal_desc}'.\nInsight A: '{data_A.get('metaphor', '')}'\nInsight B: '{data_B.get('metaphor', '')}'.\n\n"
                              "Ask one concise hypothesis-generating question (under 20 words) that would lead to a TESTABLE prediction "
                              "about the connection between A and B. The question MUST include key terms from either Insight A or B.")
                else:
                    # Get the last 20 recent node IDs from the reliable deque.
                    recent_node_ids = list(self.memory.recent_nodes)[-20:]
                    recent_nodes_data = [self.memory.graph_db.get_node(nid) for nid in recent_node_ids if self.memory.graph_db.get_node(nid)]
                    
                    # Create the snippet for the prompt from the last 4 valid concepts.
                    memory_snippet = "\n".join(
                        f"- {n.get('label', '')}: {n.get('metaphor', '')}" 
                        for n in recent_nodes_data[-4:] if n.get('label')
                    )
                    
                    # Create the prompt using the reliable snippet.
                    # Seed extractor + temperature boost knobs (short form path)
                    seed_terms: List[str] = []
                    try:
                        _k = int(os.getenv("E8_TEACHER_SUB_K", "6"))
                        seed_terms = self._subconscious_seed_terms(k_terms=_k)
                    except Exception:
                        seed_terms = []
                    teacher_temp = float(os.getenv("E8_TEACHER_TEMP", "0.8"))
                    sub_gain = float(os.getenv("E8_TEACHER_SUB_GAIN", "0.12"))
                    use_temp = min(1.35, teacher_temp + (sub_gain if seed_terms else 0.0))
                    domain_hint = f"Favor at least one of: {', '.join(seed_terms[:6])}." if seed_terms else ""

                    prompt = (f"Goal: '{top_goal_desc}'.\nRecent thoughts:\n{memory_snippet}\n{domain_hint}\n\nAsk one profound, short question (under 20 words) to advance the goal. "
                              "The question MUST include at least one key term from the 'Recent thoughts' provided.")

                question = await self._async_call_llm_internal(
                    "", _prompt_key="teacher.question", _prompt_vars={"question": prompt},
                    max_tokens=40, temperature=use_temp if 'use_temp' in locals() else 0.75
                )

                # Build grounding/top_memory_terms from the reliable recent_nodes deque
                top_memory_terms = []
                try:
                    stopwords = {'the', 'and', 'for', 'with', 'this', 'that', 'are', 'was', 'what', 'how'}
                    # recent_nodes_data was built above from recent_nodes deque
                    for node in recent_nodes_data:
                        content = f"{node.get('label','')} {node.get('metaphor','')}"
                        terms = re.findall(r"[A-Za-z0-9]{3,}", content.lower())
                        for t in terms:
                            if t not in stopwords:
                                top_memory_terms.append(t)
                    top_memory_terms = list(dict.fromkeys(top_memory_terms))[:8]
                except Exception:
                    top_memory_terms = []

                # Determine if we should perform grounding check/regeneration
                # Option A: skip grounding checks when the graph is small to avoid
                # getting stuck in a regenerate/reject loop during early runs.
                try:
                    total_nodes = len(self.memory.graph_db.graph.nodes())
                except Exception:
                    total_nodes = 0
                small_graph = total_nodes < int(os.getenv('E8_TEACHER_SMALL_GRAPH_NODES', '25'))

                # If we have too few grounding terms, we should attempt regeneration
                regen_needed = len(top_memory_terms) < 3

                needs_grounding_check = (regen_needed or (len(top_memory_terms) > 2)) and (not small_graph)
                if needs_grounding_check:
                    memory_grounded = any(t.lower() in (question or "").lower() for t in top_memory_terms)
                    common_terms = ["ai", "artificial", "intelligence", "learning", "neural", "network", "data", "model", "algorithm", "system", "memory"]
                    common_grounded = any(t.lower() in (question or "").lower() for t in common_terms)
                    grounded = memory_grounded or common_grounded

                    if not grounded:
                        self.console.log("[TEACHER] Question lacks memory grounding, regenerating with constraints.")
                        constrained_prompt = prompt + f"\n\nTry to include one of these terms if possible: {', '.join(top_memory_terms[:4])}"
                        try:
                            question = await self._async_call_llm_internal(
                                "", _prompt_key="teacher.question", _prompt_vars={"question": constrained_prompt},
                                max_tokens=40, temperature=0.8
                            )
                        except Exception:
                            question = question  # keep original

                        # Final checks after regeneration
                        final_memory_grounded = any(t.lower() in (question or "").lower() for t in top_memory_terms)
                        final_common_grounded = any(t.lower() in (question or "").lower() for t in common_terms)
                        has_reasonable_length = len((question or "").split()) >= 3

                        # Option B: accept reasonable questions even if they lack
                        # explicit grounding to avoid endless regeneration loops.
                        if not (final_memory_grounded or final_common_grounded):
                            if has_reasonable_length:
                                self.console.log("[TEACHER] No grounding but reasonable length; accepting to avoid loop.")
                                # allow fallthrough which will accept this question after filtering
                            else:
                                self.console.log("[TEACHER] Regenerated question still lacks grounding and is too short, rejecting.")
                                self.teacher_question = None
                                return

                # Build graph_terms robustly by scanning multiple likely text fields
                # (label/metaphor/summary/text/content/etc). This is more tolerant to
                # different node schemas and increases the chance teacher grounding
                # finds a meaningful overlap with the generated question.
                fields_to_scan = ['label', 'metaphor', 'text', 'content', 'body', 'summary', 'note', 'title', 'description']
                tokens = []
                try:
                    for _, node_data in self.memory.graph_db.graph.nodes(data=True):
                        # Ensure node_data behaves like a mapping
                        if not isinstance(node_data, dict):
                            try:
                                node_data = dict(node_data)
                            except Exception:
                                continue
                        for f in fields_to_scan:
                            v = node_data.get(f)
                            if not v:
                                continue
                            try:
                                v_text = str(v).lower()
                            except Exception:
                                v_text = ''
                            # take alphanumeric tokens length>=3 to avoid stopwords/noise
                            tokens.extend(re.findall(r"[A-Za-z0-9]{3,}", v_text))
                except Exception:
                    tokens = []

                graph_terms = set(tokens)

                accepted = teacher_prompt_ok(
                    question, graph_terms,
                    total_graph_nodes=len(self.memory.graph_db.graph.nodes()),
                    telemetry_cb=lambda ev: metrics_log("teacher.prompt_filter", ev))
                if not accepted:
                    # Attempt a single relaxed regeneration with explicit term list
                    if 'seed_terms' in locals() and seed_terms:
                        relaxed_prompt = prompt + f"\n\nRegenerate under 18 words and include ≥1 of: {', '.join(seed_terms[:6])}."
                    else:
                        regen_terms = ", ".join(top_memory_terms[:6])
                        relaxed_prompt = prompt + f"\n\nRegenerate a different concise question (<20 words) that includes at least one of: {regen_terms}" if regen_terms else prompt
                    try:
                        regen = await self._async_call_llm_internal(
                            "", _prompt_key="teacher.question", _prompt_vars={"question": relaxed_prompt},
                            max_tokens=40, temperature=min(1.25, (use_temp if 'use_temp' in locals() else 0.75) + 0.05)
                        )
                        if regen:
                            question = regen
                            accepted = teacher_prompt_ok(
                                question, graph_terms,
                                total_graph_nodes=len(self.memory.graph_db.graph.nodes()),
                                telemetry_cb=lambda ev: metrics_log("teacher.prompt_filter", ev))
                    except Exception:
                        pass

                if not accepted or (not question or question.startswith("[LLM")):
                    # Local fallback (integrated former M17 patch logic)
                    try:
                        q_local = self._local_teacher_question()
                        self.teacher_question = sanitize_line(q_local, max_chars=140)
                        metrics_log("teacher.fallback_used", {"step": self.step_num})
                    except Exception as e:
                        self.console.log(f"[TEACHER] fallback generation failed: {e}")
                        self.teacher_question = "What connection can we draw now?"
                        metrics_log("teacher.ultimate_fallback_used", {"step": self.step_num})
                else:
                    self.teacher_question = str(question).strip().replace('"', '')

                # Ensure teacher_log exists and append a concise record
                try:
                    if not hasattr(self, 'teacher_log') or self.teacher_log is None:
                        self.teacher_log = []
                    self.teacher_log.append({"step": self.step_num, "q": self.teacher_question})
                except Exception:
                    # Swallow errors here to avoid breaking the teacher flow
                    pass
                try:
                    self.console.log(f"[TEACHER] produced question: {self.teacher_question}")
                except Exception:
                    pass
                try:
                    if self.teacher_question: 
                        self.current_task_embedding = await self.get_embedding(self.teacher_question)
                except Exception as e:
                    self.console.log(f"[TEACHER] embedding generation failed: {e}")
                
                async with self.console_lock:
                    try:
                        self.console.print(Panel(
                            f"[bold white]{sanitize_block(self.teacher_question, 2, 240)}[/]",
                            title="[bold cyan]🧑‍🏫 TEACHER[/]", border_style="cyan"
                        ))
                    except Exception as e:
                        # Log the specific error to help debug Panel issues
                        self.console.log(f"[TEACHER] Rich Panel formatting failed: {e}")
                        # Enhanced fallback that maintains some visual structure
                        self.console.print("â”Œâ”€ ðŸŽ“ TEACHER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
                        for line in sanitize_block(self.teacher_question, 2, 240).split('\n'):
                            self.console.print(f"â”‚ {line[:55]:<55} â”‚")
                        self.console.print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
            except Exception as e:
                self.console.log(f"[TEACHER] skipped (error): {e}")
                # Final fallback if exception occurred
                try:
                    q_local = self._local_teacher_question()
                    if q_local:
                        self.teacher_question = sanitize_line(q_local, max_chars=140)
                        metrics_log("teacher.exception_fallback_used", {"step": self.step_num, "reason": str(e)})
                        return
                except Exception as fallback_e:
                    self.console.log(f"[TEACHER] exception fallback also failed: {fallback_e}")
                self.teacher_question = "What connection can we draw now?"  # Ultimate fallback

    async def _explorer_answer_pending_question(self):
        async with self.teacher_explorer_lock:
            q = getattr(self, "teacher_question", None)
            if not q: return
            answer = ""
            label = ""
            explorer_meta = None  # Initialize to ensure it's always defined
            try:
                unified_prompt = (
                    "You are the Explorer module generating TESTABLE HYPOTHESES. Produce a STRICT first-line JSON object with keys: "
                    "claim (single testable hypothesis as declarative sentence), evidence_refs (array of terse strings max 4), "
                    "uncertainty (0..1 float, higher = less certain), actions (array of 1-2 concrete validation steps), "
                    "tags (array of short topical tokens). No extra keys. THEN a blank line and a concise hypothesis explanation (<=4 sentences) "
                    "that includes WHY this hypothesis is testable and HOW it could be validated.\n"
                    f"Question: {q}\nGenerate a testable hypothesis. First line JSON:"
                )
                raw = await self._async_call_llm_internal(unified_prompt, max_tokens=220, temperature=0.8)
                if raw and not raw.startswith("[LLM"):
                    lines = raw.splitlines()
                    first_line = lines[0] if lines else ""
                    payload = _parse_json_object(first_line)

                    # Extract structured fields safely
                    claim = (payload.get('claim') or '').strip()
                    evidence_refs = payload.get('evidence_refs') if isinstance(payload.get('evidence_refs'), list) else []
                    uncertainty = payload.get('uncertainty')
                    try:
                        uncertainty = float(uncertainty) if uncertainty is not None else 0.5
                    except (ValueError, TypeError):
                        uncertainty = 0.5
                    # clamp uncertainty to [0,1]
                    try:
                        uncertainty = max(0.0, min(1.0, float(uncertainty)))
                    except Exception:
                        uncertainty = 0.5
                    actions = payload.get('actions') if isinstance(payload.get('actions'), list) else []
                    tags = [t for t in (payload.get('tags') or []) if isinstance(t, str)] if payload.get('tags') is not None else []

                    # Prefer the prose section after the first JSON line (per Explorer spec)
                    prose = "\n".join(lines[1:]).strip() if len(lines) > 1 else ""
                    if prose:
                        answer = prose
                    elif claim:
                        # Use just the claim, not the raw JSON
                        answer = claim
                    else:
                        # Last resort: clean text from raw, avoiding JSON
                        answer = raw if raw and not raw.strip().startswith('{') else "No hypothesis generated"

                    # Friendly label
                    label = ' '.join(claim.split()[:6]) if claim else 'Explorer answer'

                    # Build sanitized schema
                    explorer_meta = {
                        'claim': claim,
                        'evidence_refs': [str(e) for e in evidence_refs][:4],
                        'uncertainty': float(uncertainty),
                        'actions': [str(a) for a in actions][:2],
                        'tags': [str(t) for t in tags][:8]
                    }
                if not answer:
                    answer = raw if raw and not raw.startswith("[LLM") else ""
                if not label:
                    # quick heuristic fallback label from first 6 words
                    label = "Explorer answer"
                    if answer:
                        label = " ".join(answer.strip().split()[:6])[:42]
            except Exception as e:
                self.console.log(f"[EXPLORER] unified call error: {e}")
                answer = ""
                label = ""
            
            # Fallback if still empty or looks like failure
            if (not answer) or answer.startswith("[LLM"):
                try:
                    local_ans = self._local_explorer_answer(q)
                    if local_ans:
                        answer = sanitize_block(local_ans, max_sentences=3, max_chars=420)
                        label = label or "Local hypothesis"
                        metrics_log("explorer.fallback_used", {"step": self.step_num})
                    else:
                        answer = "(no answer)"
                        label = label or "Explorer silence"
                except Exception as e:
                    self.console.log(f"[EXPLORER] fallback generation error: {e}")
                    answer = "(no answer)"
                    label = "Explorer silence"

            safe = escape(label or "Unknown")
            async with self.console_lock:
                try:
                    self.console.print(Panel(
                        f"[bold white]{sanitize_block(answer, 10, 1000)}[/]",
                        title=f"[bold green]🧭 EXPLORER[/] · {safe[:42]}{'…' if len(safe)>42 else ''}",
                        border_style="green"
                    ))
                except Exception as e:
                    # Log the specific error to help debug Panel issues
                    self.console.log(f"[EXPLORER] Rich Panel formatting failed: {e}")
                    # Enhanced fallback that maintains some visual structure
                    self.console.print("â”Œâ”€ ðŸ” EXPLORER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
                    self.console.print(f"â”‚ {label[:55]:<55} â”‚")
                    self.console.print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
                    for line in sanitize_block(answer, 2, 240).split('\n'):
                        self.console.print(f"â”‚ {line[:55]:<55} â”‚")
                    self.console.print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

            if answer and answer not in ("(no answer)",):
                # Rate the answer, but don't let failures stop processing
                try:
                    rating = await self.rate_concept(f"{label}: {answer}")
                except Exception as e:
                    rating = None
                    try:
                        self.metrics_log({'event': 'explorer.rating_error', 'error': str(e)})
                    except Exception:
                        pass

                try:
                    # Build a safe meta payload
                    safe_label = sanitize_line(label or "Explorer answer", max_chars=80)
                    safe_answer = sanitize_block(answer or "", max_sentences=6, max_chars=2000)
                    meta_payload = {
                        "type": "explorer_insight",
                        "label": safe_label,
                        "metaphor": safe_answer,
                        "rating": float(rating) if rating is not None else None,
                        "step": self.step_num,
                    }
                    if explorer_meta is not None:
                        meta_payload['explorer_schema'] = explorer_meta

                    # Ensure parent_ids is a list
                    parent_ids = getattr(self, '_teacher_question_context_ids', None) or []
                    if not isinstance(parent_ids, (list, tuple)):
                        parent_ids = [parent_ids]

                    new_node_id = await self.memory.add_entry(meta_payload, parent_ids=parent_ids)

                    # Enhanced hypothesis logging for visibility
                    try:
                        self.console.print(f"\n[bold yellow]💡 HYPOTHESIS GENERATED[/]: [cyan]{safe_label}[/]")
                        self.console.print(f"[dim]   Content:[/] {safe_answer[:200]}{'...' if len(safe_answer) > 200 else ''}")
                        self.console.print(f"[dim]   Rating:[/] {rating:.3f}" if rating is not None else "[dim]   Rating: pending[/]")
                        self.console.print(f"[dim]   Node ID:[/] {new_node_id[:12]}...")
                        if explorer_meta and 'uncertainty' in explorer_meta:
                            self.console.print(f"[dim]   Uncertainty:[/] {explorer_meta['uncertainty']:.2f}")
                    except Exception:
                        # Fallback to simple logging
                        self.console.log(f"[HYPOTHESIS] {safe_label}: {safe_answer[:100]}...")

                    try:
                        await self._append_insight_log({
                            "run_id": getattr(self, "run_id", None),
                            "step": int(self.step_num),
                            "type": "explorer_insight",
                            "node_id": new_node_id,
                            "label": safe_label,
                            "content": safe_answer,
                            "schema": explorer_meta,
                            "rating": float(rating) if rating is not None else None,
                            "question": q,
                            "parent_ids": list(parent_ids),
                        })
                    except Exception:
                        # best-effort logging
                        pass

                    self.explorer_last_answer = answer
                    self.last_teacher_question = q
                    try:
                        self.subconscious_event_log.append({'type': 'teacher_explorer', 'step': self.step_num, 'data': {'q': q, 'a': answer}})
                    except Exception:
                        pass
                    try:
                        self.drives.reward("curiosity", 0.15)
                    except Exception:
                        pass
                except Exception as e:
                    # Make sure post-processing failures are non-fatal
                    try:
                        self.console.log(f"[EXPLORER] post-processing error: {e}")
                    except Exception:
                        pass
            # Present final explorer outcome using the same green bordered Panel as other explorer prints
            try:
                base_label = (self.explorer_last_answer or "Explorer").strip()
                truncated_label = base_label[:42]
                ellipsis = '…' if len(base_label) > len(truncated_label) else ''
                safe_label = escape(truncated_label)
                quiet_label = sanitize_line(truncated_label or "Explorer", max_chars=42)

                if getattr(self, 'quiet', False):
                    summary = sanitize_line(answer or "", max_chars=120)
                    try:
                        self.console.log(f"[explorer quiet] {quiet_label}: {summary}")
                    except Exception:
                        pass
                else:
                    async with self.console_lock:
                        try:
                            self.console.print(Panel(
                                f"[bold white]{sanitize_block(answer, 6, 800)}[/]",
                                title=f"[bold green]🧭 EXPLORER[/] · {safe_label}{ellipsis}",
                                border_style="green"
                            ))
                        except Exception:
                            # fallback to single-line dimmed log if Panel fails
                            try:
                                self.console.log(f"[dim][Explorer Final Outcome] {answer}[/dim]")
                            except Exception:
                                self.console.log(f"[Explorer Final Outcome] {answer}")
            except Exception:
                # ensure we never crash on logging the final outcome
                try:
                    self.console.log(f"[Explorer Final Outcome] {answer}")
                except Exception:
                    pass
            self._teacher_question_context_ids, self.teacher_question = [], None

    def _on_market_tick(self, symbol: str, tick: dict):
        self.market_last[symbol] = tick.get("p", 0.0)

    def _on_market_bar(self, symbol: str, timeframe: str, bar: Bar):
        self.market_last_bar[(symbol, timeframe)] = bar

    async def critique_and_refine(self, thought: str, goal_desc: str) -> str:
        """
        Critiques a thought against a goal and refines it.
        This is a practical application of Constitutional AI principles.
        """
        if not thought or not goal_desc:
            return thought

        try:
            critique_prompt = self.prompts.render(
                "critique_thought",
                thought=thought,
                goal=goal_desc
            )
            critique = await self.llm_pool.enqueue_and_wait(
                critique_prompt, max_tokens=600, temperature=0.4
            )
            if critique and "[NO CHANGE]" in critique:
                return thought

            refine_prompt = self.prompts.render(
                "refine_thought",
                thought=thought,
                critique=critique,
                goal=goal_desc
            )
            refined_thought = await self.llm_pool.enqueue_and_wait(
                refine_prompt, max_tokens=600, temperature=0.7
            )

            if refined_thought and not refined_thought.startswith("[LLM"):
                async with self.console_lock:
                    self.console.print(Panel(f"[dim]Original:[/] {thought}\n[bold]Refined:[/] {refined_thought}",
                                       title="[bold yellow]SELF-CRITIQUE[/]", border_style="red"))
                return refined_thought.strip()
        except Exception as e:
            self.console.log(f"[Self-Critique] Error during refinement: {e}")

        return thought

    async def _append_proximity_log(self, record: dict):
        try:
            rec = dict(record)
            rec["ts"] = datetime.now(timezone.utc).isoformat()
            line = json.dumps(rec, ensure_ascii=False)
            async with self._prox_lock:
                with open(self.proximity_log_path, "a", encoding="utf-8") as f:
                    f.write(line + "\n")
        except Exception as e:
            self.console.log(f"[PROX-LOG] write failed: {e}")

    async def _show_fallback_proximity_alert(self, message: str):
        """Show a fallback proximity alert when normal proximity detection fails."""
        try:
            if getattr(self, 'quiet', False):
                try:
                    self.console.log(f"[RAY ALERT suppressed] {sanitize_line(message, 200)}")
                except Exception:
                    pass
                return
            # Suppress fallback if a successful proximity alert already occurred this step
            if getattr(self, '_last_proximity_success_step', -1) == getattr(self, 'step_num', None):
                return
            async with self.console_lock:
                self.console.print(Panel(
                    f"[yellow]⚠️ {message}[/]\n"
                    f"Step: {self.step_num}\n"
                    f"Status: [dim]Ray/geodesic fallback mode[/]",
                    title="[bold red]RAY ALERT (FALLBACK)[/]", 
                    border_style="red"
                ))
            
            # Log fallback alerts too
            await self._append_proximity_log({
                "step": int(self.step_num),
                "source_dim": None,
                "target_dim": None,
                "source_id": None,
                "target_id": None,
                "source_label": "N/A",
                "source": {"name": "N/A"},
                "target_label": "N/A",
                "target": {"name": "N/A"},
                "distance": None,
                "hypothesis": message,
                "fallback": True
            })
            # Record a minimal fallback event for frontend telemetry
            try:
                evt = {
                    "step": int(self.step_num),
                    "source_dim": None,
                    "target_dim": None,
                    "source_label": "N/A",
                    "target_label": "N/A",
                    "distance": None,
                    "tier": "fallback",
                    "score": None,
                    "percentile": None,
                }
                self._last_proximity_event = dict(evt)
                self._proximity_events.append(evt)
            except Exception:
                pass
        except Exception as e:
            self.console.log(f"[PROXIMITY] Fallback alert failed: {e}")

    async def _show_wormhole_alert(self, message: str = "Ray proximity detected", tier: str = "ALERT", e8_tier: str = None):
        """Show a RAY ALERT (formerly wormhole) with customizable tier and message."""
        try:
            async with self.console_lock:
                if e8_tier:
                    title = f"[bold red]RAY ALERT {tier.upper()} • {e8_tier.upper()}[/]"
                else:
                    title = f"[bold red]RAY ALERT {tier.upper()}[/]"
                
                self.console.print(Panel(
                    f"⚡ {message}\n"
                    f"Step: {self.step_num}\n"
                    f"Status: [dim]Ray/geodesic analysis active[/]",
                    title=title, 
                    border_style="red"
                ))
            
            # Log wormhole alerts
            await self._append_proximity_log({
                "step": int(self.step_num),
                "source_dim": None,
                "target_dim": None,
                "source_id": None,
                "target_id": None,
                "source_label": "RAY_ALERT",
                "source": {"name": "RAY_ALERT"},
                "target_label": "DETECTED",
                "target": {"name": "DETECTED"},
                "distance": 0.0,
                "hypothesis": message,
                "ray_alert": True,
                "tier": tier,
                "e8_tier": e8_tier
            })
            # Record a simple wormhole event for frontend telemetry
            try:
                evt = {
                    "step": int(self.step_num),
                    "source_dim": None,
                    "target_dim": None,
                    "source_label": "RAY_ALERT",
                    "target_label": "DETECTED",
                    "distance": 0.0,
                    "tier": tier,
                    "score": 1.0,
                    "percentile": 100.0,
                    "stats": {
                        "p_lock": 0.5,
                        "p_strong": 0.7,
                        "p_moderate": 0.9
                    }
                }
                self._last_proximity_event = dict(evt)
                self._proximity_events.append(evt)
            except Exception:
                pass
        except Exception as e:
            self.console.log(f"[RAY ALERT] Alert failed: {e}")

    # Preferred alias going forward; keep the underlying implementation centralized
    async def _show_ray_alert(self, message: str = "Ray proximity detected", tier: str = "ALERT", e8_tier: str = None):
        return await self._show_wormhole_alert(message=message, tier=tier, e8_tier=e8_tier)

    async def _run_insight_cycle(self):
        if self.insight_cycle_lock.locked():
            return
        async with self.insight_cycle_lock:
            # Improved multi-attempt proximity discovery to reduce false fallback alerts.
            # We now:
            #  1. Filter shells to only those that currently contain vectors.
            #  2. Attempt several distinct (source,target) pairs (default up to 5) before concluding no match.
            #  3. For each attempt, if target shell empty or query returns only self, we retry another pair.
            #  4. Only emit fallback if ALL attempts fail.

            try:
                max_attempts = int(os.getenv("E8_PROXIMITY_MAX_ATTEMPTS", "5") or 5)
            except Exception:
                max_attempts = 5

            # Collect dims that actually have vectors
            populated_dims = [d for d in DIMENSIONAL_SHELL_SIZES if getattr(self.dimensional_shells[d], 'vectors', None)]
            populated_dims = [d for d in populated_dims if len(self.dimensional_shells[d].vectors) > 0]

            # Optional mood / temperature bias: prefer shells with higher recent activity / temperature when enabled
            use_mood_bias = os.getenv("E8_PROXIMITY_USE_MOOD", "1") != "0"
            dim_weights: Dict[int, float] = {}
            if use_mood_bias and hasattr(self, 'memory'):
                for d in populated_dims:
                    shell = self.dimensional_shells[d]
                    try:
                        # Cached temperature proxy
                        avg_t = float(self.proximity_engine.get_avg_temperature(d)) if hasattr(self, 'proximity_engine') else 0.0
                        # Mood entropy influence (higher entropy -> weight outer dims slightly more)
                        mood_entropy = 0.0
                        if hasattr(self, 'mood_engine') and getattr(self.mood_engine, 'state', None):
                            mood_entropy = float(self.mood_engine.state.get('entropy', 0.0))
                        # Combine factors (tanh limiting)
                        weight = 1.0 + 0.5 * math.tanh(avg_t) + 0.2 * mood_entropy
                        dim_weights[d] = max(0.05, weight)
                    except Exception:
                        dim_weights[d] = 1.0
                # Normalize
                total_w = sum(dim_weights.values()) or 1.0
                for k in dim_weights:
                    dim_weights[k] /= total_w
            else:
                for d in populated_dims:
                    dim_weights[d] = 1.0 / max(1, len(populated_dims))

            if len(populated_dims) < 2:
                await self._show_fallback_proximity_alert("Insufficient populated shells for proximity analysis (need â‰¥2)")
                return

            tried_pairs = set()
            success_payload = None

            for attempt in range(max_attempts):
                # Random distinct pair not tried yet
                remaining_pairs = []
                for sd in populated_dims:
                    for td in populated_dims:
                        if td == sd:
                            continue
                        if (sd, td) in tried_pairs:
                            continue
                        remaining_pairs.append((sd, td))
                if not remaining_pairs:
                    break
                # Weighted choice if mood bias active: weight by product of dim weights
                if use_mood_bias and dim_weights:
                    pair_weights = []
                    for (sd, td) in remaining_pairs:
                        w = dim_weights.get(sd, 1.0) * dim_weights.get(td, 1.0)
                        pair_weights.append(max(1e-6, w))
                    total_pw = sum(pair_weights)
                    r = random.random() * total_pw
                    cumulative = 0.0
                    chosen = remaining_pairs[0]
                    for (pair, w) in zip(remaining_pairs, pair_weights):
                        cumulative += w
                        if r <= cumulative:
                            chosen = pair
                            break
                    source_dim, target_dim = chosen
                else:
                    source_dim, target_dim = random.choice(remaining_pairs)
                tried_pairs.add((source_dim, target_dim))

                source_shell = self.dimensional_shells[source_dim]
                if not source_shell.vectors:
                    continue  # Should not happen after filtering

                # Pick a random node id in source shell
                try:
                    random_node_id = random.choice(list(source_shell.vectors.keys()))
                except IndexError:
                    continue

                query_vector = source_shell.get_vector(random_node_id)
                if query_vector is None:
                    continue

                # Request k=3 to give us some alternatives besides self
                try:
                    raw_results = self.proximity_engine.cross_dimensional_query(query_vector, source_dim, target_dim, k=3)
                except Exception as qe:
                    try:
                        self.console.log(f"[PROXIMITY] cross_dimensional_query error ({source_dim}->{target_dim}): {qe}")
                    except Exception:
                        pass
                    continue

                # Filter out self & duplicates, keep first valid
                seen = set([random_node_id])
                filtered = []
                for nid, dist in raw_results:
                    if nid in seen:
                        continue
                    seen.add(nid)
                    filtered.append((nid, dist))
                    if len(filtered) >= 1:
                        break

                if not filtered:
                    continue  # Try another pair

                connected_node_id, distance = filtered[0]
                # Adaptive classification
                classification = None
                try:
                    classification = self.proximity_engine.classify(target_dim, distance)
                except Exception:
                    classification = None
                self.register_proximity_alert(distance)

                # Percentile-based gravitational lock gating (more permissive display)
                # Note: some classify() implementations return 'raw' when stats are not
                # available. Treat 'raw' (and other unknown markers) the same as None
                # for the purpose of fallback gating so small distances still trigger
                # ray-alert display/locks.
                tier = (classification or {}).get('tier') if classification else None
                unknown_tier_values = (None, 'raw', 'unknown', '')
                is_tier_unknown = tier in unknown_tier_values

                # Show RAY ALERT for 'lock', 'strong', or fallback distance < 0.7
                should_show_ray_alert = (tier in ['lock', 'strong'] or 
                                      (is_tier_unknown and distance < 0.7))

                # Assign gravitational lock if explicit 'lock' OR fallback distance indicates a lock
                if tier == 'lock' or (is_tier_unknown and distance < 0.5):
                    self.gravitational_lock_target = (random_node_id, connected_node_id)

                A = self.memory.graph_db.get_node(random_node_id) or {}
                B = self.memory.graph_db.get_node(connected_node_id) or {}
                a_label = sanitize_line(A.get("label") or random_node_id, 60)
                b_label = sanitize_line(B.get("label") or connected_node_id, 60)
                a_meta = sanitize_line(A.get("metaphor") or "", 160)
                b_meta = sanitize_line(B.get("metaphor") or "", 160)

                hypothesis = ""
                try:
                    prompt = (
                        "Write one short, plain sentence (â‰¤24 words) that explains a possible connection between A and B"
                        "Avoid hype. Be concrete."
                        f"A (title): {a_label}\n"
                        f"A (content): {a_meta}\n"
                        f"B (title): {b_label}\n"
                        f"B (content): {b_meta}\n"
                        "Sentence:"
                    )
                    resp = await asyncio.wait_for(
                        self.llm_pool.enqueue_and_wait(prompt, max_tokens=60, temperature=0.6),
                        timeout=HYPOTHESIS_TIMEOUT
                    )
                    if isinstance(resp, str) and not resp.startswith("[LLM"):
                        hypothesis = sanitize_line(resp, 180)
                except Exception:
                    pass
                if not hypothesis:
                    hypothesis = f"Possible link: {a_label} â†” {b_label}."

                success_payload = {
                    "source_dim": source_dim,
                    "target_dim": target_dim,
                    "random_node_id": random_node_id,
                    "connected_node_id": connected_node_id,
                    "distance": _safe_number(distance, 0.0, 0.0, None),
                    "a_label": a_label,
                    "b_label": b_label,
                    "a_meta": a_meta,
                    "b_meta": b_meta,
                    "hypothesis": hypothesis,
                    "classification": classification
                }
                break  # We found a match

            if not success_payload:
                # All attempts failed
                attempted_pairs = ", ".join([f"{sd}->{td}" for (sd, td) in tried_pairs]) or "none"
                await self._show_fallback_proximity_alert(
                    f"No proximity matches after {len(tried_pairs)} attempts (pairs tried: {attempted_pairs})"
                )
                return

            # Emit success alert
            distance = _safe_number(success_payload["distance"], 0.0, 0.0, None)
            source_dim = success_payload["source_dim"]
            target_dim = success_payload["target_dim"]
            random_node_id = success_payload["random_node_id"]
            connected_node_id = success_payload["connected_node_id"]
            a_label = success_payload["a_label"]
            b_label = success_payload["b_label"]
            a_meta = success_payload.get("a_meta", "")
            b_meta = success_payload.get("b_meta", "")
            hypothesis = success_payload["hypothesis"]

            # Mark success step to avoid redundant fallback in same cognitive step
            self._last_proximity_success_step = getattr(self, 'step_num', 0)

            classification = success_payload.get('classification') if isinstance(success_payload, dict) else None
            stats = (classification or {}).get('stats') if classification else None
            # Compute local curvature (kappa) at endpoints for RAY telemetry
            kappa_a = None
            kappa_b = None
            try:
                if hasattr(self, 'proximity_engine') and hasattr(self, 'field_mantle') and getattr(self.field_mantle, 'get_shell_field_state', None):
                    shell_a = self.dimensional_shells.get(source_dim)
                    shell_b = self.dimensional_shells.get(target_dim)
                    if shell_a and shell_a.vectors and random_node_id in shell_a.vectors:
                        pos_a = shell_a.vectors.get(random_node_id)
                        fs_a = self.field_mantle.get_shell_field_state(source_dim) or {}
                        kappa_a = float(abs(self.proximity_engine._compute_local_curvature(pos_a, source_dim, fs_a)))
                    if shell_b and shell_b.vectors and connected_node_id in shell_b.vectors:
                        pos_b = shell_b.vectors.get(connected_node_id)
                        fs_b = self.field_mantle.get_shell_field_state(target_dim) or {}
                        kappa_b = float(abs(self.proximity_engine._compute_local_curvature(pos_b, target_dim, fs_b)))
            except Exception:
                kappa_a, kappa_b = kappa_a, kappa_b
            kappa_hit_thresh = getattr(getattr(self, 'proximity_engine', None), 'kappa_hit_threshold', 1.0)
            kappa_hit = None
            try:
                if kappa_a is not None or kappa_b is not None:
                    vals = [v for v in [kappa_a, kappa_b] if v is not None]
                    kappa_hit = (max(vals) >= float(kappa_hit_thresh)) if vals else None
            except Exception:
                kappa_hit = None

            # Optional path health telemetry (same-shell only for now)
            ph_len = ph_attn = ph_action = ph_max_kappa = ph_ms = None
            try:
                path_health_on = os.getenv("E8_RAY_PATH_HEALTH_ON", "1") != "0"
                if path_health_on and source_dim == target_dim and hasattr(self, 'proximity_engine'):
                    tracer = self.proximity_engine._get_tracer()
                    asset = tracer.trace(random_node_id, connected_node_id, source_dim)
                    if asset:
                        ph_len = float(asset.length)
                        ph_attn = float(asset.attenuation)
                        ph_action = float(asset.action)
                        try:
                            ph_ms = int(getattr(asset, 'computation_time', 0.0) * 1000)
                        except Exception:
                            ph_ms = None
                        try:
                            # Compute max |kappa| along waypoints
                            fs = self.field_mantle.get_shell_field_state(source_dim) if hasattr(self, 'field_mantle') else None
                            shell = self.dimensional_shells.get(source_dim)
                            if fs and shell and getattr(shell, 'vectors', None) and getattr(asset, 'waypoints', None):
                                mx = None
                                for nid in asset.waypoints:
                                    p = shell.vectors.get(nid)
                                    if p is None:
                                        continue
                                    try:
                                        kv = abs(self.proximity_engine._compute_local_curvature(p, source_dim, fs))
                                        mx = kv if mx is None else max(mx, kv)
                                    except Exception:
                                        pass
                                if mx is not None:
                                    ph_max_kappa = float(mx)
                        except Exception:
                            ph_max_kappa = ph_max_kappa
                        try:
                            metrics_log("proximity.path_health", {"event": "path.health", "dim": source_dim, "len": ph_len, "attn": ph_attn, "action": ph_action, "max_kappa": ph_max_kappa, "ms": ph_ms})
                        except Exception:
                            pass
            except Exception:
                pass

            try:
                log_payload = {
                    "step": int(self.step_num),
                    "source_dim": int(source_dim),
                    "target_dim": int(target_dim),
                    "source_id": random_node_id,
                    "target_id": connected_node_id,
                    "source_label": a_label,
                    "source": {"name": a_label},
                    "target_label": b_label,
                    "target": {"name": b_label},
                    "distance": _safe_number(distance, 0.0, 0.0, None),
                    "tier": (classification or {}).get('tier'),
                    "score": (classification or {}).get('score'),
                    "percentile": (classification or {}).get('percentile'),
                    "stats_mean": (stats or {}).get('mean') if stats else None,
                    "stats_p_lock": (stats or {}).get('p_lock') if stats else None,
                    "stats_p_strong": (stats or {}).get('p_strong') if stats else None,
                    "stats_p_moderate": (stats or {}).get('p_moderate') if stats else None,
                    "hypothesis": hypothesis,
                    "attempts": len(tried_pairs),
                    "kappa_source": kappa_a,
                    "kappa_target": kappa_b,
                    "kappa_hit": kappa_hit,
                    # Path health (may be None when cross-shell)
                    "path_length": ph_len,
                    "path_attenuation": ph_attn,
                    "path_action": ph_action,
                    "path_max_kappa": ph_max_kappa,
                    "path_ms": ph_ms,
                }
                await self._append_proximity_log(log_payload)
            except Exception:
                pass
            # Telemetry for frontend suppressed in this build

            # Only display if ray-alert criteria met (lock/strong tier or high distance)
            if should_show_ray_alert and E8_UI_RAY_ALERTS:
                try:
                    entry = build_alert_entry(
                        source_label=a_label,
                        target_label=b_label,
                        source_dim=int(source_dim),
                        target_dim=int(target_dim),
                        distance=float(distance) if distance is not None else None,
                        tier=(classification or {}).get('tier') if classification else None,
                        score=(classification or {}).get('score') if classification else None,
                        hypothesis=hypothesis,
                        extra={"geo_len": float(ph_len)} if (ph_len is not None) else None,
                    )
                except Exception:
                    entry = None
                if entry is not None:
                    try:
                        render_ray_alert(entry, console=self.console)
                    except Exception:
                        pass
                    
                    # Create proximity event for frontend telemetry
                    try:
                        stats = (classification or {}).get('stats') if classification else None
                        proximity_evt = {
                            "step": int(getattr(self, 'step_num', 0)),
                            "source_dim": int(source_dim),
                            "target_dim": int(target_dim),
                            "source_label": a_label,
                            "target_label": b_label,
                            "distance": float(distance) if distance is not None else 0.0,
                            "tier": (classification or {}).get('tier') if classification else 'unknown',
                            "score": (classification or {}).get('score') if classification else None,
                            "percentile": (classification or {}).get('percentile', 0) / 100.0 if classification and classification.get('percentile') is not None else None,
                            "mean": stats.get('mean') if stats else None,
                            "p_lock": stats.get('p_lock') if stats else None,
                            "p_strong": stats.get('p_strong') if stats else None,
                            "stats": stats,
                            "hypothesis": hypothesis,
                            "kappa_a": kappa_a,
                            "kappa_b": kappa_b,
                            "kappa_hit": kappa_hit
                        }
                        self._last_proximity_event = dict(proximity_evt)
                        self._proximity_events.append(proximity_evt)
                    except Exception as prox_err:
                        pass

    def _ensure_insight_log_state(self):
        if not hasattr(self, "_insight_log_inited"):
            path = get_path("logs/insights.ndjson", getattr(self, "run_id", "run"))
            os.makedirs(os.path.dirname(path), exist_ok=True)
            self.insight_log_path = path
            self._insight_lock = asyncio.Lock()
            self._insight_log_inited = True

    async def _append_insight_log(self, record: dict):
        self._ensure_insight_log_state()
        try:
            line = json.dumps({**record, "ts": datetime.now(timezone.utc).isoformat()}, ensure_ascii=False)
            async with self._insight_lock:
                with open(self.insight_log_path, "a", encoding="utf-8") as f:
                    f.write(line + "\n")
        except Exception as e:
            self.console.log(f"[INSIGHT-LOG] write failed: {e}")

    async def _generate_subconscious_narrative(self):
        all_events = self.subconscious_event_log + self.black_hole_log
        self.subconscious_event_log.clear()
        self.black_hole_log.clear()
        if not all_events:
            return
        all_events.sort(key=lambda x: x.get('step', self.step_num))
        await self.subconscious.generate_narrative_summary(all_events)

    async def _generate_internal_monologue_step(self, step_num, current_node_index, prev_node_index):
        if prev_node_index is None:
            return
        try:
            delta = self.physics.roots[current_node_index] - self.physics.roots[prev_node_index]
            themes = classify_geometry_theme(delta)

            theme_str = themes[0] if themes else "stillness"
            prompt = (f"You are the mind's inner voice, verbalizing a single, fleeting moment of thought. "
                      f"Your current subconscious narrative is: \"{self.subconscious.narrative}\"\n"
                      f"Your mood is: {self.mood.describe()}\n"
                      f"The physical sensation of this thought was a movement of '{theme_str}'.\n\n"
                      "Describe this single, instantaneous event in a single, short, first-person sentence.")
            thought_sentence = await self.llm_pool.enqueue_and_wait(prompt, max_tokens=60, temperature=0.4)
            if thought_sentence and not thought_sentence.startswith("[LLM"):
                async with self.console_lock:
                    self.console.print(Panel(f"[italic white]{thought_sentence}[/]", title=f"[bold #A9A9A9]Inner Monologue | Step {step_num}[/]", border_style="#A9A9A9"))
                if random.random() < 0.1:
                    rating = await self.rate_concept(thought_sentence)
                    if rating > 0.6:
                        await self.memory.add_entry({"type": "monologue_thought", "label": sanitize_line(thought_sentence, 25), "metaphor": thought_sentence,
                                                     "rating": rating, "step": step_num})
        except Exception as e:
            async with self.console_lock:
                self.console.log(f"[Monologue Error] Step {step_num} failed: {e}")

    async def _generate_phase_summary(self):
        """Generates a summary of the most recent phase of thought."""
        try:
            recent_nodes = [
                d.get('label', 'untitled')
                for _, d in self.memory.graph_db.graph.nodes(data=True)
                if isinstance(d, dict) and not d.get("folded") and d.get('type') == 'concept'
            ][-10:]

            if len(recent_nodes) < 3:
                return

            prompt = (f"Concepts explored recently: {', '.join(recent_nodes)}. "
                      f"Synthesize these into a one-sentence summary of this phase of thought.")

            summary = await self.llm_pool.enqueue_and_wait(prompt, max_tokens=600, temperature=0.6)
            if not summary or summary.startswith("[LLM"):
                return

            label_prompt = f'Create a 3-4 word title for this summary: "{summary}"'
            label = await self.llm_pool.enqueue_and_wait(label_prompt, max_tokens=15, temperature=0.5)
            if not label or label.startswith("[LLM"):
                return

            await self.memory.add_entry({
                "label": label.strip().replace('"', ''),
                "type": "phase_summary",
                "metaphor": summary,
                "rating": 0.8,
                "step": self.step_num
            })
            self.console.print(Panel(
                f"New summary created: '{label}'",
                title="[bold orange]PHASE[/bold orange]",
                border_style="dark_orange"
            ))
        except Exception as e:
            self.console.log(f"[bold red]Failed to generate phase summary: {e}[/bold red]")

    async def _generate_meta_reflection(self):
        """Reflects on recent internal monologues to find higher-level insights."""
        try:
            refl_file = self._path("reflections.txt")
            if not os.path.exists(refl_file):
                return

            with open(refl_file, "r", encoding="utf-8") as f:
                content = f.read()

            recent_egos = re.findall(r"--- Step \d+ ---\n(.*?)(?=\n--- Step|\Z)", content, re.DOTALL)[-5:]
            if not recent_egos:
                return

            prompt = ("Reflect on these recent internal monologues:\n" +
                      "\n".join(f"- {e.strip()}" for e in recent_egos) +
                      "\n\nWhat pattern or concept emerges? Synthesize a new, higher-level insight.")

            reflection = await self.llm_pool.enqueue_and_wait(prompt, temperature=0.8, max_tokens=250)
            if not reflection or reflection.startswith("[LLM"):
                return

            label_prompt = f'Summarize this insight in a 3-5 word title: "{reflection}"'
            label = await self.llm_pool.enqueue_and_wait(label_prompt, max_tokens=20)
            if not label or label.startswith("[LLM"):
                return

            await self.memory.add_entry({
                "label": label.strip().replace('"', ''),
                "type": "meta_reflection",
                "metaphor": reflection,
                "rating": 0.85,
                "step": self.step_num
            })
            self.mood.process_event("reflection")
            self.console.print(Panel(
                f"Meta-reflection '{label}' added.",
                title="[bold white]META[/bold white]",
                border_style="white"
            ))
        except Exception as e:
            self.console.log(f"[bold red]Meta-reflection failed: {e}[/bold red]")

    async def perform_retro_relink(self, new_node_id, new_vec, k=12, min_age_steps=20):
        """Links a new node to semantically similar older nodes."""
        G = self.memory.graph_db.graph
        if not G.has_node(new_node_id):
            return

        candidates = []
        for nid, d in G.nodes(data=True):
            if nid != new_node_id and d.get("step", 0) <= (self.step_num - min_age_steps):
                vec = d.get("embedding")
                if vec is not None:
                    candidates.append((nid, np.asarray(vec, dtype=float)))

        if not candidates:
            return

        newv = np.asarray(new_vec, dtype=float)

        def _norm(x):
            return x / (np.linalg.norm(x) + 1e-9)

        newv_norm = _norm(newv)
        use_potential_sim = os.getenv("E8_USE_POTENTIAL_IN_RETROLINK", "1") == "1"
        sims_raw = []
        for nid, v in candidates:
            sim = float(np.dot(newv_norm, _norm(v)))
            step_val = (self.memory.graph_db.get_node(nid) or {}).get("step", 0)
            if use_potential_sim:
                pot_existing = (self.memory.graph_db.get_node(nid) or {}).get("connectivity_potential", self.memory.INITIAL_POTENTIAL)
                # Boost similarity smoothly: effective_sim = sim * (0.7 + 0.6*pot)
                sim *= (0.7 + 0.6 * pot_existing)
            sims_raw.append((nid, sim, step_val))
        sims = sorted(
            sims_raw,
            key=lambda x: (1 - x[1], -x[2]),  # distance asc (via 1-sim), then newer first
        )

        # Dynamically determine number of retrolinks based on the new node's potential
        node_data = self.memory.graph_db.get_node(new_node_id) or {}
        node_potential = node_data.get("connectivity_potential", self.memory.INITIAL_POTENTIAL)
        base = int(os.getenv("E8_RETROLINK_BASE", "2"))   # min links
        cap  = int(os.getenv("E8_RETROLINK_CAP",  "12"))  # max links
        num_retro_links = max(base, min(cap, int(round(base + node_potential * (cap - base)))))

        self.console.log(
            f"Node {new_node_id[:8]} potential is {node_potential:.2f}, creating {num_retro_links} retrolinks."
        )

        # Find enough neighbors to satisfy the dynamic cap
        new_node_data = self.memory.graph_db.get_node(new_node_id) or {}
        parent_ids_existing = new_node_data.get("parent_ids", [])
        similar_nodes = sims[:num_retro_links + len(parent_ids_existing)]

        links_created = 0
        use_edge_potential = os.getenv("E8_USE_POTENTIAL_EDGE_WEIGHT", "1") == "1"
        new_pot = node_data.get("connectivity_potential", self.memory.INITIAL_POTENTIAL)
        for nid, w, _ in similar_nodes:
            if links_created >= num_retro_links:
                break
            # Check if nid is not a parent (avoid self-links)
            parent_ids = parent_ids_existing
            if nid not in parent_ids and nid != new_node_id:
                try:
                    final_weight = w
                    if use_edge_potential:
                        pot_existing = (self.memory.graph_db.get_node(nid) or {}).get("connectivity_potential", self.memory.INITIAL_POTENTIAL)
                        # Combine potentials (average) and scale similarity weight
                        combo = 0.5 * (new_pot + pot_existing)
                        final_weight = w * (0.6 + 0.8 * combo)  # keeps weights in reasonable band
                    try:
                        self._assert_writer()
                    except Exception:
                        pass
                    self.memory.graph_db.add_edge(new_node_id, nid, kind="retrotag", weight=final_weight, raw_similarity=w)
                    links_created += 1
                except Exception:
                    pass

                node = G.nodes.get(nid)
                if node:
                    node["temperature"] = float(node.get("temperature", 0.5) + 0.05 * w)

        try:
            content = f"Created [bold]{links_created}[/] dynamic retro-links for node [cyan]{new_node_id[:8]}[/]"
            self.console.print(Panel(content, title="[bold magenta]🔗 GRAPH LINKING[/]", border_style="magenta"))
        except Exception:
            self.console.log(f"[Graph] Created {links_created} dynamic retro-links for node {new_node_id[:8]}")

    async def _update_synaptic_plasticity(self):
        """Periodically update the connectivity potential of all nodes."""
        try:
            self.console.print(Panel(
                "Reinforcing successful nodes and applying decay to memory connections.",
                title="[bold #5F9EA0]🧠 SYNAPTIC PLASTICITY[/]",
                border_style="#5F9EA0"
            ))
        except Exception:
            pass
        
        # --- 1. Get Global Modulators from AI state ---
        current_goal = getattr(self, 'current_goal_name', 'STABILITY')
        mood = self.mood
        global_modulator = 1.0
        if current_goal == "NOVELTY":
            global_modulator *= 1.15
        elif current_goal == "STABILITY":
            global_modulator *= 0.85
        
        mood_entropy = float(mood.mood_vector.get('entropy', 0.5)) if hasattr(mood, 'mood_vector') else 0.5
        global_modulator *= (1.0 + (mood_entropy - 0.5) * 0.2) # Higher entropy boosts potential
        
        # --- 2. Reinforce Successful Nodes ---
        # Get IDs of recent, highly-rated insights
        successful_nodes = [
            node_id for node_id, data in self.memory.graph_db.graph.nodes(data=True)
            if data.get("type") in ["insight_synthesis", "explorer_insight", "meta_reflection"]
            and data.get("step", 0) > (self.step_num - 500) # Recent
            and data.get("rating", 0) > 0.9 # Successful
        ]
        
        max_delta = float(os.getenv('E8_PLASTICITY_MAX_DELTA', '0.15'))
        for node_id in successful_nodes:
            node = self.memory.graph_db.get_node(node_id)
            if node:
                before = node.get('connectivity_potential', self.memory.INITIAL_POTENTIAL)
                update_node_potential(node, 0.9, global_modulator)
                after = node.get('connectivity_potential', before)
                if after - before > max_delta:
                    node['connectivity_potential'] = before + max_delta

        # --- 3. Apply Decay to a Sample of Other Nodes ---
        DECAY_RATE = 0.005
        # To avoid iterating all nodes every time, we sample
        all_nodes = list(self.memory.graph_db.graph.nodes(data=True))
        if len(all_nodes) > 100:
            nodes_to_decay = random.sample(all_nodes, 100)
        else:
            nodes_to_decay = all_nodes
        
        for node_id, node_data in nodes_to_decay:
            if node_id not in successful_nodes:
                current_potential = node_data.get('connectivity_potential', self.memory.INITIAL_POTENTIAL)
                decay_amount = (DECAY_RATE * (2.0 - global_modulator))
                if decay_amount > max_delta:
                    decay_amount = max_delta
                new_potential = max(0.0, current_potential - decay_amount)
                node_data['connectivity_potential'] = new_potential

    async def _run_proactive_insight_synthesis(self, forced_parents=None):
        async with self.insight_cycle_lock:
            if forced_parents:
                parent_a_data, parent_b_data = forced_parents
                parent_a_id = parent_a_data.get('idx') or parent_a_data.get('id')
                parent_b_id = parent_b_data.get('idx') or parent_b_data.get('id')
                if parent_a_id is None or parent_b_id is None:
                    return  # safety
                # ensure embeddings are available downstream as in original flow
                parent_a = parent_a_data
                parent_b = parent_b_data
            else:
                # ... original parent selection logic remains unchanged ...
                # set: parent_a_id, parent_b_id, parent_a, parent_b
                pass

            # continue with the existing synthesis code path (including Step 1 Quantum–E8 block)

            parent_a_label = parent_a_data.get('label', 'A')
            parent_b_label = parent_b_data.get('label', 'B')

            cleaned_a = re.sub(r'^(Synthesis:\s*)+', '', parent_a_label).strip()
            cleaned_b = re.sub(r'^(Synthesis:\s*)+', '', parent_b_label).strip()

            novelty_vec = await self.get_embedding(new_concept_text)
            coh = await self.novelty_scorer.calculate_coherence(new_concept_text)
            if coh < float(os.getenv("E8_COHERENCE_FLOOR", "0.45")):
                cands = [new_concept_text]
                for _ in range(int(os.getenv("E8_RETRIES", "2"))):
                    alt = await self.insight_agent.create_hybrid_concept(parent_a_data, parent_b_data)
                    if alt and not alt.startswith("[LLM"):
                        cands.append(alt)
                scored = []
                for t in cands:
                    c = await self.novelty_scorer.calculate_coherence(t)
                    scored.append((t, c))
                new_concept_text, coh = max(scored, key=lambda x: x[1])
                if coh < float(os.getenv("E8_COHERENCE_FLOOR", "0.45")):
                    self.console.log(f"[bold yellow]Synthesis rejected: Coherence too low ({coh:.2f}) after retries.[/bold yellow]")
                    return

            nov = self.novelty_scorer.calculate_novelty(novelty_vec)

            def _tokens(s: str) -> set: return set(re.findall(r"[A-Za-z0-9]+", (s or "").lower()))
            def _jaccard(a: set, b: set) -> float:
                if not a and not b: return 1.0
                return len(a & b) / max(1, len(a | b))
            def _perplexity_stub(s: str) -> float:
                w = max(1, len(s.split())); avg_len = sum(len(t) for t in s.split()) / w
                return 25.0 + 3.0 * (avg_len < 3) + 6.0 * (avg_len > 10)

            tau_ppl, beta = float(os.getenv("E8_PPL_THRESH", "35.0")), 0.01
            grammar_pen = max(0.0, _perplexity_stub(new_concept_text) - tau_ppl) * beta
            pa = (parent_a_data.get("label", "") + " " + parent_a_data.get("metaphor", "")); pb = (parent_b_data.get("label", "") + " " + parent_b_data.get("metaphor", ""))
            dup_pen = 0.1 if _jaccard(_tokens(new_concept_text), _tokens(pa + " " + pb)) > 0.8 else 0.0
            # Calculate rating using unified framework
            components = RatingComponents(
                novelty=float(nov),
                coherence=float(coh),
                utility=0.5,  # Default utility for synthesis
                confidence=0.8  # High confidence for formula-based calculation
            )
            
            # Calculate penalties
            tau_ppl, beta = float(os.getenv("E8_PPL_THRESH", "35.0")), 0.01
            grammar_pen = max(0.0, _perplexity_stub(new_concept_text) - tau_ppl) * beta
            pa = (parent_a_data.get("label", "") + " " + parent_a_data.get("metaphor", ""))
            pb = (parent_b_data.get("label", "") + " " + parent_b_data.get("metaphor", ""))
            dup_pen = 0.1 if _jaccard(_tokens(new_concept_text), _tokens(pa + " " + pb)) > 0.8 else 0.0
            total_penalty = grammar_pen + dup_pen
            
            # Use unified rating system if available
            if hasattr(self, 'unified_rating') and self.unified_rating is not None:
                base_rating = self.unified_rating.calculate_rating(components, RatingTypes.SYNTHESIS_QUALITY)
            else:
                # Fallback calculation (preserve original weights for compatibility)
                base_rating = 0.35 * components.novelty + 0.65 * components.coherence
            
            final_rating = max(0.0, min(1.0, base_rating - total_penalty))
            
            # Improved insight shaping with validation
            original_rating = final_rating
            shaping_applied = []
            
            try:
                _lc_txt = (new_concept_text or "").lower()
                
                # Boost kernel ruggedness concepts
                if 'kernel' in _lc_txt and any(t in _lc_txt for t in ('rugged','ruggedness','robust','stability')):
                    if RatingValidator.validate_rating_change(final_rating, final_rating + 0.05, "synthesis_boost"):
                        final_rating = min(0.98, final_rating + 0.05)
                        shaping_applied.append('boost_kernel_ruggedness')
                
                # Constrain overly broad claims
                if 'universal cure' in _lc_txt:
                    new_concept_text += " (operationalized: measurable dependent metric e.g. reduces kernel failure rate under adversarial perturbation by >=5%)"
                    if RatingValidator.validate_rating_change(final_rating, final_rating - 0.05, "coherence_degradation"):
                        final_rating = max(0.0, final_rating - 0.05)
                        shaping_applied.append('constrain_universal_cure')
                
                # Log shaping actions
                if shaping_applied:
                    metrics_log('insight_shaping', {
                        'actions': shaping_applied,
                        'original_rating': original_rating,
                        'final_rating': final_rating,
                        'rating_delta': final_rating - original_rating
                    })
                    
            except Exception as e:
                self.console.log(f"[SYNTHESIS] Insight shaping error: {e}")
            
            # Replay reward aligns with final rating to avoid divergence
            reward = float(final_rating)

            novelty_score, coherence_score = float(nov), float(coh)
            self.insight_agent.learn_from_reward(float(reward), episode_data={
                "type": "synthesis", "parents": [parent_a_id, parent_b_id], "text": new_concept_text,
                "reward": float(reward), "novelty": float(nov), "coherence": float(coh)
            })

            # --- QUANTUM–E8 COLLAPSE (M22-COMPATIBLE) ---
            E = float(max(0.0, min(1.0, nov))) * float(max(0.0, min(1.0, coh)))  # [0,1]
            k = float(os.getenv("E8_COLLAPSE_K", "8.0"))
            theta = float(os.getenv("E8_COLLAPSE_THETA", "0.35"))
            def _sigmoid(x):
                x = max(-40.0, min(40.0, x))
                return 1.0 / (1.0 + math.exp(-x))
            p_collapse = _sigmoid(k * (E - theta))

            import numpy as _np, zlib as _z
            seed_material = f"{GLOBAL_SEED}:{int(self.step_num)}:{parent_a_id}:{parent_b_id}".encode()
            seed = _z.adler32(seed_material) & 0xFFFFFFFF
            _rng = _np.random.default_rng(seed)
            collapsed = bool(_rng.random() < p_collapse)

            parent_vec_a = self.memory.main_vectors.get(parent_a_id)
            parent_vec_b = self.memory.main_vectors.get(parent_b_id)
            if parent_vec_a is not None and parent_vec_b is not None:
                vsa_bound = self.embedding_bootstrap.bind_vectors(parent_vec_a, parent_vec_b)
            else:
                vsa_bound = novelty_vec  # fallback

            # E8 root
            if hasattr(self.physics, "roots_unit") and self.physics.roots_unit and len(self.physics.roots_unit) > 0:
                e8_root = _np.asarray(self.physics.roots_unit[0], dtype=_np.float32)
                e8_root = e8_root / (np.linalg.norm(e8_root) + 1e-9)
            else:
                _rng2 = _np.random.default_rng(seed ^ 0x9E3779B9)
                e8_root = _rng2.standard_normal(len(vsa_bound)).astype(_np.float32)
                e8_root = e8_root / (np.linalg.norm(e8_root) + 1e-9)

            vb = _np.asarray(vsa_bound, dtype=_np.float32)
            alignment = float(_np.dot(vb, e8_root) / ((_np.linalg.norm(vb) * _np.linalg.norm(e8_root)) + 1e-9))

            reward = float(final_rating)
            alpha = float(os.getenv("E8_QP_ALIGN_GAIN", "0.5"))
            if collapsed:
                quantum_potential = reward * (0.5 + 0.5 * _sigmoid(alpha * alignment))
            else:
                quantum_potential = 0.5 * reward

            quantum_potential = float(max(0.0, min(1.0, quantum_potential)))
            alignment = float(max(-1.0, min(1.0, alignment)))

            # attach into new_entry (ensure you add these keys when constructing the dict)
            qp_telemetry = {
                "quantum_potential": quantum_potential,
                "e8_alignment": alignment,
                "collapsed": bool(collapsed)
            }
            # when you build new_entry, include: **qp_telemetry
            # --- END QUANTUM–E8 COLLAPSE ---

            new_entry = {
                "type": "insight_synthesis",
                "label": sanitize_line(f"Synthesis: {cleaned_a} + {cleaned_b}"),
                "metaphor": new_concept_text, "rating": final_rating, "step": self.step_num,
                **qp_telemetry
            }
            new_entry["coherence"] = float(coherence_score)

            parent_vec_a = self.memory.main_vectors.get(parent_a_id); parent_vec_b = self.memory.main_vectors.get(parent_b_id)
            processed_vec = self.memory.hopfield.clean_up(novelty_vec)
            if parent_vec_a is not None and parent_vec_b is not None:
                parentage_vec = self.memory.vsa.encode_parentage(parent_vec_a, parent_vec_b)
                processed_vec = normalize_vector(0.8 * processed_vec + 0.2 * parentage_vec)
            new_entry["embedding"] = processed_vec

            if not self.memory.reranker.validate(processed_vec, [parent_a_id, parent_b_id], nov, coh): return

            new_node_id = await self.memory.add_entry(new_entry, parent_ids=[parent_a_id, parent_b_id])

            try:
                for pid in [parent_a_id, parent_b_id]:
                    try:
                        try:
                            self._assert_writer()
                        except Exception:
                            pass
                        self.memory.graph_db.add_edge(pid, new_node_id, kind="parent", weight=1.0)
                    except Exception: pass
                for _nid in [new_node_id, parent_a_id, parent_b_id]:
                    try: self.memory._trim_degree(_nid, max_deg=8)
                    except Exception: pass
            except Exception: pass

            # --- MICRO-ACCRETION BOOST (SAFE) ---
            try:
                vec_new = new_entry.get("embedding")
                if vec_new is not None:
                    gain = float(os.getenv("E8_QP_ACCRETION_GAIN", "0.05"))
                    pot_boost = float(new_entry.get("quantum_potential", 0.0))

                    sim_pa = self.memory._cos_sim(vec_new, self.memory.main_vectors.get(parent_a_id))
                    sim_pb = self.memory._cos_sim(vec_new, self.memory.main_vectors.get(parent_b_id))
                    sim = max(0.0, (sim_pa + sim_pb) * 0.5)

                    node = self.memory.graph_db.get_node(new_node_id)
                    if node is not None:
                        base = float(node.get("connectivity_potential", 0.5))
                        delta = gain * (0.5 * pot_boost + 0.5 * sim)
                        node["connectivity_potential"] = float(np.clip(base + delta, 0.0, 1.0))
            except Exception:
                pass
            # --- END MICRO-ACCRETION BOOST ---

            await self._append_insight_log({
                "run_id": getattr(self, "run_id", None), "step": int(self.step_num), "type": "insight_synthesis",
                "node_id": new_node_id, "label": new_entry["label"], "content": new_concept_text,
                "rating": float(final_rating), "novelty": float(novelty_score), "coherence": float(coherence_score),
                "parent_ids": [parent_a_id, parent_b_id],
            })
            self.subconscious_event_log.append({'type': 'insight_synthesis', 'label': new_entry['label'], 'step': self.step_num})
            self.console.print(Panel(f"[bold]New Synthesis:[/bold] {new_concept_text}\n[yellow]Novelty:[/] {novelty_score:.2f} | [cyan]Coherence:[/] {coherence_score:.2f} | [green]Reward:[/] {reward:.2f}",
                                     title="[bold blue]INSIGHT SYNTHESIS[/]", border_style="blue"))

            # Use centralized config for validator threshold
            config = AppConfig.from_env()
            min_rating = config.validator_min_rating
            if final_rating >= min_rating: self.slots.insight.start(self._spawn_validator(new_node_id))

    def bh_panel_snapshot(self, mass: float = 1.0, dim: int = 32) -> dict:
        """Generate BH Console panel snapshot with Q(t) triplet and causal parameters.
        
        Reuses calibrated Q(t) functions to ensure UI matches validated test results.
        """
        try:
            # Use calibrated Q(t) pipeline from validated functions
            t_prime = get_cosmic_time_parameter(self.step_num)  # τ-shaped mapping
            q_t = compute_emergence_q_t(self.step_num, mass=mass, dim=dim)
            
            # Compute sQ global using same scaling as in emergence law
            sq = E8_Q_SCALE * scenario_scale_for_bh(mass, dim)
            
            # Phase classification based on validated thresholds
            if q_t >= 0.95:
                phase = "saturated"
            elif q_t >= 0.10:
                phase = "emerging"
            else:
                phase = "dormant"
            
            # Scenario scaling factor (non-uniform gains)
            scale = scenario_scale_for_bh(mass, dim)
            
            # Causal modulation parameters (if available from recent BH compression)
            target_compression = getattr(self, '_last_target_compression', None)
            beta = getattr(self, '_last_vae_beta', None)
            
            # M23: Add curvature field metrics
            curvature_intensity = 0.0
            curvature_residual = 0.0
            curvature_neutrality = 1.0
            proximity_lock_correlation = 0.0
            
            try:
                if hasattr(self, 'curvature_field') and self.curvature_field:
                    curvature_intensity = self.curvature_field.get_field_intensity()
                    curvature_residual = self.curvature_field.get_residual_quality()
                    curvature_neutrality = self.curvature_field.check_neutrality()
                proximity_lock_correlation = compute_proximity_lock_correlation(self)
            except Exception:
                pass
            
            return {
                "q_t": float(q_t),
                "sq": float(sq),
                "t_prime": float(t_prime),
                "phase": phase,
                "scale": float(scale),
                "target_compression": float(target_compression) if target_compression is not None else None,
                "beta": float(beta) if beta is not None else None,
                "step": int(self.step_num),
                "mass": float(mass),
                "dim": int(dim),
                # M23: Curvature field metrics
                "curvature_intensity": float(curvature_intensity),
                "curvature_residual": float(curvature_residual),
                "curvature_neutrality": float(curvature_neutrality),
                "proximity_lock_correlation": float(proximity_lock_correlation)
            }
        except Exception as e:
            # Safe fallback with basic values
            return {
                "q_t": 0.0,
                "sq": 0.25,
                "t_prime": 0.0,
                "phase": "error",
                "scale": 1.0,
                "target_compression": None,
                "beta": None,
                "step": int(getattr(self, 'step_num', 0)),
                "error": str(e)
            }

    def _build_telemetry_snapshot_core(self) -> dict:
        shells_data, shell_tensions = {}, {}
        for dim, shell in self.dimensional_shells.items():
            # Generate dynamic yaw angle based on shell activity
            vector_count = len(shell.vectors)
            activity_hash = hash(f"{dim}_{vector_count}_{self.step_num}") % 1000
            yaw_angle = (activity_hash / 1000.0) * 2 * np.pi  # 0 to 2Ï€
            shells_data[dim] = {"orientation": {"yaw": yaw_angle}}

            matrix, _ = shell.get_all_vectors_as_matrix()
            if matrix is not None and matrix.shape[0] > 1:
                dists = np.linalg.norm(matrix - matrix.mean(axis=0, keepdims=True), axis=1)
                shell_tensions[dim] = float(dists.mean())
            else:
                shell_tensions[dim] = 0.0

            # Add angular velocity based on shell activity
            omega = (vector_count * 0.1 + shell_tensions[dim] * 2.0) * (0.5 + 0.5 * np.sin(self.step_num * 0.1))
            shells_data[dim]["omega"] = float(omega)

        rh = getattr(self.insight_agent, "reward_history", None)
        insight_reward_avg = float(np.mean(list(rh))) if rh and len(rh) > 0 else 0.0
        step = int(self.step_num)

        def _steps_to_next(current_step, every, offset):
            if every <= 0: return 0
            if current_step < offset: return offset - current_step
            mod = (current_step - offset) % every
            return 0 if mod == 0 else every - mod

        ingestion_feed_data = []
        validation_lab_data = []
        recent_nodes = list(self.memory.graph_db.graph.nodes(data=True))[-50:]

        for node_id, data in recent_nodes:
            if data.get("type") == "external_concept":
                ingestion_feed_data.append(f"[{data.get('source', 'ext')}] {data.get('label', '...')}")

            if "validation_status" in data:
                status = data["validation_status"]
                v_type = status.get('type', 'unknown').replace('_', ' ').title()
                validation_lab_data.append(f"'{data.get('label', '...')}' â†’ {v_type}")

        insight_holocron_data = [f"BH Event: Consolidated {d.get('size', 0)} concepts (Mass: {d.get('mass', 0):.2f})" for d in self.black_hole_log]
        for _, data in recent_nodes:
            if data.get("type") == "insight_synthesis":
                insight_holocron_data.append(f"Synthesis: {data.get('label', '...')}")

        if not hasattr(self, '_last_node_count'):
            self._last_node_count = 0

        new_memory_nodes_data = []
        if hasattr(self, 'new_node_id_queue'):
            while self.new_node_id_queue:
                node_id = self.new_node_id_queue.popleft()
                data = self.memory.graph_db.get_node(node_id)
                if data and 'blueprint_location_id' in data and data['blueprint_location_id'] is not None:
                    node_info = data.copy()
                    node_info['id'] = node_id
                    if 'embedding' in node_info:
                        try:
                            import numpy as _np
                            if isinstance(node_info['embedding'], _np.ndarray):
                                node_info['embedding'] = node_info['embedding'].tolist()
                        except Exception:
                            pass
                    new_memory_nodes_data.append(node_info)
        
        all_nodes_with_data = list(self.memory.graph_db.graph.nodes(data=True))
        new_node_count = len(all_nodes_with_data)
        self._last_node_count = new_node_count

        shell_population = {}
        for _dim, _shell in getattr(self, "dimensional_shells", {}).items():
            try:
                if hasattr(_shell, "get_all_vectors_as_matrix"):
                    _M, _ = _shell.get_all_vectors_as_matrix()
                    _count = int(np.asarray(getattr(_M, "shape", ).ravel()[0])[0]) if _M is not None else 0
                elif hasattr(_shell, "vectors"):
                    _count = len(_shell.vectors) if _shell.vectors is not None else 0
                else:
                    _count = 0
            except Exception:
                _count = 0
            shell_population[str(_dim)] = _count

        if not hasattr(self, "kdtree_failures"):
            self.kdtree_failures = 0

        psi_entropy = None
        qeng = getattr(self, "qeng", None)
        if qeng is not None:
            try:
                probs = None
                if hasattr(qeng, "_probs"):
                    probs = qeng._probs()
                elif hasattr(qeng, "probs"):
                    probs = qeng.probs()
                if probs is not None:
                    try:
                        arr = probs[0]
                    except Exception:
                        arr = probs
                    try:
                        import numpy as _np
                        p = _np.asarray(arr, dtype=float).ravel()
                        p = _np.clip(p, 1e-12, 1.0)
                        psi_entropy = float(-(_np.where(p>0, p*_np.log(p), 0.0)).sum())
                    except Exception:
                        import math as _math
                        _flat = []
                        try:
                            for _x in arr: _flat.append(float(_x))
                        except TypeError:
                            _flat = [float(arr)]
                        s = 0.0
                        for _p in _flat:
                            _p = min(max(_p, 1e-12), 1.0)
                            s += _p * _math.log(_p)
                        psi_entropy = float(-s)
            except Exception:
                psi_entropy = None

        novelty = getattr(self, "novelty", 0.0)
        compression_gain = getattr(self, "compression_gain", 0.0)
        disagreement = getattr(self, "disagreement", 0.0)
        lam = getattr(self, "lam", 0.0)

        # Build quantum telemetry
        quantum_telemetry = {}
        if qeng is not None:
            quantum_telemetry["psi_entropy"] = 0.0 if psi_entropy is None else float(psi_entropy)
            quantum_telemetry["lam"] = float(lam) if isinstance(lam, (int, float)) else 0.0
            quantum_telemetry["dt"] = float(qeng.config.dt)
            quantum_telemetry["gamma"] = float(qeng.config.gamma)
            quantum_telemetry["dephase"] = float(qeng.config.dephase)
        else:
            quantum_telemetry["psi_entropy"] = 0.0
            quantum_telemetry["lam"] = float(lam) if isinstance(lam, (int, float)) else 0.0
            quantum_telemetry["dt"] = 0.0
            quantum_telemetry["gamma"] = 0.0
            quantum_telemetry["dephase"] = 0.0

        telemetry = {
            "shell_population": shell_population,
            "novelty": float(novelty) if isinstance(novelty, (int, float)) else 0.0,
            "compression_gain": float(compression_gain) if isinstance(compression_gain, (int, float)) else 0.0,
            "disagreement": float(disagreement) if isinstance(disagreement, (int, float)) else 0.0,
            "kdtree_failures": int(getattr(self, "kdtree_failures", 0)),
            "run_id": self.run_id,
            "step": step,
            "mood": self.mood.mood_vector,
            "black_hole_pressure": self.black_hole_pressure,
            "goals": {n: d.get("activation", 0.0) for n, d in self.goal_field.goals.items()} if self.goal_field.is_initialized else {},
            "shells": shells_data,
            "shell_tensions": shell_tensions,
            "global_tension": float(sum(shell_tensions.values())/len(shell_tensions)) if shell_tensions else 0.0,
            "memory_count": self.memory.graph_db.graph.number_of_nodes(),
            "teacher_in": _steps_to_next(step, TEACHER_ASK_EVERY, TEACHER_OFFSET),
            "explorer_in": _steps_to_next(step, TEACHER_ASK_EVERY, EXPLORER_OFFSET),
            "environment_theme": self.synthetic_env.current_theme_region,
            "symbolic_weather": self.mood.get_symbolic_weather(),
            "teacher_question": self.teacher_question,
            "explorer_answer": self.explorer_last_answer,
            "subconscious_narrative": self.subconscious.narrative,
            "insight_agent_avg_reward": insight_reward_avg,
            "autoencoder_trained": bool(self.autoencoder and self.autoencoder.is_trained),
            "ingestion_feed": ingestion_feed_data,
            "validation_lab": validation_lab_data,
            "insight_holocron": insight_holocron_data,
            "new_memory_nodes": new_memory_nodes_data,
            "quantum_telemetry": quantum_telemetry,
        }
        # Attach proximity telemetry for M230 frontend if available
        try:
            last_evt = getattr(self, '_last_proximity_event', None)
            if last_evt is not None:
                telemetry["last_proximity"] = last_evt
                # Some frontends also check simplified key
                telemetry["proximity_event"] = last_evt
            evts = list(getattr(self, '_proximity_events', []))
            if evts:
                telemetry["proximity_events"] = evts[-10:]
        except Exception:
            pass
        
        # Add horizon energy gauges if horizons are enabled
        try:
            if hasattr(self, 'horizon_manager') and self.horizon_manager:
                horizon_snapshot = self.horizon_manager.get_horizon_snapshot()
                if horizon_snapshot:
                    # Extract energy totals for gauges
                    energy_totals = horizon_snapshot.get('energy_totals', {})
                    for energy_type, energy_value in energy_totals.items():
                        telemetry[f"horizon_{energy_type}"] = float(energy_value)
                    
                    # Add horizon layer counts
                    layers = horizon_snapshot.get('layers', {})
                    telemetry["horizon_layer_count"] = len(layers)
                    
                    # Add total boundary sites across all horizons
                    total_sites = sum(layer_info.get('num_sites', 0) for layer_info in layers.values())
                    telemetry["horizon_total_sites"] = total_sites
                    
        except Exception as horizon_telemetry_err:
            try:
                self.console.log(f"[Telemetry] Failed to collect horizon gauges: {horizon_telemetry_err}")
            except Exception:
                pass
        
        if isinstance(getattr(self, 'm20_metrics', None), dict):
            telemetry["m20_metrics"] = dict(self.m20_metrics)

        # Provide a compact mantle summary for fluid panel fallbacks
        try:
            _m20 = telemetry.get('m20_metrics', {}) if isinstance(telemetry.get('m20_metrics'), dict) else {}
            telemetry['mantle'] = {
                'flux': _safe_number(_m20.get('poynting_flux', 0.0), 0.0, 0.0, float(os.getenv('E8_FLUX_GLOBAL_CAP', '100.0'))),
                # 'curvature': _safe_number(_m20.get('spacetime_curvature', 0.0), 0.0, 0.0, float(os.getenv('E8_CURV_GLOBAL_CAP', '6.0'))),  # COMMENTED OUT - removing 6.0 curvature limiter
                'curvature': _safe_number(_m20.get('spacetime_curvature', 0.0), 0.0, 0.0, None),  # Removed cap limiter
                'flow': _safe_number(_m20.get('consciousness_flow_rate', _m20.get('poynting_flux', 0.0)), 0.0, 0.0, None),
                'tension': _safe_number(_m20.get('field_pressure_proxy', 0.0), 0.0, 0.0, None)
            }
        except Exception:
            pass

        try:
            telemetry['sdi'] = {
                'branch_head': sdi.get_branch_head(self.run_id),
                'capsules_count': sdi.count_capsules(self.run_id),
                'commits_count': sdi.count_commits(self.run_id),
            }
        except Exception:
            telemetry['sdi'] = {
                'branch_head': None,
                'capsules_count': 0,
                'commits_count': 0,
            }

        if self.market:
            telemetry["market"] = {"symbols": self.market_symbols, "last": self.market_last}
        # Simple numeric proximity distance if available
        try:
            if hasattr(self, 'last_proximity_distance'):
                lp = getattr(self, 'last_proximity_distance', None)
                if isinstance(lp, (int, float)):
                    telemetry["proximity"] = _safe_number(lp, 0.0, 0.0, None)
        except Exception:
            pass

        if hasattr(self, "field_mantle") and self.field_mantle:
            telemetry.setdefault("field_mantle", {}).update({
                "inv_driver_counts": self.field_mantle._inv_uses,
                "metric_backtracks": self.field_mantle._metric_backtracks,
                "E_norm": self.field_mantle.Enorm,
                "B_norm": self.field_mantle.Bnorm
            })

        # Add VAE telemetry if enabled
        if E8_VAE_TELEM:
            ae = getattr(self, "autoencoder", None)
            telemetry["vae"] = {
                "enabled": E8_VAE_ENABLE,
                "trained": bool(ae and getattr(ae, "is_trained", False)),
                "layers": getattr(ae, "layer_sizes", []),
                "latent": E8_VAE_LATENT,
                "beta": E8_VAE_BETA,
                "buffer": len(getattr(self, "_vae_buf", [])),
                "buffer_cap": self._vae_buf_cap,
                "min_buffer": self._vae_min,
                "train_every": self._vae_train_every,
                "last_losses": getattr(self, "_last_vae_losses", None)
            }

        return telemetry

    def _build_telemetry_snapshot(self):
        """Build telemetry snapshot and include 'insights' for tetra visualization."""
        snap = self._build_telemetry_snapshot_core()
        try:
            # Only compute if not already present or malformed
            if not isinstance(snap.get("insights"), list):
                import numpy as _np

                def _to8(v):
                    try:
                        arr = _np.asarray(v, dtype=_np.float32).ravel()
                    except Exception:
                        try:
                            arr = v.ravel().astype("float32")
                        except Exception:
                            try:
                                arr = _np.asarray(list(v), dtype=_np.float32).ravel()
                            except Exception:
                                return []
                    # Prefer SDM projection if available
                    try:
                        sdm = getattr(getattr(self, "memory", None), "sdm", None)
                        if sdm is not None and hasattr(sdm, "_get_vec8d"):
                            w = sdm._get_vec8d(arr)
                            if w is not None:
                                return list(getattr(w, "tolist", lambda: w)())
                    except Exception:
                        pass
                    return list(arr[:8])

                mm = getattr(self, "memory", None)
                gdb = getattr(mm, "graph_db", None)
                graph = getattr(gdb, "graph", None)
                out = []
                if graph is not None and mm is not None:
                    # scan recent nodes, newest-first
                    try:
                        nodes_data = list(graph.nodes(data=True))
                    except Exception:
                        nodes_data = []
                    entries = []
                    for nid, data in nodes_data[-256:][::-1]:
                        t = (data or {}).get("type", "")
                        if t in ("insight_synthesis", "explorer_insight", "meta_reflection", "external_concept"):
                            entries.append((nid, data))
                        if len(entries) >= 40:
                            break

                    shell8 = None
                    try:
                        shell8 = getattr(self, "dimensional_shells", {}).get(8, None)
                    except Exception:
                        shell8 = None

                    for nid, data in entries:
                        vec_main = None
                        try:
                            vec_main = getattr(mm, "main_vectors", {}).get(nid)
                        except Exception:
                            vec_main = None

                        v8 = None
                        if shell8 is not None and hasattr(shell8, "get_vector"):
                            try:
                                v8 = shell8.get_vector(nid)
                            except Exception:
                                v8 = None
                        if v8 is None and vec_main is not None:
                            v8 = _to8(vec_main)
                        if v8 is None:
                            continue

                        neighbors = []
                        sims = []
                        try:
                            if vec_main is not None:
                                sims = mm.find_similar_in_main_storage(vec_main, k=3) or []
                        except Exception:
                            sims = []
                        for nb_id, _dist in sims:
                            nb_vec = None
                            if shell8 is not None and hasattr(shell8, "get_vector"):
                                try:
                                    nb_vec = shell8.get_vector(nb_id)
                                except Exception:
                                    nb_vec = None
                            if nb_vec is None:
                                try:
                                    nb_vec = getattr(mm, "main_vectors", {}).get(nb_id)
                                except Exception:
                                    nb_vec = None
                            if nb_vec is None:
                                continue
                            nb8 = _to8(nb_vec)
                            if nb8:
                                neighbors.append(nb8)

                        # reward heuristic
                        r = 0.0
                        try:
                            rating = data.get("rating")
                            r = float(rating) if isinstance(rating, (int, float)) else float(getattr(self, "last_insight_reward", 0.0))
                        except Exception:
                            pass

                        v8_list = list(v8) if hasattr(v8, "__iter__") and not isinstance(v8, (str, bytes)) else []
                        out.append({
                            "vector": v8_list[:8],
                            "vec8D": v8_list[:8],
                            "neighbors": neighbors[:3],
                            "reward": float(r)
                        })
                        if len(out) >= 8:
                            break

                snap["insights"] = out
        except Exception:
            # Never break telemetry on errors
            pass
        
        # Add BH panel data for frontend display
        try:
            # Generate BH panel snapshot and cache it
            mass = float(os.getenv("E8_BH_PANEL_MASS", "1.0"))
            dim = int(os.getenv("E8_BH_PANEL_DIM", "32"))
            bh_panel = self.bh_panel_snapshot(mass=mass, dim=dim)
            
            # Store causal parameters for next cycle if holographic compression ran
            if hasattr(self, '_last_target_compression'):
                bh_panel['target_compression'] = self._last_target_compression
            if hasattr(self, '_last_vae_beta'):
                bh_panel['beta'] = self._last_vae_beta
            
            # Add to telemetry as both events and direct field for compatibility
            if 'events' not in snap:
                snap['events'] = {}
            snap['events']['bh.panel'] = bh_panel
            snap['bh_panel'] = bh_panel  # Direct field fallback
            
            # Log metrics for cycle tracking
            metrics_log("bh.panel", bh_panel)
            
            # Cache for REST API
            self._last_bh_panel = bh_panel
            
        except Exception as e:
            # Don't break telemetry on BH panel errors
            try:
                self.console.log(f"[BH Panel] Error in telemetry: {e}")
            except:
                pass
            pass
        return snap

    async def _sse_push_telemetry(self):
        # Deliver telemetry to any connected clients over SSE and/or WebSocket.
        # Previously this returned early when there were no SSE clients, which
        # unintentionally starved WebSocket-only frontends. Now we proceed when
        # either transport has listeners.
        sse_clients = getattr(self, "sse_clients", None)
        try:
            ws_clients = getattr(self, "ws_clients", None)
        except Exception:
            ws_clients = None
        if not sse_clients and not ws_clients:
            return
        try:
            # Throttle based on minimum interval
            now = time.time()
            if (now - self._last_sse_sent_ts) * 1000.0 < self._sse_min_interval_ms:
                return
            payload = json.dumps(self._build_telemetry_snapshot(), cls=NumpyEncoder, ensure_ascii=False)
            # Push to SSE subscribers (if any)
            if sse_clients:
                dead = set()
                for q in list(sse_clients):
                    try:
                        q.put_nowait(payload)
                    except asyncio.QueueFull:
                        dead.add(q)
                for q in dead:
                    sse_clients.discard(q)
            # Also broadcast to any connected WebSocket clients (if present)
            try:
                await self._ws_broadcast_text(payload)
            except Exception:
                pass
            self._last_sse_sent_ts = now
        except Exception:
            pass

    async def _ws_broadcast_text(self, text: str) -> None:
        """Broadcast a plain JSON string to connected WebSocket clients.

        This mirrors SSE broadcasts for frontends preferring websockets.
        """
        try:
            ws_clients = getattr(self, 'ws_clients', None)
        except Exception:
            ws_clients = None
        if not ws_clients:
            return
        
        # Create a snapshot of clients to avoid set modification during iteration
        clients_snapshot = list(ws_clients)
        if not clients_snapshot:
            return
            
        dead_ws = set()
        broadcast_count = 0
        
        for ws in clients_snapshot:
            try:
                if ws.closed:
                    dead_ws.add(ws)
                    continue
                    
                await asyncio.wait_for(ws.send_str(text), timeout=1.0)  # 1 second timeout
                broadcast_count += 1
                
            except asyncio.TimeoutError:
                console.log("[WS] Broadcast timeout for a WebSocket client, marking as dead")
                dead_ws.add(ws)
            except Exception as e:
                # Connection is dead or has issues
                dead_ws.add(ws)
        
        # Clean up dead connections
        if dead_ws:
            for ws in dead_ws:
                try:
                    ws_clients.discard(ws)
                except Exception:
                    pass
            console.log(f"[WS] Cleaned up {len(dead_ws)} dead WebSocket connections, broadcasted to {broadcast_count} clients")

    def _ensure_console_export_state(self):
        if not hasattr(self, "_console_export_inited"):
            base = get_path("logs/console", self.run_id)
            os.makedirs(base, exist_ok=True)
            self.console_export_dir = base
            self._console_last_export_len = 0
            self._console_chunk_index = 0
            self._console_export_inited = True

    def _export_console_chunk(self, end_step: int, final: bool = False) -> None:
        self._ensure_console_export_state()
        try:
            text_all = self.console.export_text()

            if len(text_all) < self._console_last_export_len:
                self._console_last_export_len = 0
                self._console_chunk_index += 1

            new_text = text_all[self._console_last_export_len:]

            if not new_text and not final:
                return

            start_step = self._console_chunk_index * CONSOLE_EXPORT_EVERY_STEPS
            end_inclusive = end_step
            base = f"console_{start_step:06d}-{end_inclusive:06d}"

            if CONSOLE_EXPORT_FORMAT in ("text", "both"):
                with open(os.path.join(self.console_export_dir, base + ".txt"), "w", encoding="utf-8") as f:
                    f.write(new_text)

            if CONSOLE_EXPORT_FORMAT in ("json", "both"):
                payload = {
                    "run_id": self.run_id,
                    "chunk_index": self._console_chunk_index,
                    "start_step": start_step,
                    "end_step": end_inclusive,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "content": new_text
                }
                with open(os.path.join(self.console_export_dir, base + ".json"), "w", encoding="utf-8") as f:
                    json.dump(payload, f, ensure_ascii=False)

            self._console_last_export_len = len(text_all)
            self._console_chunk_index += 1
        except Exception as e:
            self.console.log(f"[ConsoleExport] Failed: {e}")

    async def _project_self_into_memory(self):
        """
        One-time at start: read this script and inject sections into memory as concepts.
        """
        try:
            src_path = os.path.abspath(globals().get('__file__', 'e8_mind_server_M20.py'))
            with open(src_path, "r", encoding="utf-8") as f:
                code_txt = f.read()
        except Exception as e:
            self.console.log(f"[SelfProject] failed to read source: {e}")
            return

        try:
            splitter = re.compile(r"(?m)^(class\s+\w+\s*:|def\s+\w+\s*\(|if\s+__name__\s*==\s*['\"]__main__['\"]\s*:)");
            idxs = [m.start() for m in splitter.finditer(code_txt)]
            idxs = [0] + idxs + [len(code_txt)]
            sections = []
            for a, b in zip(idxs[:-1], idxs[1:]):
                chunk = code_txt[a:b].strip()
                if not chunk:
                    continue
                first = chunk.splitlines()[0].strip()
                label = sanitize_line(first[:72]) if 'sanitize_line' in globals() else first[:72]
                excerpt = "\n".join(chunk.splitlines()[:40])
                sections.append((label, excerpt))
            if not sections:
                head = "\n".join(code_txt.splitlines()[:80])
                sections = [("source: e8_mind_server", head)]
        except Exception as e:
            self.console.log(f"[SelfProject] split failed: {e}")
            return

        try:
            root_id = await self.memory.add_entry({
                "type": "self_code",
                "label": "E8 Mind � current source",
                "metaphor": "The mind reading its own blueprint.",
                "rating": 0.9,
                "step": int(getattr(self, "step_num", 0))
            })
        except Exception as e:
            self.console.log(f"[SelfProject] root insert failed: {e}")
            return

        inserted = []
        for label, excerpt in sections[:40]:
            try:
                emb = await self.get_embedding(excerpt)
            except Exception:
                emb = None
            try:
                node_id = await self.memory.add_entry({
                    "type": "self_code_section",
                    "label": label,
                    "metaphor": excerpt,
                    "embedding": emb,
                    "rating": 0.7,
                    "temperature": 0.2,
                    "step": int(getattr(self, "step_num", 0))
                }, parent_ids=[root_id])
                inserted.append(node_id)
            except Exception as e:
                self.console.log(f"[SelfProject] section insert failed: {e}")

        try:
            if 'bump_temps' in globals():
                bump_temps(self.memory, inserted, amount=0.6)
        except Exception as e:
            self.console.log(f"[SelfProject] temp bump failed: {e}")
        self.console.log(f"[SelfProject] projected {len(inserted)} code sections into memory.")

    def _build_state_vector(self) -> np.ndarray:
        """Constructs the current state vector from all relevant cognitive modules."""
        mood_vec = np.array(list(self.mood.mood_vector.values()), dtype=np.float32)

        if self.goal_field.is_initialized and self.goal_field.goals:
            goal_activations = np.array([g["activation"] for g in self.goal_field.goals.values()], dtype=np.float32)
        else:
            goal_activations = np.zeros(4, dtype=np.float32)

        shell_att_vec = self.shell_attention.build(self)

        dynamics_vec = np.array([
            self._bh_ma50,
            (self.black_hole_pressure - self._prev_bh),
            float(np.linalg.norm(self._prev_action)),
            0.0,
            0.0
        ], dtype=np.float32)

        return np.concatenate([
            mood_vec,
            goal_activations,
            shell_att_vec,
            dynamics_vec
        ])

    def _update_cognitive_modules(self, step: int):
        """Updates all core cognitive modules that evolve over time."""
        # Nudge mood with a lightweight periodic tick so vectors visibly evolve
        try:
            self.mood.process_event("weather_tick", step=step, bh=float(getattr(self, 'black_hole_pressure', 0.0)))
        except Exception:
            pass
        self.mood.update()
        self.subconscious.decay(step)
        self.goal_field.decay()
        self.goal_field.update_from_mood(self.mood.mood_vector)
        self.memory.diffuse_field()
        self._update_black_hole_pressure()
        self.memory.decay_locks()
        self.scheduler.tick(step)

    def _train_autoencoder_if_ready(self, autoencoder_train_buffer: list, batch_size: int) -> list:
        """Trains the VAE on a batch of new embeddings if the buffer is full."""
        if TORCH_AVAILABLE and self.autoencoder and self.memory.pending_embeddings:
            autoencoder_train_buffer.extend(self.memory.pending_embeddings)
            self.memory.pending_embeddings.clear()

            if len(autoencoder_train_buffer) >= batch_size:
                batch_np = np.array(autoencoder_train_buffer[:batch_size])
                autoencoder_train_buffer = autoencoder_train_buffer[batch_size:]

                try:
                    losses = self.autoencoder.train_on_batch(torch.from_numpy(batch_np).float())
                    content = (
                        f"Loss: [bold yellow]{losses['total_loss']:.4f}[/] | "
                        f"Recon: {losses['recon_loss']:.4f} | KLD: {losses['kld_loss']:.4f}"
                    )
                    self.console.print(Panel(content, title="[bold #5F9EA0]🧠 VAE TRAINED[/]", border_style="#5F9EA0"))
                except Exception as e:
                    self.console.log(f"[bold red]VAE Training Error: {e}[/bold red]")
        return autoencoder_train_buffer

    def _update_emergent_metrics(self):
        """Derive/update emergent high-level metrics used by the frontend.

        compression_gain:
            A smoothed (EMA) positive value representing relative reduction in a
            crude 'global tension' proxy (mean radial spread across dimensional shells).
        disagreement:
            Standard deviation of active goal activations (smoothed) â€“ reflects
            how misaligned current goals are. If goals uninitialized, stays 0.
        """
        try:
            # --- compute current global tension similar to telemetry builder ---
            shell_tensions = []
            for _dim, _shell in getattr(self, 'dimensional_shells', {}).items():
                try:
                    if hasattr(_shell, 'get_all_vectors_as_matrix'):
                        _M, _ = _shell.get_all_vectors_as_matrix()
                        if _M is not None and _M.shape[0] > 1:
                            import numpy as _np
                            _d = _np.linalg.norm(_M - _M.mean(axis=0, keepdims=True), axis=1)
                            shell_tensions.append(float(_d.mean()))
                        else:
                            shell_tensions.append(0.0)
                    else:
                        shell_tensions.append(0.0)
                except Exception:
                    shell_tensions.append(0.0)
            global_tension = float(sum(shell_tensions) / len(shell_tensions)) if shell_tensions else 0.0

            # --- compression gain ---
            prev = getattr(self, '_prev_global_tension', None)
            if prev is None:
                raw_gain = 0.0
            else:
                if prev <= 1e-8:
                    raw_gain = 0.0
                else:
                    raw_gain = max(0.0, (prev - global_tension) / prev)
            self._prev_global_tension = global_tension
            # Exponential moving average to smooth noise
            cg_prev = getattr(self, 'compression_gain', 0.0)
            self.compression_gain = 0.9 * cg_prev + 0.1 * raw_gain

            # --- disagreement ---
            new_disagreement = 0.0
            try:
                if getattr(self.goal_field, 'is_initialized', False) and self.goal_field.goals:
                    import numpy as _np
                    acts = []
                    for _g, _d in self.goal_field.goals.items():
                        try:
                            acts.append(float(_d.get('activation', 0.0)))
                        except Exception:
                            pass
                    if len(acts) > 1:
                        new_disagreement = float(_np.std(_np.asarray(acts, dtype=float)))
            except Exception:
                new_disagreement = 0.0
            dis_prev = getattr(self, 'disagreement', 0.0)
            self.disagreement = 0.9 * dis_prev + 0.1 * new_disagreement
        except Exception:
            # Fail silently â€“ metrics are optional and should never break loop
            pass


    # Duplicate run_cognitive_cycle definition removed; the primary method is defined above.

    def _norm_text(self, s: str) -> str:
        s = (s or "").lower().strip()
        s = re.sub(r'(synthesis:\s*)+', 'synthesis: ', s)
        s = re.sub(r'\s+', ' ', s)
        return s

    def _ngrams(self, s: str, n: int = 5):
        toks = re.findall(r'[a-z0-9]+', s)
        return set(tuple(toks[i:i+n]) for i in range(max(0, len(toks)-n+1)))

    def _repeat_score(self, t: str) -> float:
        if not self._recent_norms:
            return 0.0
        tn = self._norm_text(t)
        A = self._ngrams(tn, 5)
        jacc, ratio, exact = 0.0, 0.0, 0.0
        for r in self._recent_norms:
            if not r: continue
            if tn == r:
                exact = 1.0; break
            B = self._ngrams(r, 5)
            if B: jacc = max(jacc, len(A & B) / max(1, len(A | B)))
            try:
                import difflib as _df
                ratio = max(ratio, _df.SequenceMatcher(None, tn, r).ratio())
            except ImportError: pass
        return max(jacc, ratio, exact)

    def _remember_output(self, text: str):
        n = self._norm_text(text)
        self._recent_texts.append(text)
        self._recent_norms.append(n)

    async def _async_call_llm_internal(self, prompt: str, **kwargs) -> str:
        """
        Calls the LLM with full context, persona, and domain hints.
        Includes a fallback to a local model and a filter to reject generic responses.
        """
        # --- 1. Construct the Full Prompt ---
        try:
            persona = self.semantics.persona_prefix(self.mood.mood_vector)
        except Exception:
            persona = self.mood.get_llm_persona_prefix()

        domain_hint = f"Domain: {getattr(self.domain_tint, 'last_hint', self.semantic_domain)}."

        _prompt_key = kwargs.pop('_prompt_key', 'ask')
        _prompt_vars = kwargs.pop('_prompt_vars', None) or {'question': prompt}

        full_prompt = self.prompts.render(
            _prompt_key,
            persona=persona,
            domain_hint=domain_hint,
            **_prompt_vars
        )

        # --- 2. Prepare and Execute LLM Calls ---
        primary_task = None
        local_task = None
        llm_kwargs = {
            'model': self.client_model,
            'max_tokens': int(kwargs.get('max_tokens', 256)),
            'temperature': float(kwargs.get('temperature', 0.7)),
        }

        try:
            messages = [{"role": "user", "content": full_prompt}]
            primary_task = asyncio.wait_for(
                self.llm_client.chat(messages=messages, **llm_kwargs),
                timeout=LLM_CALL_TIMEOUT_SEC
            )
        except Exception as e:
            self.console.log(f"[LLM] Primary client call setup failed: {e}")

        if self._anti_repeat_enabled and self.local_llm_client:
            try:
                local_kwargs = {
                    'max_tokens': llm_kwargs['max_tokens'] // 2,
                    'temperature': min(1.0, llm_kwargs['temperature'] + 0.15)
                }
                local_messages = [{"role": "user", "content": prompt}]
                local_task = asyncio.wait_for(
                    self.local_llm_client.chat(messages=local_messages, **local_kwargs),
                    timeout=LLM_CALL_TIMEOUT_SEC
                )
            except Exception as e:
                self.console.log(f"[LLM] Local client call setup failed: {e}")

        # --- 3. Await and Collect Responses ---
        tasks_to_gather = [task for task in (primary_task, local_task) if task]
        if not tasks_to_gather:
            return "[LLM ERROR] No tasks could be created."

        results = await asyncio.gather(*tasks_to_gather, return_exceptions=True)
        primary_text = results[0] if primary_task and not isinstance(results[0], BaseException) else f"[LLM ERROR] Primary: {results[0]}"
        local_text = None
        if local_task:
            result_index = 1 if primary_task else 0
            if len(results) > result_index and not isinstance(results[result_index], BaseException):
                local_text = results[result_index]

        # --- 4. Select the Best Candidate, Filtering Generic Responses ---
        candidates = []
        if isinstance(primary_text, str) and not primary_text.startswith("[LLM"):
            candidates.append(primary_text.strip())
        if isinstance(local_text, str) and not local_text.startswith("[LLM"):
            candidates.append(local_text.strip())

        if not candidates:
            return primary_text or "[LLM ERROR] No valid response from any provider."

        # BUG FIX: Filter out generic, unhelpful responses that cause loops.
        IGNORE_PHRASES = {
            "how can i assist", "how can i help", "how may i assist",
            "i am ready to assist", "hello!", "certainly!", "absolutely!",
            "here's an example", "here's an updated version",
            # Greeting spam prevention
            "hello there", "hi there", "good morning", "good afternoon",
            "good evening", "nice to meet you", "pleased to meet you",
            "how are you", "how do you do", "it's nice to see you",
            "welcome back", "glad to see you", "i'm here to help",
            "i'm happy to help", "i'm glad to help", "let me know if you need",
            "feel free to ask", "don't hesitate to ask", "i'm listening",
            "i'm all ears", "what can i do for you today",
            # Additional patterns for phi3 and smaller models
            "as an ai", "as an artificial intelligence", "i'm an ai",
            "i can help", "i'd be happy to", "i'm here for you",
            "let me help you", "i can assist", "i'm capable of",
            "i have the ability", "i can provide", "i can offer",
            "i can give you", "i can show you", "i can tell you",
            "i can explain", "i can describe", "i can discuss",
            "that's an interesting", "that's a good question",
            "that's a great question", "that's fascinating",
            "that's intriguing", "that's remarkable"
        }

        filtered_candidates = [
            cand for cand in candidates
            if cand and not any(phrase in cand.lower() for phrase in IGNORE_PHRASES)
        ]

        if not filtered_candidates:
            self.console.log("[LLM] All responses were generic/unhelpful, using graceful degradation.")
            # Graceful degradation: choose the least generic candidate
            if candidates:
                # Score each candidate by how many generic phrases it contains
                def generic_score(text):
                    return sum(1 for phrase in IGNORE_PHRASES if phrase in text.lower())

                # Find candidate with fewest generic phrases
                least_generic = min(candidates, key=generic_score)
                min_score = generic_score(least_generic)

                # If multiple have the same low score, choose the one with lowest repeat score
                candidates_with_min_score = [c for c in candidates if generic_score(c) == min_score]
                best_candidate = min(candidates_with_min_score, key=self._repeat_score)

                self.console.log(f"[LLM] Selected least generic candidate (score: {min_score})")
                self._remember_output(best_candidate)
                return best_candidate
            else:
                return ""  # Only return empty if no candidates at all

        best_candidate = min(filtered_candidates, key=self._repeat_score)
        self._remember_output(best_candidate)
        return best_candidate

    async def get_embedding(self, text: str) -> np.ndarray:
        if self.is_embed_placeholder:
            v_native = deterministic_embedding_stub(text, self.embed_in_dim, GLOBAL_SEED)
            v_native = self.semantics.post_embed(v_native)
            return self.embed_adapter(v_native)

        text = self.semantics.pre_embed(text)
        raw_vec = None

        # Layer 1 Bootstrap: Try deterministic embedding first if enabled
        if hasattr(self, 'embedding_bootstrap') and self.embedding_bootstrap is not None:
            try:
                bootstrap_vec = self.embedding_bootstrap.encode_text(text)
                if bootstrap_vec is not None and len(bootstrap_vec) > 0:
                    self.console.log(f"[BOOTSTRAP] Layer 1 embedding generated for: {text[:50]}...")
                    
                    # Store shell assignment for later use
                    if hasattr(self, 'embedding_bootstrap'):
                        assigned_shells = self.embedding_bootstrap.assign_shells(bootstrap_vec, text)
                        # Store assignment info for memory system
                        if not hasattr(self, '_bootstrap_shell_assignments'):
                            self._bootstrap_shell_assignments = {}
                        self._bootstrap_shell_assignments[text] = assigned_shells
                    
                    raw_vec = bootstrap_vec
                    _increment_metric('bootstrap_embeddings_used')
                else:
                    self.console.log("[BOOTSTRAP] Layer 1 embedding returned empty vector")
            except Exception as e:
                self.console.log(f"[BOOTSTRAP] Layer 1 embedding failed: {e}")

        # Fallback to LLM embedding if bootstrap not available or failed
        if raw_vec is None:
            try:
                raw_vec = await asyncio.wait_for(self.llm_client.embedding(text, model=self.embedding_model), timeout=EMBEDDING_TIMEOUT_SEC)
            except asyncio.TimeoutError:
                self.console.log("[yellow]Embedding timeout. Using fallback vector.[/yellow]")
            except Exception as e:
                self.console.log(f"[yellow]Embedding error: {e}. Using fallback vector.[/yellow]")

        if raw_vec is None:
            raw_vec = deterministic_embedding_stub(text, self.embed_in_dim, GLOBAL_SEED)

        raw_vec = self.semantics.post_embed(raw_vec)
        return self.embed_adapter(np.asarray(raw_vec, dtype=np.float32))

    async def rate_concept(self, concept_text: str, context: str = "") -> float:
        """Improved concept rating using structured multi-dimensional assessment"""
        if self.is_embed_placeholder:
            return self._normalize_rating_by_model(0.6)
            
        # Structured prompt for multi-dimensional rating
        prompt = f"""Rate this concept on four dimensions (0.0 to 1.0 each):

NOVELTY: How original, unexpected, or unique is this idea?
COHERENCE: How logical, well-formed, and meaningful is this idea?
UTILITY: How practically useful or valuable could this idea be?
CONFIDENCE: How certain are you about these ratings?

{f"Context: {context}" if context else ""}
Concept: "{concept_text}"

Respond with only: novelty=X.X coherence=Y.Y utility=Z.Z confidence=W.W
Example: novelty=0.7 coherence=0.8 utility=0.6 confidence=0.9"""

        try:
            response = await self.llm_pool.enqueue_and_wait(prompt, max_tokens=50, temperature=0.1)
            
            if isinstance(response, str) and response and not response.startswith("[LLM"):
                # Parse structured response
                components = self._parse_rating_response(response)
                if components:
                    # Use unified rating system if available
                    if hasattr(self, 'unified_rating') and self.unified_rating is not None:
                        rating = self.unified_rating.calculate_rating(
                            components, RatingTypes.CONCEPT_QUALITY
                        )
                        model_key = (self.client_model or "unknown").lower()
                        return self.unified_rating.calibrate_rating(rating, model_key)
                    else:
                        # Fallback calculation
                        return float(np.clip(
                            0.3 * components.novelty + 0.4 * components.coherence + 0.3 * components.utility,
                            0.0, 1.0
                        ))
            
            # Fallback: try old parsing method
            numbers = re.findall(r"[-+]?\d*\.\d+|\d+", response or "")
            if numbers:
                rating = float(numbers[0])
                if rating > 1.0:
                    rating = rating / 100.0
                return self._normalize_rating_by_model(rating)
                    
        except Exception as e:
            if hasattr(self, 'console'):
                self.console.log(f"[RATING] Error in rate_concept: {e}")
        
        # Final fallback
        return self._normalize_rating_by_model(0.5)
    
    def _parse_rating_response(self, response: str) -> Optional[RatingComponents]:
        """Parse structured rating response into components"""
        try:
            # Extract values using regex
            patterns = {
                'novelty': r'novelty=(\d*\.?\d+)',
                'coherence': r'coherence=(\d*\.?\d+)', 
                'utility': r'utility=(\d*\.?\d+)',
                'confidence': r'confidence=(\d*\.?\d+)'
            }
            
            values = {}
            for key, pattern in patterns.items():
                match = re.search(pattern, response.lower())
                if match:
                    values[key] = float(match.group(1))
            
            # Require at least novelty and coherence
            if 'novelty' in values and 'coherence' in values:
                return RatingComponents(
                    novelty=values.get('novelty', 0.5),
                    coherence=values.get('coherence', 0.5),
                    utility=values.get('utility', 0.5),
                    confidence=values.get('confidence', 0.7)  # Default reasonable confidence
                )
        except Exception:
            pass
        return None


    def _bh_recent_unknown_rate(self) -> float:
        """Fraction of recent validations that are unknown/needs_work."""
        try:
            G = self.memory.graph_db.graph
        except Exception:
            return 0.0
        records = []
        for nid, data in G.nodes(data=True):
            vs = data.get("validation_status", {})
            vtype = (vs.get("type") or "").lower()
            if vtype:
                step = data.get("last_step") or data.get("created_step") or -1
                records.append((int(step), vtype))
        if not records:
            return 0.0
        records.sort(key=lambda x: x[0], reverse=True)
        tail = records[:BH_SIGNAL_WINDOW]
        if not tail:
            return 0.0
        bad = sum(1 for _, t in tail if t in ("unknown", "needs_work"))
        return float(bad) / float(len(tail))

    def _update_black_hole_pressure_e8(self):
        """E8-integrated pressure calculation using dimensional shell curvature and rotor stress."""
        
        # Initialize pressure components
        geometric_pressure = 0.0
        rotor_pressure = 0.0
        shell_pressure = 0.0
        
        try:
            # (1) E8 Geometric Pressure - using lattice curvature
            recent_nodes = list(self.memory.graph_db.graph.nodes(data=True))[-64:]
            hot_nodes = [nid for nid, d in recent_nodes if d.get('temperature', 0) > 1.0]
            
            if hot_nodes:
                curvatures = []
                rotor_stresses = []
                
                for nid in hot_nodes[:32]:  # Sample for performance
                    # E8 lattice curvature
                    curvature = self.memory._calculate_e8_curvature(nid)
                    if curvature > 0:
                        curvatures.append(float(curvature))
                        
                    # Rotor stress
                    stress = self.memory._calculate_rotor_stress(nid)
                    if stress > 0:
                        rotor_stresses.append(float(stress))
                
                # Golden ratio weighting
                phi = (1 + np.sqrt(5)) / 2
                
                if curvatures:
                    max_curvature = float(np.max(curvatures))
                    geometric_pressure = max_curvature * phi
                    
                if rotor_stresses:
                    max_rotor_stress = float(np.max(rotor_stresses))
                    rotor_pressure = max_rotor_stress / phi  # Inverse scaling
                    
        except Exception:
            pass
            
        try:
            # (2) Dimensional Shell Pressure - imbalance across shells
            shell_densities = []
            for shell_dim, shell in self.dimensional_shells.items():
                if hasattr(shell, 'vectors') and shell.vectors:
                    density = len(shell.vectors) / max(100, shell_dim * 2)  # Normalize by shell capacity
                    shell_densities.append(density)
                    
            if len(shell_densities) > 1:
                # Pressure from shell imbalance (E8 prefers balance)
                shell_variance = float(np.var(shell_densities))
                shell_pressure = shell_variance * 2.0  # Scale for pressure range
                
        except Exception:
            pass
            
        # (3) Validator unknown rate (original logic)
        unknown_rate = 0.0
        try:
            unknown_rate = float(np.clip(self._bh_recent_unknown_rate(), 0.0, 1.0))
        except Exception:
            pass
            
        # (4) LLM degraded EMA (original logic) 
        degraded_pressure = 0.0
        try:
            degraded_pressure = float(np.clip(self._bh_degraded_ema, 0.0, 1.0))
        except Exception:
            pass
            
        # E8 Blended Pressure: geometric + rotor + shell + unknown + degraded
        # Weight geometric and rotor higher for E8 integration
        phi = (1 + np.sqrt(5)) / 2
        blended_pressure = (
            geometric_pressure * 0.35 +      # E8 lattice curvature (primary)
            rotor_pressure * 0.25 +          # Clifford rotor stress  
            shell_pressure * 0.20 +          # Dimensional shell imbalance
            unknown_rate * 0.10 +            # Validator uncertainty
            degraded_pressure * 0.10         # LLM degradation
        ) * phi  # Golden ratio scaling
        
        self.black_hole_pressure = float(np.clip(blended_pressure, 0.0, 2.0))

    def _update_black_hole_pressure(self):
        """Adaptive pressure = blend(density, validator-unknown rate, LLM degraded/timeout)."""
        
        # Use E8-integrated pressure if available
        if E8_LATTICE_QUANTIZATION and SPACETIME_CURVATURE_ENABLED:
            return self._update_black_hole_pressure_e8()

        # Legacy pressure calculation - kept for compatibility
        # -- (a) original density-based term (kept) --
        black_hole_pressure = 0.0
        try:
            # Get recent nodes as hot nodes proxy
            recent_nodes = list(self.memory.graph_db.graph.nodes(data=True))[-64:]
            hot_nodes = [nid for nid, d in recent_nodes if d.get('temperature', 0) > 1.5]
            if hot_nodes:
                densities = []
                for nid in hot_nodes[:64]:
                    density = self.memory._local_density(nid, radius=2)
                    if density > 0:
                        densities.append(float(density))
                if densities:
                    max_local_density = float(np.max(densities))
                    saturation_factor  = math.log1p(max(1, self.memory.graph_db.graph.number_of_nodes()) / 50.0)
                    black_hole_pressure = float(np.clip(max_local_density * saturation_factor, 0.0, 2.0))
        except Exception:
            pass

        # -- (b) validator "unknown" rate (0..1) --
        unknown_rate = 0.0
        try:
            unknown_rate = float(np.clip(self._bh_recent_unknown_rate(), 0.0, 1.0))
        except Exception:
            unknown_rate = 0.0

        # -- (c) LLM degraded/timeout EMA (0..1) from pool flag --
        try:
            degraded_now = 1.0 if getattr(self.llm_pool, "degraded", False) else 0.0  # pool returns "" on degrade in M17 patch
            self._bh_degraded_ema = 0.9 * float(self._bh_degraded_ema) + 0.1 * degraded_now
        except Exception:
            self._bh_degraded_ema = 0.0

        # Weighted blend with electromagnetic pressure contribution
        field_pressure = 0.0
        mantle = getattr(self, 'field_mantle', None) or getattr(self, 'fluid_mantle', None)
        if mantle is not None and hasattr(mantle, 'compute_pressure_proxy'):
            try:
                field_pressure = float(np.clip(mantle.compute_pressure_proxy(), 0.0, 2.0))
            except Exception:
                field_pressure = 0.0
        field_weight = float(os.getenv('E8_FIELD_PRESSURE_WEIGHT', '0.2'))
        
        # Integrate holographic stress hooks
        holographic_stress = 0.0
        if hasattr(self, 'physics') and self.physics is not None:
            try:
                # Get boundary geometry from recent memory structure
                boundary_points = []
                if hasattr(self.memory, 'get_recent_memories'):
                    recent_memories = self.memory.get_recent_memories(20)
                    for memory in recent_memories or []:
                        if hasattr(memory, 'vector') and memory.vector is not None:
                            try:
                                vec = np.array(memory.vector, dtype=float)
                                if vec.size >= 8:  # E8 dimension
                                    boundary_points.append(vec[:8])
                            except Exception:
                                pass
                
                if boundary_points:
                    # Extract bulk geometry information
                    bulk_geometry = {
                        'energy_density': field_pressure if field_pressure > 0 else 0.1,
                        'field_energy': field_pressure,
                        'memory_nodes': len(boundary_points),
                        'temperature_gradient': unknown_rate
                    }
                    
                    # Compute holographic boundary stress
                    stress_results = self.physics.compute_holographic_boundary_stress(
                        boundary_points, bulk_geometry
                    )
                    
                    if stress_results and 'energy_density' in stress_results:
                        holographic_stress = float(stress_results['energy_density'])
                        
                        # Log holographic stress contribution
                        self.console.log(f"[HOLOGRAPHIC] Boundary stress energy: {holographic_stress:.4f}")
                        
                        # Update metrics if available
                        if isinstance(getattr(self, 'm20_metrics', None), dict):
                            try:
                                self.m20_metrics['holographic_stress'] = holographic_stress
                                self.m20_metrics['boundary_nodes'] = len(boundary_points)
                            except Exception:
                                pass
            except Exception as e:
                self.console.log(f"[HOLOGRAPHIC] Stress computation failed: {e}")
                holographic_stress = 0.0
        
        holographic_weight = float(os.getenv('E8_HOLOGRAPHIC_STRESS_WEIGHT', '0.15'))
        
        blended = (
            BH_W_DENSITY * float(np.clip(black_hole_pressure, 0.0, 2.0)) +
            BH_W_UNKNOWN * unknown_rate +
            BH_W_TIMEOUT * float(np.clip(self._bh_degraded_ema, 0.0, 1.0)) +
            field_weight * field_pressure +
            holographic_weight * float(np.clip(holographic_stress, 0.0, 2.0))
        )
        self.black_hole_pressure = float(np.clip(blended, 0.0, 2.0))
        if isinstance(getattr(self, 'm20_metrics', None), dict):
            try:
                self.m20_metrics['field_pressure_proxy'] = field_pressure
            except Exception:
                pass

        # --- Soft consolidation (micro-collapse) ---
        try:
            if 0.25 <= self.black_hole_pressure < float(getattr(self, '_bh_threshold_dynamic', BH_THRESH_MAX)):
                # Light-weight potential/weight nudging instead of full collapse
                if not hasattr(self, '_last_micro_collapse_step') or (self.step_num - getattr(self, '_last_micro_collapse_step')) >= 50:
                    G = self.memory.graph_db.graph
                    # Select a few recent high-temp nodes
                    candidates = [n for n, d in list(G.nodes(data=True))[-128:] if d.get('temperature',0) > 1.2]
                    random.shuffle(candidates)
                    touched = 0
                    for nid in candidates[:8]:
                        d = G.nodes[nid]
                        # Nudge rating slightly toward mean of neighbors
                        try:
                            neigh = [G.nodes[x].get('rating',0.5) for x in G.neighbors(nid)]
                            if neigh:
                                target = sum(neigh)/len(neigh)
                                r0 = d.get('rating',0.5)
                                d['rating'] = float(np.clip(r0 + 0.05*(target - r0), 0.0, 1.0))
                                touched += 1
                        except Exception:
                            pass
                    if touched:
                        note = f"Adjusted [bold]{touched}[/] nodes (Pressure: {self.black_hole_pressure:.3f})"
                        # Panel: Micro-collapse highlight (non-blocking helper)
                        self._log_panel_nowait("🕳️ MICRO-COLLAPSE", note, style="yellow")
                        # Diary note (append lightweight)
                        try:
                            with open(get_path('micro_collapse_diary.md', self.run_id), 'a', encoding='utf-8') as fmc:
                                fmc.write(f"{_now_ts()} | step {self.step_num} | {note}\n")
                        except Exception:
                            pass
                        metrics_log('micro_collapse', {"event": "bh", "step": self.step_num, "pressure": self.black_hole_pressure, "touched": touched})
                        self._last_micro_collapse_step = self.step_num
        except Exception:
            pass

        # Debug trace - throttled to reduce spam
        import time
        current_time = time.time()
        steps_since_log = self.step_num - self._bh_last_log_step
        time_since_log = current_time - getattr(self, '_bh_last_log_time', 0)

        # Minimal anti-spam tweak:
        # Old logic logged every tick while hovering just below threshold because near-threshold clause was always true.
        # New logic:
        #  - periodic heartbeat: every 500 steps OR every 300s (extended from 60s)
        #  - OR if (near threshold @95%) AND pressure changed by >=0.02 since last log.
        # This preserves visibility of meaningful moves toward collapse without per-second noise plateau.
        near_threshold = self.black_hole_pressure >= (self._bh_threshold_dynamic * 0.95)
        delta = abs(self.black_hole_pressure - getattr(self, '_bh_last_logged_pressure', 0.0))
        periodic = (steps_since_log >= 500) or (time_since_log >= 300.0)
        significant_move = near_threshold and (delta >= 0.02)
        should_log = periodic or significant_move
        # (Reference old heuristic kept for context)
        # prev_should_log = (steps_since_log >= 500 or time_since_log >= 60.0 or
        #                    self.black_hole_pressure >= self._bh_threshold_dynamic * 0.9 or
        #                    abs(self.black_hole_pressure - getattr(self, '_bh_last_logged_pressure', 0.0)) >= 0.05)

        if should_log:
            try:
                self.console.log(
                    f"🕳️ [BH] pressure={{density:{black_hole_pressure:.3f}, unknown:{unknown_rate:.2f}, timeout_ema:{self._bh_degraded_ema:.2f}}} "
                    f"â†’ blended={self.black_hole_pressure:.3f} (thr={self._bh_threshold_dynamic:.2f}, cd={self._bh_cooldown_dynamic})"
                )
                self._bh_last_log_step = self.step_num
                self._bh_last_logged_pressure = self.black_hole_pressure
                self._bh_last_log_time = current_time
            except Exception:
                pass

        # Adaptive cadence control
        self._handle_black_hole_cycle(self.step_num)

    def _handle_black_hole_cycle(self, step: int):
        # Debounce via adaptive cooldown
        steps_since = (step - self._bh_last_event_step) if self._bh_last_event_step >= 0 else 1_000_000
        if steps_since < self._bh_cooldown_dynamic:
            return

        # Gate by adaptive threshold
        if float(self.black_hole_pressure) < float(self._bh_threshold_dynamic):
            return

        # --- Pre-check for an event horizon candidate BEFORE announcing collapse ---
        try:
            center_id, horizon_pressure = self.memory.find_event_horizon()
        except Exception as e:
            center_id, horizon_pressure = None, None
            # If find_event_horizon itself fails, we treat as no candidate.
            if self.DEBUG:
                try:
                    self.console.log(f"[BH PreCheck] find_event_horizon exception: {e}")
                except Exception:
                    pass

        if not center_id:
            # No candidate: suppress the dramatic 'Initiating collapse' message.
            # Mildly lengthen cooldown & nudge threshold upward so we don't hammer.
            self._bh_no_candidate_count = getattr(self, '_bh_no_candidate_count', 0) + 1
            self._bh_cooldown_dynamic = min(BH_COOLDOWN_MAX, int(self._bh_cooldown_dynamic * 1.15) + 3)
            # Slight upward nudge (half step) makes re-trigger require a bit more pressure.
            self._bh_threshold_dynamic = float(np.clip(self._bh_threshold_dynamic + (BH_THRESH_STEP_UP * 0.5), BH_THRESH_MIN, BH_THRESH_MAX))

            # Periodic diagnostics every 5th consecutive miss to verify horizon logic.
            if (self._bh_no_candidate_count % 5) == 0:
                temp_pass = 0
                dens_pass = 0
                both_pass = 0
                graph_iter = []
                try:
                    graph_iter = list(self.memory.graph_db.graph.nodes(data=True))
                except Exception:
                    pass
                for nid, data in graph_iter:
                    if data.get('age', 0) < 1:
                        continue
                    temp = data.get('temperature', 0.0)
                    dens = 0.0
                    try:
                        dens = self.memory._local_density(nid)
                    except Exception:
                        pass
                    t_ok = temp > 1.05
                    d_ok = dens >= 0.20
                    if t_ok:
                        temp_pass += 1
                    if d_ok:
                        dens_pass += 1
                    if t_ok and d_ok:
                        both_pass += 1
                try:
                    self.console.log(
                        f"[BH PreCheck] no candidate (count={self._bh_no_candidate_count}) Â· temp_pass={temp_pass} dens_pass={dens_pass} both={both_pass} "
                        f"cooldownâ†’{self._bh_cooldown_dynamic} thrâ†’{self._bh_threshold_dynamic:.2f}"
                    )
                except Exception:
                    pass
            return

        # Candidate exists: reset miss counter and proceed, now log an informative line.
        self._bh_no_candidate_count = 0
        try:
            self.console.log(
                f"[BH Attempt] candidate={center_id} horizon_pressure={horizon_pressure:.3f} global_pressure={self.black_hole_pressure:.3f} thr={self._bh_threshold_dynamic:.2f}"
            )
        except Exception:
            pass

        success = False
        aborted_reason = None

        # === collapse routine (center_id already known) ===
        try:
            cluster = self.memory.collect_cluster(center_id)
            need = max(2, CONSOLIDATE_MIN)

            # M23: Begin BH source reconciliation (remove pre-collapse mass)
            if hasattr(self, 'black_hole_source_manager') and self.black_hole_source_manager:
                self.black_hole_source_manager.begin_black_hole_event(cluster)

            if not cluster or len(cluster) < need:
                self.console.log(f"[BH Cycle] Cluster for '{center_id}' too small ({len(cluster)} < {need}). Attempting local padding...")
                base = set(cluster) if cluster else {center_id}

                center_vec = self.memory.main_vectors.get(center_id)
                if center_vec is not None:
                    similar_nodes = self.memory.find_similar_in_main_storage(center_vec, k=need * 3)
                    for nid, _ in similar_nodes:
                        if nid not in base and self.memory.main_vectors.get(nid) is not None:
                            base.add(nid)
                        if len(base) >= need:
                            break

                cluster = list(base)
                if len(cluster) < need:
                    aborted_reason = f"Cluster too small ({len(cluster)}) even after local padding."
                    self.console.log(f"[BH Cycle] Aborted: {aborted_reason}")
                else:
                    self.memory.fold_and_prune(cluster)
                    success = True
                    
                    # M23: Finalize BH source reconciliation (add remnant mass)
                    if hasattr(self, 'black_hole_source_manager') and self.black_hole_source_manager:
                        condensed_mass = len(cluster) * 0.8  # Estimate condensed mass
                        self.black_hole_source_manager.finalize_black_hole_remnant(center_id, condensed_mass)
            else:
                self.memory.fold_and_prune(cluster)
                success = True
                
                # M23: Finalize BH source reconciliation (add remnant mass)
                if hasattr(self, 'black_hole_source_manager') and self.black_hole_source_manager:
                    condensed_mass = len(cluster) * 0.8  # Estimate condensed mass
                    self.black_hole_source_manager.finalize_black_hole_remnant(center_id, condensed_mass)
        except Exception as e:
            aborted_reason = f"Exception: {e}"

        # === cadence bookkeeping ===
        if success:
            # record interval
            if self._bh_last_event_step >= 0:
                self._bh_event_intervals.append(steps_since)
            self._bh_last_event_step = int(step)

            # adapt cooldown to target interval
            recent = (sum(self._bh_event_intervals) / len(self._bh_event_intervals)) if self._bh_event_intervals else steps_since
            err   = float(BH_TARGET_INTERVAL - recent)
            factor = float(np.clip(1.0 + 0.5 * (err / max(10.0, BH_TARGET_INTERVAL)), 0.5, 2.0))
            new_cd = int(np.clip(int(self._bh_cooldown_dynamic * factor), BH_COOLDOWN_MIN, BH_COOLDOWN_MAX))

            # small random jitter to avoid phase-locking
            jitter = int(new_cd * BH_JITTER_FRAC * (random.random() - 0.5) * 2.0)
            self._bh_cooldown_dynamic = max(BH_COOLDOWN_MIN, min(BH_COOLDOWN_MAX, new_cd + jitter))

            # nudge threshold upward on success (be a bit harder next time)
            self._bh_threshold_dynamic = float(np.clip(self._bh_threshold_dynamic + BH_THRESH_STEP_UP, BH_THRESH_MIN, BH_THRESH_MAX))

            try:
                self.console.log(f"[BH Cadence] success Â· interval={int(recent)} Â· cooldownâ†’{self._bh_cooldown_dynamic} Â· thrâ†’{self._bh_threshold_dynamic:.2f}")
            except Exception:
                pass
        else:
            # On failure/abort, relax threshold so we can try again sooner
            self._bh_threshold_dynamic = float(np.clip(self._bh_threshold_dynamic - BH_THRESH_STEP_DOWN, BH_THRESH_MIN, BH_THRESH_MAX))
            # shorten cooldown a bit on repeated aborts
            self._bh_cooldown_dynamic = max(BH_COOLDOWN_MIN, int(self._bh_cooldown_dynamic * 0.75))

            try:
                self.console.log(f"[BH Cadence] abort ({aborted_reason}) Â· cooldownâ†’{self._bh_cooldown_dynamic} Â· thrâ†’{self._bh_threshold_dynamic:.2f}")
            except Exception:
                pass

    async def _blackhole_cycle(self, step_num: int):
        self._bh_inflight = True
        
        # Pre-black hole mass accounting for systematic physics validation
        pre_bh_mass = 0.0
        if hasattr(self, 'systematic_physics') and self.systematic_physics is not None:
            try:
                # Count semantic mass of nodes about to be absorbed
                graph = self.memory.graph_db.graph
                pre_bh_mass = float(graph.number_of_nodes())  # simplified semantic mass proxy
            except Exception:
                pre_bh_mass = 0.0
                
        try:
            center_id, pressure = self.memory.find_event_horizon()
            if not center_id:
                self.console.log("[BH Cycle] Aborted: No event horizon found.")
                return None

            cluster = self.memory.collect_cluster(center_id)
            need = max(2, CONSOLIDATE_MIN)
            if not cluster or len(cluster) < need:
                self.console.log(f"[BH Cycle] Cluster for '{center_id}' too small ({len(cluster)} < {need}). Attempting multi-anchor padding...")
                base = self.memory._pad_cluster_nodes(center_id, cluster or [center_id], min_size=need, per_anchor=4)
                cluster = list(base)
                if len(cluster) < need:
                    self.console.log(f"[BH Cycle] Aborted: Cluster too small ({len(cluster)}) even after enhanced padding.")
                    return None

            # Calculate mass that will be removed
            mass_being_removed = float(len(cluster))
            
            # Store pre-compression information for verification
            pre_compression_info = {
                'total_nodes': len(cluster),
                'total_vectors': len([v for nid in cluster if self.memory.main_vectors.get(nid) is not None]),
                'center_id': center_id,
                'pressure': pressure
            }

            remnant_data, remnant_vec, mass = await self.memory.synthesize_remnant(cluster, label_hint=f"EmergenceSeed@{step_num}")

            if not remnant_data or remnant_vec is None:
                self.console.log("[BH Cycle] Aborted: Failed to synthesize remnant.")
                return None

            # Mass injected back as remnant (simplified as 1 node)
            mass_being_injected = 1.0
            
            # Track mass flow for systematic physics validation
            if hasattr(self, 'systematic_physics') and self.systematic_physics is not None:
                try:
                    self.systematic_physics.track_mass_flow(mass_being_injected, mass_being_removed)
                    
                    # Validate mass conservation
                    conservation = self.systematic_physics.validate_mass_conservation()
                    if not conservation['valid']:
                        self.systematic_physics.log(f"Mass conservation violation: drift={conservation['drift']:.3f} (tolerance={conservation['tolerance']:.3f})", "warning")
                except Exception:
                    pass

            # Verify information preservation
            holographic_data = remnant_data.get('holographic_data', {})
            compression_ratio = holographic_data.get('compression_ratio', 1.0)
            
            if compression_ratio > 1.0:
                self.console.log(f"[BH Cycle] Holographic compression achieved: {compression_ratio:.2f}:1 ratio")
            
            # Create spacetime event with proper E8 geometry
            spacetime_signature = self._create_spacetime_signature(center_id, cluster, remnant_vec, mass)
            
            self.mood.process_event("blackhole", magnitude=float(mass))
            
            # Apply proper cosmological expansion using E8 lattice dynamics
            await self.memory._cosmological_spread(remnant_vec, mass)

            # Add spacetime curvature information to remnant
            remnant_data.update({
                "temperature": 2.0,
                "spacetime_signature": spacetime_signature,
                "holographic_data": holographic_data,
                "information_preserved": True,
                "compression_fidelity": holographic_data.get('fidelity', 1.0),
                "pre_compression_info": pre_compression_info
            })
            
            remnant_id = await self.memory.add_entry(remnant_data)
            if not remnant_id:
                return None

            self._bh_cooldown_until = step_num + BLACK_HOLE_COOLDOWN_STEPS
            
            # 🌀 CRITICAL: Perform E8 cyclic memory stitching
            # This projects the holographic remnant back into all dimensional shells
            # with proper E8 decoding - the "stitch" in the cyclic memory model
            self.memory.cyclic_memory_stitch(remnant_id, remnant_vec, holographic_data)
            
            # Create spacetime connections using proper geometric relationships
            await self._create_spacetime_connections(remnant_id, cluster, remnant_vec, spacetime_signature)

            # Fold cluster nodes with information preservation
            self.memory.fold_and_prune(cluster)

            # Create emergence seed with enhanced physics
            z8 = self._project_to_e8_boundary(remnant_vec, spacetime_signature)

            seed = EmergenceSeed(
                remnant_id=remnant_id, 
                embedding_vector=remnant_vec, 
                projected_vector=z8, 
                mass=mass, 
                absorbed_ids=cluster, 
                step_created=step_num
            )
            
            # Enhanced logging with physics information
            physics_info = f"mass={mass:.2f}, compression={compression_ratio:.1f}:1"
            if 'fidelity' in holographic_data:
                physics_info += f", fidelity={holographic_data['fidelity']:.3f}"
                
            self.console.print(Panel(
                f"Emergence Seed created at step {step_num} ({physics_info}) â€” [bold red]BLACK HOLE EVENT[/bold red]\n"
                f"Spacetime signature: {spacetime_signature.get('geometry_type', 'unknown')}", 
                border_style="red", expand=False
            ))
            
            # Additional Q(t) emergence metrics logging
            try:
                q_t_current = compute_emergence_q_t(step_num, mass, dim=32)
                s_q_global = get_s_q_global(self)
                t_prime = get_cosmic_time_parameter(step_num)
                lock_rate_512 = get_lock_rate_512(self)
                q_lock_correlation = compute_q_lock_correlation(self)
                
                # Extract causal modulation parameters from holographic data
                target_comp = holographic_data.get('causal_modulation', {}).get('target_comp', 0.8)
                beta_used = holographic_data.get('causal_modulation', {}).get('beta_used', 1.0)
                
                self.console.log(f"[BH Q(t) Metrics] Q(t)={q_t_current:.3f} sQ_global={s_q_global:.3f} t'={t_prime:.3f} lock_rate_512={lock_rate_512:.1f} Q↔LOCK_r={q_lock_correlation:.3f}")
                
                # Log cycle metrics to runtime/cycle_metrics.jsonl
                log_cycle_metrics(self, step_num, q_t_current, s_q_global, lock_rate_512, 
                                q_lock_correlation, target_comp, beta_used)
                
                # Check drift sentinels for anomaly detection
                sentinel_warnings = check_drift_sentinels(self, step_num)
                for warning in sentinel_warnings:
                    self.console.log(f"[bold red][DRIFT_SENTINEL] {warning}[/bold red]")
                    
            except Exception as e:
                self.console.log(f"[BH Q(t) Metrics] Error computing metrics: {e}")
            
            self.black_hole_pressure = 0.0
            self.black_hole_log.append({
                "type": "black_hole", 
                "step": step_num, 
                "size": len(cluster), 
                "mass": float(mass),
                "compression_ratio": compression_ratio,
                "spacetime_signature": spacetime_signature
            })
            
            return seed
        finally:
            self._bh_inflight = False

    def _build_bh_console_panel(self, step_num: int, physics_info: str, spacetime_signature: Dict, mass: float) -> str:
        """Build enhanced BH console panel with Q(t) emergence metrics."""
        try:
            # Compute Q(t) emergence metrics
            q_t_current = compute_emergence_q_t(step_num, mass, dim=32)
            s_q_global = get_s_q_global(self)
            t_prime = get_cosmic_time_parameter(step_num)
            lock_rate_512 = get_lock_rate_512(self)
            q_lock_correlation = compute_q_lock_correlation(self)
            
            # Build enhanced panel content
            panel_content = (
                f"Emergence Seed created at step {step_num} ({physics_info}) - [bold red]BLACK HOLE EVENT[/bold red]\n"
                f"Spacetime signature: {spacetime_signature.get('geometry_type', 'unknown')}\n\n"
                f"[bold cyan]=== Q(t) Emergence Metrics ===[/bold cyan]\n"
                f"Q(t) = {q_t_current:.3f}  |  sQ_global = {s_q_global:.3f}  |  t' = {t_prime:.3f}\n"
                f"lock_rate_512 = {lock_rate_512:.1f}  |  Q<->LOCK r = {q_lock_correlation:.3f}"
            )
            
            return panel_content
            
        except Exception as e:
            # Fallback to basic output
            return (
                f"Emergence Seed created at step {step_num} ({physics_info}) - [bold red]BLACK HOLE EVENT[/bold red]\n"
                f"Spacetime signature: {spacetime_signature.get('geometry_type', 'unknown')}\n"
                f"[yellow]Q(t) metrics unavailable: {e}[/yellow]"
            )

    def _create_spacetime_signature(self, center_id: str, cluster: List[str], remnant_vec: np.ndarray, mass: float) -> Dict:
        """Create spacetime curvature signature for the black hole event."""
        try:
            # Find E8 lattice position
            e8_projection = remnant_vec[:8] if remnant_vec.size >= 8 else np.zeros(8)
            center_root_idx = self.physics.find_nearest_root_index(e8_projection)
            
            # Compute Schwarzschild radius in E8 space
            schwarzschild_radius = 2.0 * mass  # Simplified: r_s = 2GM/c^2, with G=c=1
            
            # Generate Clifford rotor for spacetime curvature if available
            curvature_rotor = None
            if hasattr(self, 'dimensional_shells') and CLIFFORD_AVAILABLE:
                try:
                    shell_8d = self.dimensional_shells.get(8)
                    if shell_8d and shell_8d.rotor_generator:
                        # Angle proportional to mass (stronger curvature for larger mass)
                        curvature_angle = np.arctan(mass) * 2.0  # Scale to [0, Ï€]
                        curvature_rotor = shell_8d.rotor_generator.generate_rotor(shell_8d, curvature_angle)
                except Exception:
                    pass
            
            spacetime_signature = {
                'geometry_type': 'schwarzschild_e8',
                'center_root_idx': int(center_root_idx) if center_root_idx is not None else -1,
                'schwarzschild_radius': float(schwarzschild_radius),
                'mass': float(mass),
                'curvature_angle': float(np.arctan(mass) * 2.0),
                'e8_projection': e8_projection.tolist(),
                'cluster_size': len(cluster),
                'has_clifford_rotor': curvature_rotor is not None
            }
            
            # Attach horizon snapshot with stress and boundary field components
            try:
                if hasattr(self, 'horizon_manager') and self.horizon_manager:
                    horizon_snapshot = self.horizon_manager.get_horizon_snapshot()
                    if horizon_snapshot:
                        spacetime_signature['horizon_snapshot'] = horizon_snapshot
                        self.console.log(f"[Spacetime] Attached horizon snapshot with {len(horizon_snapshot.get('layers', {}))} layers")
            except Exception as horizon_err:
                self.console.log(f"[Spacetime] Failed to attach horizon snapshot: {horizon_err}")
            
            return spacetime_signature
            
        except Exception as e:
            self.console.log(f"[yellow]Spacetime signature creation failed: {e}[/yellow]")
            return {
                'geometry_type': 'fallback',
                'mass': float(mass),
                'cluster_size': len(cluster)
            }

    async def _create_spacetime_connections(self, remnant_id: str, cluster: List[str], remnant_vec: np.ndarray, spacetime_signature: Dict):
        """Create connections representing spacetime geometry around the black hole."""
        try:
            # Connect remnant to absorbed nodes (representing event horizon crossing)
            for nid in cluster:
                if nid == remnant_id: continue
                old_vec = self.memory.main_vectors.get(nid)
                if old_vec is not None:
                    # Information preservation connection
                    similarity = float(self.memory._cos_sim(remnant_vec, old_vec))
                    self.memory.graph_db.add_edge(
                        remnant_id, nid, 
                        type="holographic_absorption", 
                        weight=similarity,
                        spacetime_relation="event_horizon_crossing"
                    )

            # Create geodesic connections to nearby nodes in E8 space
            center_root_idx = spacetime_signature.get('center_root_idx', -1)
            if center_root_idx >= 0:
                # Find nodes along E8 geodesics
                geodesic_nodes = self._find_e8_geodesic_neighbors(center_root_idx, remnant_vec)
                
                for nid, geodesic_distance in geodesic_nodes[:BLACK_HOLE_K]:
                    if nid not in cluster and nid != remnant_id:
                        # Gravitational connection strength inversely proportional to distance
                        weight = 1.0 / (1.0 + geodesic_distance)
                        self.memory.graph_db.add_edge(
                            remnant_id, nid,
                            type="gravitational_influence",
                            weight=weight,
                            geodesic_distance=geodesic_distance,
                            spacetime_relation="curved_spacetime"
                        )

        except Exception as e:
            self.console.log(f"[yellow]Spacetime connection creation failed: {e}[/yellow]")
            # Fallback to original KNN connections
            cands = sorted([(nid, self.memory._cos_sim(remnant_vec, v)) 
                          for nid, v in self.memory.main_vectors.items() 
                          if nid != remnant_id and nid not in cluster], 
                         key=lambda t: -t[1])
            for nid, s in cands[:BLACK_HOLE_K]:
                try:
                    self._assert_writer()
                except Exception:
                    pass
                self.memory.graph_db.add_edge(remnant_id, nid, type="knn", weight=float(s))

    def _find_e8_geodesic_neighbors(self, center_root_idx: int, remnant_vec: np.ndarray) -> List[Tuple[str, float]]:
        """Find nodes along E8 geodesics from the black hole center."""
        try:
            e8_roots = getattr(self.physics, 'roots_unit', None)
            if e8_roots is None or center_root_idx >= len(e8_roots):
                return []
            
            center_root = e8_roots[center_root_idx]
            geodesic_neighbors = []
            
            # Find nodes by computing E8 geodesic distances
            for node_id, vector in self.memory.main_vectors.items():
                if vector is None or vector.size < 8:
                    continue
                    
                node_e8 = vector[:8]
                # Geodesic distance in E8 lattice space
                geodesic_dist = np.linalg.norm(node_e8 - center_root)
                
                if geodesic_dist > 0:  # Exclude center itself
                    geodesic_neighbors.append((node_id, geodesic_dist))
            
            # Sort by geodesic distance
            geodesic_neighbors.sort(key=lambda x: x[1])
            return geodesic_neighbors[:50]  # Limit for performance
            
        except Exception:
            return []

    def _project_to_e8_boundary(self, remnant_vec: np.ndarray, spacetime_signature: Dict) -> np.ndarray:
        """Project remnant to E8 boundary with spacetime curvature."""
        z8 = np.zeros(8)
        
        try:
            # Use autoencoder if available and trained
            if TORCH_AVAILABLE and self.autoencoder and self.autoencoder.is_trained:
                with torch.no_grad():
                    z8_tensor = self.autoencoder.project_to_dim(
                        torch.from_numpy(remnant_vec).float().unsqueeze(0), 8
                    )
                    if z8_tensor is not None: 
                        z8 = safe_tensor_to_numpy(z8_tensor).squeeze()
            
            # Apply spacetime curvature to projection
            if z8.size >= 8 and 'e8_projection' in spacetime_signature:
                e8_proj = np.array(spacetime_signature['e8_projection'])
                if e8_proj.size == 8:
                    # Blend autoencoder projection with E8 lattice structure
                    curvature_strength = spacetime_signature.get('mass', 1.0) / 10.0  # Scale factor
                    alpha = np.tanh(curvature_strength)  # Smooth blending
                    z8 = (1 - alpha) * z8 + alpha * e8_proj
            
            # Ensure projection lies on E8 lattice
            if z8.size >= 8:
                root_idx = self.physics.find_nearest_root_index(z8)
                if root_idx is not None:
                    e8_roots = getattr(self.physics, 'roots_unit', None)
                    if e8_roots is not None and root_idx < len(e8_roots):
                        # Final snap to lattice for geometric consistency
                        z8 = e8_roots[root_idx].copy()
                        
        except Exception as e:
            self.console.log(f"[yellow]E8 boundary projection failed: {e}[/yellow]")
            # Fallback to simple truncation
            if remnant_vec.size >= 8:
                z8 = remnant_vec[:8].copy()
        
        return z8

    def get_focus_vector(self) -> np.ndarray:
        try:
            parts = []
            weights = []
            if getattr(self, "goal_field", None) and getattr(self.goal_field, "is_initialized", False):
                for name, g in self.goal_field.goals.items():
                    emb = g.get("embedding")
                    act = float(g.get("activation", 0.0))
                    if isinstance(emb, np.ndarray) and emb.size == int(1536):
                        parts.append(emb); weights.append(max(act, 1e-6))
            if getattr(self, "memory", None) and getattr(self.memory, "_main_storage_matrix", None) is not None:
                M = self.memory._main_storage_matrix
                if isinstance(M, np.ndarray) and M.size > 0:
                    parts.append(M.mean(axis=0)); weights.append(0.25)
            if parts:
                W = np.asarray(weights, dtype=np.float32); W = W / (W.sum() + 1e-12)
                v = np.sum([w*p for w, p in zip(W, parts)], axis=0).astype(np.float32)
                n = np.linalg.norm(v) + 1e-12
                return v / n
        except Exception:
            pass
        return np.zeros(int(1536), dtype=np.float32)

    def get_memory_matrix(self):
        try:
            M = getattr(self.memory, "_main_storage_matrix", None)
            ids = getattr(self.memory, "_main_storage_ids", None)
            if isinstance(M, np.ndarray) and M.size > 0 and isinstance(ids, list) and len(ids) == M.shape[0]:
                return M.astype(np.float32), ids
        except Exception:
            pass
        return np.empty((0, int(1536)), dtype=np.float32), []

    def apply_hamiltonian_bias(self, bias: np.ndarray) -> None:
        try:
            if isinstance(bias, np.ndarray) and bias.size > 0:
                self._wavey_bias_last = bias.astype(np.float32).copy()
        except Exception:
            self._wavey_bias_last = None

    def apply_attention_weights(self, weights: np.ndarray, labels=None) -> None:
        try:
            if not isinstance(weights, np.ndarray) or weights.ndim != 1: return
            if labels is None:
                labels = getattr(self.memory, "_main_storage_ids", [])
            k = min(8, weights.shape[0])
            if k == 0: return
            idxs = np.argsort(-weights)[:k]
            for i in idxs:
                nid = labels[i] if i < len(labels) else None
                if nid:
                    self.memory.spike_temperature(nid, amount=float(weights[i]) * 0.5 + 0.05)
        except Exception:
            pass

    def _update_anchors_from_wavey(self, wavey_out: dict):
        encoder_pref = os.getenv("E8_WAVEY_ENCODER", "tiny").lower()
        def _encode_wavey_center(vec):
            comp = getattr(self.memory, "_compressor", None) if encoder_pref == "tiny" else None
            if comp and hasattr(comp, "encode"):
                try:
                    return comp.encode(vec)
                except Exception:
                    pass
            holo = getattr(self, "holo", None)
            if holo and hasattr(holo, "encode"):
                return holo.encode(vec)
            import numpy as _np
            return _np.asarray(vec, dtype=_np.float32)[:8]

        try:
            anchor_list = []
            pots = wavey_out.get("potentials") or []
            for pot in pots:
                try:
                    center = np.asarray(getattr(pot, "center", None), dtype=np.float32)
                except Exception:
                    center = None
                depth = float(getattr(pot, "depth", 0.0) or 0.0)
                if center is None or center.size == 0 or depth <= 0:
                    continue
                v8 = _encode_wavey_center(center)
                n = np.linalg.norm(v8) + 1e-12
                v8 = (v8 / n).astype(np.float32)
                anchor_list.append((v8, depth))
            if isinstance(self._wavey_bias_last, np.ndarray) and self._wavey_bias_last.size > 0:
                v8b = self.holo.encode(self._wavey_bias_last)
                nb = np.linalg.norm(v8b) + 1e-12
                v8b = (v8b / nb).astype(np.float32)
                w = float(np.linalg.norm(self._wavey_bias_last)) * 0.15
                if w > 0: anchor_list.append((v8b, w))
            if getattr(self, "goal_field", None) and getattr(self.goal_field, "is_initialized", False):
                for name, g in self.goal_field.goals.items():
                    emb = g.get("embedding")
                    act = float(g.get("activation", 0.0))
                    if isinstance(emb, np.ndarray) and emb.size > 0 and act > 0:
                        v8g = self.holo.encode(emb)
                        v8g = (v8g / (np.linalg.norm(v8g)+1e-12)).astype(np.float32)
                        anchor_list.append((v8g, 0.12 * act))
            # Avoid calling .set() when anchors subsystem is not initialized (e.g., no torch)
            if getattr(self, 'anchors', None) is not None:
                try:
                    self.anchors.set(anchor_list)
                except Exception:
                    pass
        except Exception as e:
            try:
                self.console.log(f"[Wavey] Anchor update error: {e}")
            except Exception:
                pass

    def _deprecated_cognitive_cycle(self):
        try:
            if hasattr(self, "memory") and hasattr(self.memory, "all_embeddings_matrix"):
                mats = self.memory.all_embeddings_matrix() or []
                import numpy as _np
                if isinstance(mats, list):
                    mats = _np.asarray(mats, dtype=_np.float32)
                if getattr(self, "holo", None) is not None and mats is not None and mats.size >= self.holo.out_dim:
                    self.holo.train_on_memory(mats)
        except Exception:
            pass

# --- Web Handlers (Moved outside of the E8Mind class) ---

async def shutdown_sse(app):
    clients = app.get('sse_clients')
    if not clients:
        return
    for q in list(clients):
        try:
            q.put_nowait(None)
        except Exception:
            pass

async def shutdown_market_feed(app):
    mind = app.get('mind')
    if mind and getattr(mind, "market", None):
        await mind.market.stop()

async def shutdown_ws(app):
    ws_set = app.get('ws_clients')
    if not ws_set:
        return
    for ws in list(ws_set):
        try:
            await ws.close(code=1001, message='Server shutdown')
        except Exception:
            pass

async def handle_get_graph(request):
    mind = request.app['mind']
    graph_data = export_graph(mind.memory.graph_db.graph)
    # Ensure a friendly 'label' exists for each node so the frontend can show a title.
    try:
        nodes = graph_data.get('nodes') or []
        for n in nodes:
            # If label is missing or empty, try common fallbacks (name, title) then id.
            if not n.get('label'):
                n['label'] = n.get('name') or n.get('title') or n.get('id') or '(unnamed concept)'
    except Exception:
        # Be defensive: if anything goes wrong, return the raw graph_data
        pass

    return web.Response(text=json.dumps(graph_data, cls=NumpyEncoder), content_type='application/json')

async def handle_get_node(request):
    """Get detailed information for a specific node by ID."""
    try:
        node_id = request.match_info['node_id']
        mind = request.app['mind']
        
        # Get node data from graph
        G = mind.memory.graph_db.graph
        if not G.has_node(node_id):
            return web.Response(text=json.dumps({"error": "Node not found"}), 
                               content_type='application/json', status=404)
        
        # Get full node data
        node_data = dict(G.nodes[node_id])
        node_data['id'] = node_id  # Ensure ID is included
        
        # Get connections
        neighbors = []
        for neighbor_id in G.neighbors(node_id):
            neighbor_data = dict(G.nodes[neighbor_id])
            neighbor_data['id'] = neighbor_id
            neighbors.append(neighbor_data)
        
        # Get edge information
        edges = []
        for edge in G.edges(node_id, data=True):
            edge_data = {
                'source': edge[0],
                'target': edge[1],
                'data': edge[2]
            }
            edges.append(edge_data)
        
        response_data = {
            'node': node_data,
            'neighbors': neighbors[:8],  # Limit to 8 neighbors
            'edges': edges,
            'total_neighbors': len(neighbors)
        }
        
        return web.Response(text=json.dumps(response_data, cls=NumpyEncoder), 
                           content_type='application/json')
        
    except Exception as e:
        error_response = {"error": str(e)}
        return web.Response(text=json.dumps(error_response), 
                           content_type='application/json', status=500)

async def handle_ws_telemetry(request):
    """WebSocket endpoint that streams the same telemetry payloads as SSE.

    Frontend clients send no messages or may send small pings; we ignore input
    except for clean close. We periodically push telemetry frames.
    """
    mind = request.app['mind']
    client_ip = request.remote or 'unknown'
    
    try:
        ws = web.WebSocketResponse(heartbeat=30.0, timeout=60.0)
    except TypeError:
        # Older aiohttp without heartbeat/timeout kw
        try:
            ws = web.WebSocketResponse(heartbeat=30.0)
        except TypeError:
            ws = web.WebSocketResponse()
    
    try:
        await ws.prepare(request)
    except Exception as e:
        console.log(f"[WS] Failed to prepare WebSocket for {client_ip}: {e}")
        return ws

    console.log(f"[WS] New WebSocket connection from {client_ip}")

    # Register client
    try:
        request.app['ws_clients'].add(ws)
        mind.ws_clients = request.app['ws_clients']
        console.log(f"[WS] Registered client, total WS clients: {len(request.app['ws_clients'])}")
    except Exception as e:
        console.log(f"[WS] Failed to register client {client_ip}: {e}")

    # Send an initial snapshot promptly with error handling
    try:
        snap = mind._build_telemetry_snapshot()
        await ws.send_str(json.dumps(snap, cls=NumpyEncoder, ensure_ascii=False))
        console.log(f"[WS] Sent initial telemetry to {client_ip}")
    except Exception as e:
        console.log(f"[WS] Failed to send initial snapshot to {client_ip}: {e}")

    # Reader loop (consume and ignore non-control messages)
    try:
        async for msg in ws:
            if msg.type == aiohttp.WSMsgType.TEXT:
                data = (msg.data or '').strip()
                if data.lower() in {"close", "quit", "bye"}:
                    console.log(f"[WS] Client {client_ip} requested close")
                    await ws.close()
                    break
                # Optionally handle ping messages from frontend
                elif data.lower() == "ping":
                    try:
                        await ws.send_str("pong")
                    except Exception:
                        break
                continue
            elif msg.type == aiohttp.WSMsgType.ERROR:
                console.log(f"[WS] WebSocket error from {client_ip}: {ws.exception()}")
                break
            elif msg.type == aiohttp.WSMsgType.CLOSE:
                console.log(f"[WS] WebSocket closed by {client_ip}")
                break
    except asyncio.CancelledError:
        console.log(f"[WS] WebSocket cancelled for {client_ip}")
    except Exception as e:
        console.log(f"[WS] WebSocket error for {client_ip}: {e}")
    finally:
        # Always cleanup
        try:
            request.app['ws_clients'].discard(ws)
            remaining = len(request.app['ws_clients'])
            console.log(f"[WS] Unregistered client {client_ip}, remaining WS clients: {remaining}")
        except Exception as e:
            console.log(f"[WS] Failed to unregister client {client_ip}: {e}")
        
        try:
            if not ws.closed:
                await ws.close()
        except Exception as e:
            console.log(f"[WS] Error closing WebSocket for {client_ip}: {e}")
            
    return ws

async def handle_add_concept_legacy(request):
        """Legacy endpoint alias for adding a concept (frontend compatibility)."""
        return await handle_add_concept(request)

async def handle_get_graph_summary(request):
    """Lightweight graph summary for quick frontend polling without full graph payload."""
    mind = request.app['mind']
    G = mind.memory.graph_db.graph
    try:
        node_count = G.number_of_nodes()
        edge_count = G.number_of_edges()
        recent = []
        # Use maintained deque of recent ids for O(k) access instead of O(N) list conversion
        for nid in list(mind.memory.recent_nodes)[-25:][::-1]:  # newest first
            try:
                data = G.nodes[nid]
            except Exception:
                continue
            if data.get('label'):
                recent.append({
                    'id': nid,
                    'label': data.get('label'),
                    'type': data.get('type'),
                    'rating': data.get('rating'),
                    'temperature': data.get('temperature')
                })
            if len(recent) >= 10:
                break
        return web.json_response({'nodes': node_count, 'edges': edge_count, 'recent': recent})
    except Exception as e:
        return web.json_response({'error': str(e)}, status=500)

INDEX_HTML = r"""<!DOCTYPE html>
<html lang='en'>
<head>
    <meta charset='UTF-8'/>
    <title>E8 Mind Console</title>
    <style>
        body { font-family: system-ui, Arial, sans-serif; margin:0; padding:0; background:#0d1117; color:#e6edf3; }
        header { background:#161b22; padding:12px 18px; display:flex; align-items:center; gap:18px; }
        h1 { font-size:18px; margin:0; }
        main { display:grid; grid-template-columns: 340px 1fr 340px; gap:14px; padding:14px; }
        section { background:#161b22; padding:12px; border:1px solid #30363d; border-radius:6px; overflow:auto; }
        textarea, input[type=text] { width:100%; background:#0d1117; color:#e6edf3; border:1px solid #30363d; border-radius:4px; padding:6px; font-family:inherit; }
        button { background:#238636; color:#fff; border:none; padding:6px 12px; border-radius:4px; cursor:pointer; font-weight:600; }
        button:disabled { opacity:0.5; cursor:not-allowed; }
        .small { font-size:12px; opacity:0.75; }
        #log { white-space:pre-wrap; font-size:12px; line-height:1.3; }
        #recentConcepts div { padding:4px 0; border-bottom:1px solid #30363d; }
        #recentConcepts div:last-child { border-bottom:none; }
        .badge { display:inline-block; padding:2px 6px; background:#30363d; border-radius:10px; font-size:11px; margin-left:6px; }
        #sseStatus.ok { color:#3fb950; }
        #sseStatus.err { color:#f85149; }
        .metric { font-size:12px; }
    </style>
</head>
<body>
    <header>
        <h1>E8 Mind Console</h1>
        <div class='small'>SSE: <span id='sseStatus'>connecting...</span></div>
        <div class='small'>Step: <span id='stepNum'>?</span></div>
        <div class='small'>Concepts: <span id='conceptCount'>?</span></div>
    </header>
    <main>
        <section>
            <h3>Add Concept</h3>
            <textarea id='conceptText' rows='4' placeholder='Enter a concept idea...'></textarea>
            <button id='addBtn'>Add</button>
            <div id='addStatus' class='small'></div>
            <hr/>
            <h3>Memory Search</h3>
            <input type='text' id='searchQ' placeholder='search text...' />
            <button id='searchBtn'>Search</button>
            <div id='searchResults' class='small'></div>
            <hr/>
            <h3>Recent Concepts</h3>
            <div id='recentConcepts' class='small'></div>
        </section>
        <section>
            <h3>Live Stream</h3>
            <div id='log'></div>
            <hr/>
            <h4>Curvature Overlay</h4>
            <label class='small'><input type='checkbox' id='curvToggle'/> Show curvature</label>
            <canvas id='curvCanvas' width='640' height='120' style='width:100%;height:120px;border:1px solid #30363d;display:none;'></canvas>
        </section>
        <section>
            <h3>Metrics (summary)</h3>
            <div id='metrics'></div>
            <hr/>
            <h3>Emergent</h3>
            <div class='small'>
                Region: <span id='regionVal'>?</span><br/>
                Compression Gain: <span id='compressionGainVal'>?</span><br/>
                Disagreement: <span id='disagreementVal'>?</span>
            </div>
            <hr/>
            <h3>Blueprint</h3>
            <div id='blueprint' class='small'></div>
        </section>
    </main>
<script>
const logEl = document.getElementById('log');
const sseStatus = document.getElementById('sseStatus');
const stepNum = document.getElementById('stepNum');
const conceptCount = document.getElementById('conceptCount');
const recentConcepts = document.getElementById('recentConcepts');
const metricsEl = document.getElementById('metrics');
const blueprintEl = document.getElementById('blueprint');
const searchBtn = document.getElementById('searchBtn');
const searchQ = document.getElementById('searchQ');
const searchResults = document.getElementById('searchResults');
const addBtn = document.getElementById('addBtn');
const conceptText = document.getElementById('conceptText');
const addStatus = document.getElementById('addStatus');
const regionVal = document.getElementById('regionVal');
const compressionGainVal = document.getElementById('compressionGainVal');
const disagreementVal = document.getElementById('disagreementVal');
const curvToggle = document.getElementById('curvToggle');
const curvCanvas = document.getElementById('curvCanvas');
const curvCtx = curvCanvas.getContext('2d');

function appendLog(msg){
    const atBottom = (logEl.scrollTop + logEl.clientHeight) >= (logEl.scrollHeight - 8);
    logEl.textContent += msg + "\n";
    if(atBottom){ logEl.scrollTop = logEl.scrollHeight; }
    // cap size
    if(logEl.textContent.length > 20000){
        logEl.textContent = logEl.textContent.slice(-15000);
    }
}

function connectSSE(){
    const es = new EventSource('/api/telemetry/stream');
    es.onopen = ()=>{ sseStatus.textContent='ok'; sseStatus.className='ok'; appendLog('[SSE] connected'); };
    es.onerror = ()=>{ sseStatus.textContent='err'; sseStatus.className='err'; };
    es.addEventListener('telemetry', ev=>{
        try {
            const obj = JSON.parse(ev.data);
            if(obj && obj.step!=null){ stepNum.textContent = obj.step; }
            // Update region/theme if present
            if(obj && obj.environment_theme){ regionVal.textContent = obj.environment_theme; }
            if(obj && typeof obj.compression_gain === 'number'){ compressionGainVal.textContent = obj.compression_gain.toFixed(4); }

        # Rich is used for pretty console output; provide lightweight fallbacks to avoid many
        # static-analysis unresolved-import warnings and runtime failures in headless environments.
        try:
            from rich.console import Console  # type: ignore
            from rich.panel import Panel  # type: ignore
            from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn  # type: ignore
            from rich.markup import escape  # type: ignore
        except Exception:
            # Basic text-only Console/Panel/Progress stubs
            class Panel(str):
                def __new__(cls, content, title=None, border_style=None, expand=False):
                    return str(content)

            class Console:
                def __init__(self, *a, **k):
                    pass
                def print(self, *a, **k):
                    try:
                        print(*a)
                    except Exception:
                        sys.stdout.write(str(a))
                def log(self, *a, **k):
                    try:
                        print(*a)
                    except Exception:
                        sys.stdout.write(str(a))

            def escape(s: str) -> str:
                return str(s)

            class _ProgressStub:
                def __init__(self, *a, **k): pass
                def add_task(self, *a, **k): return 0
                def start(self): pass
                def stop(self): pass
            Progress = _ProgressStub

        # Optional small utilities
        try:
            from sklearn.decomposition import PCA  # type: ignore
        except Exception:
            PCA = None  # type: ignore

        try:
            import unidecode  # type: ignore
        except Exception:
            unidecode = None  # type: ignore
            if(obj && typeof obj.disagreement === 'number'){ disagreementVal.textContent = obj.disagreement.toFixed(4); }
        } catch(e) {}
    });
    es.addEventListener('tetra_update', ev=>{
        appendLog('[TETRA] ' + ev.data.slice(0,180));
    });
    es.addEventListener('curvature', ev=>{
        try {
            const msg = JSON.parse(ev.data);
            renderCurvature(msg);
        } catch(e) {}
    });
}

async function refreshGraphSummary(){
        try:
            self.console.print(Panel(
                "Reinforcing successful nodes and applying decay to memory connections.",
                title="[bold #5F9EA0]🧠 SYNAPTIC PLASTICITY[/]",
                border_style="#5F9EA0"
            ))
        except Exception:
            pass
        conceptCount.textContent = j.nodes;
        recentConcepts.innerHTML = '';
        j.recent.forEach(c=>{
            const d = document.createElement('div');
            d.textContent = c.label || c.id;
            const b = document.createElement('span'); b.className='badge'; b.textContent=(c.temperature||0).toFixed(2);
            d.appendChild(b);
            recentConcepts.prepend(d);
        });
    }catch(e){}
}

async function refreshMetrics(){
    try{
        const r = await fetch('/metrics/summary'); if(!r.ok) return;
        const j = await r.json();
        metricsEl.innerHTML = '';
        Object.entries(j.gauges||{}).forEach(([k,v])=>{
            const p=document.createElement('div'); p.className='metric'; p.textContent=k+': '+v.toFixed(2); metricsEl.appendChild(p);
        });
    }catch(e){}
}

async function refreshBlueprint(){
    try{ const r=await fetch('/api/blueprint'); if(!r.ok) return; const j=await r.json(); blueprintEl.textContent = j.length + ' points'; }catch(e){}
}

searchBtn.onclick = async ()=>{
    const q = searchQ.value.trim(); if(!q) return;
    searchResults.textContent='Searching...';
    try{ const r=await fetch('/api/memory/search?q='+encodeURIComponent(q)+'&k=5'); const j=await r.json(); if(j.error){ searchResults.textContent=j.error; return; }
                searchResults.innerHTML = j.results.map(x=> `<div>${x.label||x.id} <span class='badge'>${(1-x.distance).toFixed(2)}</span></div>`).join('');
    }catch(e){ searchResults.textContent='Error'; }
};

addBtn.onclick = async ()=>{
    const t = conceptText.value.trim(); if(!t) return;
    addBtn.disabled=true; addStatus.textContent='Submitting...';
    try{ const r=await fetch('/api/concept/add',{method:'POST', headers:{'Content-Type':'application/json'}, body:JSON.stringify({text:t})});
             const j=await r.json(); if(j.error){ addStatus.textContent=j.error; } else { addStatus.textContent='Added '+j.node_id.slice(0,8); conceptText.value=''; refreshGraphSummary(); }
    }catch(e){ addStatus.textContent='Error'; }
    finally{ addBtn.disabled=false; }
};

function renderCurvature(msg){
    if(!curvToggle.checked){ return; }
    try{
        curvCanvas.style.display = 'block';
        const W = curvCanvas.width, H = curvCanvas.height;
        curvCtx.clearRect(0,0,W,H);
        // axes
        curvCtx.strokeStyle = '#30363d'; curvCtx.beginPath(); curvCtx.moveTo(0,H/2); curvCtx.lineTo(W,H/2); curvCtx.stroke();
        const samples = (msg && Array.isArray(msg.samples)) ? msg.samples : [];
        const mn = typeof msg.min==='number'? msg.min : -1;
        const mx = typeof msg.max==='number'? msg.max : 1;
        const rng = (mx - mn) || 1;
        const n = samples.length || 1;
        const dx = W / n;
        curvCtx.fillStyle = '#3fb950';
        for(let i=0;i<n;i++){
            const s = samples[i];
            const k = (s && typeof s.k==='number')? s.k : 0;
            const norm = (k - mn) / rng; // 0..1
            const y = H - norm*H; // invert
            const h = Math.abs(H/2 - y);
            const x = i*dx;
            // draw vertical bar from midline
            if(y < H/2){ // positive (up)
                curvCtx.fillRect(x, y, Math.max(1,dx-1), H/2 - y);
            } else { // negative (down)
                curvCtx.fillStyle = '#f85149';
                curvCtx.fillRect(x, H/2, Math.max(1,dx-1), y - H/2);
                curvCtx.fillStyle = '#3fb950';
            }
        }
    }catch(e){}
}

connectSSE();
refreshGraphSummary(); refreshMetrics(); refreshBlueprint();
setInterval(refreshGraphSummary, 8000);
setInterval(refreshMetrics, 15000);
setInterval(refreshBlueprint, 20000);
</script>
</body></html>"""

async def handle_index(request):
        """Serve inline fallback UI if no static index.html exists."""
        static_idx = os.path.join(BASE_DIR, 'static', 'index.html')
        if os.path.exists(static_idx):
                return web.FileResponse(static_idx)
        return web.Response(text=INDEX_HTML, content_type='text/html')

async def handle_get_qeng_telemetry(request):
    mind = request.app['mind']
    qeng = getattr(mind, "qeng", None)
    if qeng is None:
        return web.json_response({"error": "quantum engine not initialized"}, status=400)
    return web.json_response(qeng.telemetry_state())

async def handle_stream_telemetry(request):
    app = request.app
    q = asyncio.Queue(maxsize=16)
    app['sse_clients'].add(q)

    headers = {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'X-Accel-Buffering': 'no',
    }
    resp = web.StreamResponse(status=200, reason='OK', headers=headers)
    await resp.prepare(request)
    try:
        await resp.write(b":ok\n\n")
        # Immediately send a fresh snapshot so the frontend updates without waiting
        try:
            mind = app.get('mind')
            if mind is not None:
                init_payload = json.dumps(mind._build_telemetry_snapshot(), cls=NumpyEncoder, ensure_ascii=False)
                try:
                    q.put_nowait(init_payload)
                except asyncio.QueueFull:
                    pass
        except Exception:
            pass
        # Periodic heartbeat to keep connections alive and provide status
        async def _heartbeat_writer():
            try:
                while True:
                    await asyncio.sleep(25)
                    try:
                        hb_chunk = f"event: heartbeat\ndata: {{\"ts\": {int(time.time())}}}\n\n".encode('utf-8')
                        await resp.write(hb_chunk)
                    except Exception:
                        break
            except asyncio.CancelledError:
                pass

        hb_task = asyncio.create_task(_heartbeat_writer())
        while True:
            data = await q.get()
            if data is None:
                break
            try:
                _evt = "telemetry"
                _data_txt = data
                try:
                    _obj = json.loads(data)
                    if isinstance(_obj, dict):
                        _type = _obj.get("type")
                        if _type == "tetra_update":
                            _evt = "tetra_update"
                            _data_txt = json.dumps(_obj, ensure_ascii=False)
                        elif _type == "curvature":
                            # Stream curvature frames as a dedicated SSE event
                            _evt = "curvature"
                            _data_txt = json.dumps(_obj, ensure_ascii=False)
                except Exception:
                    pass
                chunk = f"event: {_evt}\ndata: {_data_txt}\n\n".encode('utf-8')
            except Exception:
                chunk = f"event: telemetry\ndata: {data}\n\n".encode('utf-8')
            try:
                await resp.write(chunk)
            except Exception:
                break
        hb_task.cancel()
    except (asyncio.CancelledError, ConnectionResetError, BrokenPipeError):
        pass
    finally:
        app['sse_clients'].discard(q)
        with contextlib.suppress(Exception):
            await resp.write_eof()
    return resp

async def handle_get_qeng_ablation(request):
    mind = request.app['mind']
    qeng = getattr(mind, "qeng", None)
    if qeng is None:
        return web.json_response({"error": "quantum engine not initialized"}, status=400)
    params = request.rel_url.query
    prev_idx = int(params.get('prev_idx', '0'))
    sigma_str = params.get('sigma')
    sigma = float(sigma_str) if sigma_str is not None else None
    window = int(params.get('window', '5'))
    trials = int(params.get('trials', '256'))
    res = qeng.measure_ablation(prev_idx=prev_idx, sigma=sigma, window=window, trials=trials)
    return web.json_response(res)

async def handle_get_qeng_probabilities(request):
    mind = request.app['mind']
    qeng = getattr(mind, "qeng", None)
    if qeng is None:
        return web.json_response({"error": "quantum engine not initialized"}, status=400)
    try:
        topk = int(request.rel_url.query.get("topk", "0"))
    except ValueError:
        topk = 0
    try:
        probs = None
        if hasattr(qeng, "_probs"):
            probs = qeng._probs()
        elif hasattr(qeng, "probs"):
            probs = qeng.probs()
        if probs is None:
            return web.json_response({"error": "probabilities unavailable"}, status=500)
        p = np.asarray(probs, dtype=np.float32).ravel()
        if p.size == 0:
            return web.json_response({"error": "empty probability vector"}, status=500)
        s = float(p.sum())
        if s <= 0:
            return web.json_response({"error": "invalid probability sum"}, status=500)
        p = (p / s).astype(np.float32)

        if topk > 0:
            k = min(topk, p.size)
            idxs = np.argsort(-p)[:k]
            data = [{"index": int(i), "prob": float(p[i])} for i in idxs]
            return web.json_response({"topk": k, "distribution": data})
        else:
            return web.json_response({"distribution": [float(x) for x in p.tolist()]})
    except Exception as e:
        return web.json_response({"error": str(e)}, status=500)

async def handle_memory_search(request):
    mind = request.app['mind']
    q_text = request.rel_url.query.get('q', '').strip()
    try:
        k = int(request.rel_url.query.get('k', '5'))
    except ValueError:
        k = 5
    if not q_text:
        return web.json_response({"error": "missing query param 'q'"}, status=400)
    try:
        vec = await mind.get_embedding(q_text)
    except Exception:
        # Deterministic fallback embedding
        raw = deterministic_embedding_stub(q_text, mind.embed_in_dim, GLOBAL_SEED)
        vec = mind.embed_adapter(raw)
    try:
        sims = mind.memory.find_similar_in_main_storage_e8(vec, k=k, decode_remnants=True)
    except Exception as e:
        return web.json_response({"error": f"search failed: {e}"}, status=500)
    out = []
    for nid, dist in sims:
        node = mind.memory.graph_db.get_node(nid) or {}
        out.append({
            "id": nid,
            "label": node.get("label"),
            "type": node.get("type"),
            "rating": node.get("rating"),
            "temperature": node.get("temperature"),
            "distance": float(dist)
        })
    return web.json_response({"q": q_text, "k": k, "results": out})

async def handle_get_metrics_live(request):
    mind = request.app['mind']
    metrics_file = get_path("metrics.ndjson", mind.run_id)
    if not os.path.exists(metrics_file):
        return web.json_response({"error": "metrics file not found"}, status=404)
    try:
        tail_lines = 400
        lines = []
        with open(metrics_file, "r", encoding="utf-8") as f:
            for line in f:
                lines.append(line)
                if len(lines) > tail_lines:
                    lines.pop(0)
        counters = collections.defaultdict(int)
        gauges = {}
        timings = collections.defaultdict(list)
        for ln in lines:
            ln = ln.strip()
            if not ln: continue
            try:
                rec = json.loads(ln)
            except Exception:
                continue
            t = rec.get("type")
            if t == "counter":
                counters[rec.get("name","?")] += int(rec.get("value", 0))
            elif t == "gauge":
                gauges[rec.get("name","?")] = float(rec.get("value", 0.0))
            elif t == "timing":
                timings[rec.get("name","?")].append(float(rec.get("duration_ms", 0.0)))
        timing_means = {k: (sum(v)/len(v) if v else 0.0) for k, v in timings.items()}
        return web.json_response({
            "counters": dict(counters),
            "gauges": gauges,
            "timing_means": timing_means
        })
    except Exception as e:
        return web.json_response({"error": str(e)}, status=500)

async def handle_get_metrics_summary(request):
    mind = request.app['mind']
    metrics_file = get_path("metrics.ndjson", mind.run_id)
    if not os.path.exists(metrics_file):
        return web.json_response({"error": "metrics file not found"}, status=404)

    counters = collections.defaultdict(int)
    gauges = {}
    timings = collections.defaultdict(list)

    try:
        with open(metrics_file, "r", encoding="utf-8") as f:
            for ln in f:
                ln = ln.strip()
                if not ln: continue
                try:
                    rec = json.loads(ln)
                except Exception:
                    continue
                t = rec.get("type")
                if t == "counter":
                    counters[rec.get("name","?")] += int(rec.get("value", 0))
                elif t == "gauge":
                    gauges[rec.get("name","?")] = float(rec.get("value", 0.0))
                elif t == "timing":
                    timings[rec.get("name","?")].append(float(rec.get("duration_ms", 0.0)))
        timing_stats = {}
        for name, vals in timings.items():
            if not vals: continue
            timing_stats[name] = {
                "count": len(vals),
                "mean": float(sum(vals)/len(vals)),
                "min": float(min(vals)),
                "max": float(max(vals))
            }
        return web.json_response({
            "counters": dict(counters),
            "gauges": gauges,
            "timings": timing_stats
        })
    except Exception as e:
        return web.json_response({"error": str(e)}, status=500)

async def handle_post_quantizer(request):
    mind = request.app['mind']
    try:
        data = await request.json()
    except Exception:
        return web.json_response({"error": "invalid JSON"}, status=400)
    qtype = (data.get("type") or "").lower()
    allowed = {"e8", "cubic", "random", "none"}
    if qtype not in allowed:
        return web.json_response({"error": f"invalid type; must be one of {sorted(allowed)}"}, status=400)
    mind._quantizer_override = qtype
    console.log(f"[Quantizer] override set -> {qtype}")
    return web.json_response({"status": "ok", "quantizer": qtype})

async def handle_post_snapshot(request):
    mind = request.app['mind']
    console.log("ðŸ“¸ Snapshot requested via API.")
    asyncio.create_task(mind.memory.snapshot())
    return web.json_response({"status": "ok", "message": "snapshot initiated"})

async def handle_get_state(request):
    mind = request.app['mind']
    try:
        snap = mind._build_telemetry_snapshot()
    except Exception:
        snap = {}
    state = {
        'step': getattr(mind, 'step_num', None),
        'mood': getattr(mind, 'mood', None).mood_vector if getattr(mind, 'mood', None) else None,
        'insight_reward': getattr(mind, 'last_insight_reward', None),
        'goals': getattr(mind, 'active_goals', []),
        'telemetry': snap
    }
    return web.json_response(state, dumps=lambda d: json.dumps(d, cls=NumpyEncoder))

async def handle_get_telemetry(request):
    mind = request.app['mind']
    try:
        telemetry_data = mind._build_telemetry_snapshot()
        if mind.market:
            # Ensure 'market' exists and is a dict before attaching bars
            market_obj = telemetry_data.get("market")
            if not isinstance(market_obj, dict):
                market_obj = {}
                telemetry_data["market"] = market_obj
            market_obj["bars"] = {
                "1s": {s: list(mind.market.history_1s.get(s, [])) for s in mind.market_symbols},
                "1m": {s: list(mind.market.history_1m.get(s, [])) for s in mind.market_symbols},
            }
        return web.json_response(telemetry_data, dumps=lambda d: json.dumps(d, cls=NumpyEncoder))
    except Exception as e:
        console.log(f"[Telemetry Endpoint Error] {e}")
        return web.json_response({"error": "Failed to generate telemetry"}, status=500)

async def handle_get_blueprint(request):
    return web.json_response(request.app['mind'].blueprint)

async def handle_get_bh_panel(request):
    """GET /api/bh/panel - Return latest BH console panel data."""
    mind = request.app['mind']
    try:
        # Get parameters from query string or environment
        mass = float(request.query.get('mass', os.getenv("E8_BH_PANEL_MASS", "1.0")))
        dim = int(request.query.get('dim', os.getenv("E8_BH_PANEL_DIM", "32")))
        
        # Use cached panel if available, otherwise generate fresh
        panel_data = getattr(mind, '_last_bh_panel', None)
        if not panel_data:
            panel_data = mind.bh_panel_snapshot(mass=mass, dim=dim)
        
        return web.json_response(panel_data)
    except Exception as e:
        console.log(f"[BH Panel API Error] {e}")
        return web.json_response({"error": "Failed to generate BH panel data"}, status=500)

async def handle_get_metrics_recent(request):
    """GET /api/metrics/recent - Return recent metrics for charting."""
    try:
        # Get recent metrics from the metrics log file
        metrics_file = os.path.join(RUNTIME_DIR, "metrics.ndjson")
        recent_metrics = []
        
        if os.path.exists(metrics_file):
            try:
                # Read last N lines from metrics file
                max_lines = int(request.query.get('limit', '100'))
                with open(metrics_file, 'r') as f:
                    lines = f.readlines()
                    for line in lines[-max_lines:]:
                        try:
                            metric = json.loads(line.strip())
                            recent_metrics.append(metric)
                        except json.JSONDecodeError:
                            continue
            except Exception as e:
                console.log(f"[Metrics Read Error] {e}")
        
        return web.json_response({
            "metrics": recent_metrics,
            "count": len(recent_metrics)
        })
    except Exception as e:
        console.log(f"[Metrics API Error] {e}")
        return web.json_response({"error": "Failed to retrieve recent metrics"}, status=500)


async def handle_get_sdi_branch(request):
    """GET /api/sdi/branch - Return branch head and recent commits."""
    mind = request.app['mind']
    try:
        try:
            limit = int(request.query.get('limit', '20'))
        except Exception:
            limit = 20
        limit = max(1, min(limit, 100))
        head = sdi.get_branch_head(mind.run_id)
        commits = sdi.tail_commits(mind.run_id, limit)
        return web.json_response({
            'head': head,
            'last_commits': commits,
        })
    except Exception as err:
        try:
            mind.console.log(f"[SDI] branch handler failed: {err}")
        except Exception:
            pass
        return web.json_response({"error": "Failed to load SDI branch"}, status=500)


async def handle_get_sdi_commits(request):
    """GET /api/sdi/commits - Return recent commits for current run."""
    mind = request.app['mind']
    try:
        try:
            limit = int(request.query.get('limit', '20'))
        except Exception:
            limit = 20
        limit = max(1, min(limit, 200))
        commits = sdi.tail_commits(mind.run_id, limit)
        return web.json_response({'commits': commits})
    except Exception as err:
        try:
            mind.console.log(f"[SDI] commits handler failed: {err}")
        except Exception:
            pass
        return web.json_response({"error": "Failed to load SDI commits"}, status=500)


async def handle_get_sdi_capsules(request):
    """GET /api/sdi/capsules - Return recent capsules for current run."""
    mind = request.app['mind']
    try:
        try:
            limit = int(request.query.get('limit', '20'))
        except Exception:
            limit = 20
        limit = max(1, min(limit, 200))
        capsules = sdi.tail_capsules(mind.run_id, limit)
        return web.json_response({'capsules': capsules})
    except Exception as err:
        try:
            mind.console.log(f"[SDI] capsules handler failed: {err}")
        except Exception:
            pass
        return web.json_response({"error": "Failed to load SDI capsules"}, status=500)


async def handle_add_concept(request):
    mind = request.app['mind']
    try:
        data = await request.json()
        text = data.get("text")
        if not text: return web.json_response({"error": "Text is required"}, status=400)
        rating = await mind.rate_concept(text)
        entry = {"type": "external_concept", "label": sanitize_line(text, 25), "metaphor": text, "rating": rating, "step": mind.step_num}
        node_id = await mind.memory.add_entry(entry)
        return web.json_response({"node_id": node_id, "message": "Concept added successfully"})
    except json.JSONDecodeError:
        return web.json_response({"error": "Invalid JSON"}, status=400)
    except Exception as e:
        return web.json_response({"error": str(e)}, status=500)

async def handle_trigger_dream(request):
    mind = request.app['mind']
    asyncio.create_task(mind.dream_engine.run_dream_sequence())
    return web.json_response({"status": "Dream sequence initiated"})

def _collect_config_from_user():
    # Prefer environment variables for non-interactive operation
    cfg = {}
    env_provider = os.getenv('E8_LLM_PROVIDER')
    if env_provider:
        # Allow values: 'openai', 'ollama', 'gemini' or numeric '1','2','3'
        if env_provider.lower() in ('1', 'openai'):
            provider_choice = '1'
        elif env_provider.lower() in ('2', 'ollama'):
            provider_choice = '2'
        elif env_provider.lower() in ('3', 'gemini'):
            provider_choice = '3'
        else:
            provider_choice = '1'
        cfg['provider_choice'] = provider_choice
        if provider_choice == '1':
            cfg['openai_api_key'] = os.getenv('E8_OPENAI_API_KEY', os.getenv('OPENAI_API_KEY', ''))
            cfg['openai_model_name'] = os.getenv('E8_OPENAI_MODEL', os.getenv('OPENAI_MODEL', 'gpt-4-turbo-preview'))
        elif provider_choice == '2':
            cfg['ollama_model_name'] = os.getenv('E8_OLLAMA_MODEL', os.getenv('OLLAMA_MODEL', 'llama3'))
        elif provider_choice == '3':
            cfg['gemini_api_key'] = os.getenv('E8_GEMINI_API_KEY', os.getenv('GEMINI_API_KEY', ''))
            cfg['gemini_model_name'] = os.getenv('E8_GEMINI_MODEL', os.getenv('GEMINI_MODEL', 'gemini-1.5-flash'))
        cfg['use_local_mix'] = os.getenv('E8_USE_LOCAL_MIX', '0') == '1'
        if cfg['use_local_mix']:
            cfg['local_model_name'] = os.getenv('E8_LOCAL_MODEL_NAME', os.getenv('LOCAL_MODEL_NAME', 'phi3:mini-4k'))
        return cfg

    # Interactive fallback
    print("Choose LLM provider:\n1. OpenAI\n2. Ollama (local)\n3. Gemini API")
    provider_choice = input("Enter choice (1, 2, or 3) [1]: ") or "1"
    cfg = {"provider_choice": provider_choice}
    if provider_choice == "1":
        cfg["openai_api_key"] = (input("OpenAI API Key: ") or "").strip()
        cfg["openai_model_name"] = (input("OpenAI model [gpt-4-turbo-preview]: ") or "gpt-4-turbo-preview").strip()
    elif provider_choice == "2":
        cfg["ollama_model_name"] = (input("Ollama model [llama3]: ") or "llama3").strip()
    elif provider_choice == "3":
        cfg["gemini_api_key"] = (input("Gemini API Key: ") or "").strip()
        cfg["gemini_model_name"] = (input("Gemini model [gemini-1.5-flash]: ") or "gemini-1.5-flash").strip()
    else:
        print("Invalid choice. Running with LLM stub.")
    use_local = (input("Augment with a local tiny-LLM via Ollama? (y/N): ") or "n").strip().lower() == "y"
    cfg["use_local_mix"] = bool(use_local)
    if use_local:
        cfg["local_model_name"] = (input("Local Ollama model [phi3:mini-4k]: ") or "phi3:mini-4k").strip()
    return cfg

# --- GLOBAL CONFIGURATION VARIABLES ---
llm_client = None
model_name = "stub"
embedding_model = "stub"
IS_EMBED_PLACEHOLDER = True
LLM_PROVIDER = "stub"
provider_native_embed_dim = 1536


async def main():
    """Main function to initialize and run the E8Mind server."""
    global llm_client, model_name, embedding_model, IS_EMBED_PLACEHOLDER, LLM_PROVIDER, provider_native_embed_dim

    run_id = get_run_id()

    try:
        seed_all(GLOBAL_SEED)
    except Exception:
        pass
    if os.getenv("E8_PROVIDER", "").strip().lower() in ("", "ask"):
        cfg = _collect_config_from_user()
        pc = str(cfg.get("provider_choice", "")).strip()
        if pc == "1":
            LLM_PROVIDER = "openai"
            api_key = cfg.get("openai_api_key") or os.getenv("OPENAI_API_KEY")
            if not api_key: raise ValueError("OPENAI_API_KEY not set.")
            llm_client, model_name, embedding_model = AsyncOpenAIClient(api_key, console), cfg.get("openai_model_name") or "gpt-4-turbo-preview", "text-embedding-3-small"
            IS_EMBED_PLACEHOLDER = False
        elif pc == "2":
            LLM_PROVIDER, model_name = "ollama", cfg.get("ollama_model_name") or os.getenv("OLLAMA_MODEL", "llama3")
            llm_client, embedding_model = OllamaClient(model_name, console), "nomic-embed-text"
            IS_EMBED_PLACEHOLDER = False
        elif pc == "3":
            LLM_PROVIDER = "gemini"
            api_key = cfg.get("gemini_api_key") or os.getenv("GEMINI_API_KEY")
            if not api_key: raise ValueError("GEMINI_API_KEY not set.")
            model_name = cfg.get("gemini_model_name") or "gemini-1.5-flash"
            llm_client, embedding_model = GeminiClient(api_key, model_name, console), "models/embedding-001"
            IS_EMBED_PLACEHOLDER = False
        else:
            LLM_PROVIDER, IS_EMBED_PLACEHOLDER = "stub", True
    else:
        LLM_PROVIDER = os.getenv("E8_PROVIDER", "stub").lower()
        if LLM_PROVIDER == "openai":
            api_key = os.getenv("OPENAI_API_KEY");
            if not api_key: raise ValueError("OPENAI_API_KEY not set.")
            llm_client, model_name, embedding_model = AsyncOpenAIClient(api_key, console), os.getenv("OPENAI_MODEL", "gpt-4-turbo-preview"), "text-embedding-3-small"
            IS_EMBED_PLACEHOLDER = False
        elif LLM_PROVIDER == "ollama":
            model_name = os.getenv("OLLAMA_MODEL", "llama3")
            llm_client, embedding_model = OllamaClient(model_name, console), "nomic-embed-text"
            IS_EMBED_PLACEHOLDER = False
        elif LLM_PROVIDER == "gemini":
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key: raise ValueError("GEMINI_API_KEY not set.")
            model_name = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")
            llm_client, embedding_model = GeminiClient(api_key, model_name, console), "models/embedding-001"
            IS_EMBED_PLACEHOLDER = False
        else:
            LLM_PROVIDER, IS_EMBED_PLACEHOLDER = "stub", True

    if IS_EMBED_PLACEHOLDER:
        class StubClient:
            def __init__(self, console): self.console = console
            async def chat(self, *a, **k): return "This is a placeholder response from a stubbed LLM."
            async def embedding(self, *a, **k): return np.random.randn(provider_native_embed_dim)
            async def batch_embedding(self, texts, *a, **k): return [np.random.randn(provider_native_embed_dim) for _ in texts]
        llm_client, model_name, embedding_model = StubClient(console), "stub", "stub"

    _test_vec = await llm_client.embedding("adapter_probe")
    if isinstance(_test_vec, dict) and "embedding" in _test_vec: _test_vec = _test_vec["embedding"]
    if isinstance(_test_vec, list) and _test_vec and isinstance(_test_vec[0], (list, np.ndarray)): _test_vec = _test_vec[0]
    embed_in_dim = int(len(_test_vec))
    if embed_in_dim > 1: provider_native_embed_dim = embed_in_dim
    console.log(f"[INIT] Detected provider embedding dimension: {provider_native_embed_dim}")

    profile_name = os.getenv("MIND_PROFILE", "default")
    sem_plugin, _ = load_profile(profile_name)
    probe_native = np.zeros(provider_native_embed_dim, dtype=np.float32)
    
    try:
        # Properly instantiate the semantics object if it's a plugin module
        if hasattr(sem_plugin, 'PLUGIN'):
            sem = sem_plugin.PLUGIN()
        else:
            sem = sem_plugin
        
        probe_post = sem.post_embed(probe_native)
        adapter_in_dim = int(np.asarray(probe_post, dtype=np.float32).size)
        console.log(f"[INIT] post_embed output dim: {adapter_in_dim} (provider {provider_native_embed_dim})")
    except Exception as e:
        adapter_in_dim = provider_native_embed_dim
        console.log(f"[INIT] post_embed probe failed: {e}. Falling back to provider dim.")

    embed_adapter = UniversalEmbeddingAdapter(adapter_in_dim, EMBED_DIM)
    console.log(f"[INIT] Universal Embedding Adapter created: {adapter_in_dim} -> {EMBED_DIM}")

    mind = E8Mind(
        semantic_domain_val=SEMANTIC_DOMAIN,
        run_id=run_id,
        llm_client_instance=llm_client,
        client_model=model_name,
        embedding_model_name=embedding_model,
        embed_adapter=embed_adapter,
        embed_in_dim=provider_native_embed_dim,
        console=console,
        is_embed_placeholder=IS_EMBED_PLACEHOLDER
    )

    # inserted: seed domain concept if requested
    try:
        await mind.seed_domain_if_empty()
    except Exception as _e:
        console.log(f"[seed] seed_domain_if_empty error: {_e}")
    
    # Initialize EmergenceController
    try:
        emergence_controller = mind.create_emergence_controller()
        console.log(f"[INIT] EmergenceController initialized in {emergence_controller.emergence_mode} mode")
    except Exception as e:
        console.log(f"[INIT] Failed to initialize EmergenceController: {e}")
        emergence_controller = None
    
    try:
        cfg = locals().get("cfg", {})
        if cfg.get("use_local_mix") and ollama is not None:
            local_model = cfg.get("local_model_name") or "phi3:mini-4k"
            mind.local_llm_client = OllamaClient(local_model, console)
            mind.local_llm_model = local_model
            console.log(f"[LLM MIX] Local tiny-LLM enabled via Ollama model='{local_model}'.")
        else:
            console.log("[LLM MIX] Local tiny-LLM disabled or not available.")
    except Exception as e:
        console.log(f"[LLM MIX] Failed to init local tiny-LLM: {e}")
    # Require aiohttp for the HTTP server
    if web is None:
        console.log("[bold yellow]aiohttp not installed. Skipping server startup; core mind initialized headlessly.[/bold yellow]")
        return
    # Build the HTTP app via modular adapter (no behavior change)
    try:
        from e8.api.server import create_app  # type: ignore
    except Exception:
        create_app = None  # type: ignore
    if create_app is None:
        console.log("[bold yellow]API adapter unavailable; falling back to inline app setup is not supported in this build.[/bold yellow]")
        return
    app = create_app(mind, console=console)
    if app is None:
        return

    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', 7870)
    await site.start()
    console.log(f"[bold green]E8 Mind Server running at http://localhost:7870[/bold green]")
    console.log(f"Run ID: {run_id}")

    max_steps_env = os.getenv("E8_MAX_STEPS", "")
    try:
        max_steps = int(max_steps_env) if max_steps_env else 297600
    except ValueError:
        console.log(f"[bold yellow]Invalid E8_MAX_STEPS='{max_steps_env}', using default 297600[/bold yellow]")
        max_steps = 297600
    
    # Debug: Check all methods and class internals
    console.log(f"[DEBUG] mind type: {type(mind)}")
    try:
        methods = [m for m in dir(mind) if not m.startswith('_')]
        console.log(f"[DEBUG] dir(mind)[:40]: {methods[:40]}")
        console.log(f"[DEBUG] has run_cognitive_cycle: {hasattr(mind, 'run_cognitive_cycle')}")
        # Inspect class dict and MRO
        cls = type(mind)
        console.log(f"[DEBUG] E8Mind __mro__: {[c.__name__ for c in cls.__mro__]}")
        try:
            console.log(f"[DEBUG] E8Mind.__dict__ keys: {list(cls.__dict__.keys())[:40]}")
        except Exception as e:
            console.log(f"[DEBUG] failed to list E8Mind.__dict__: {e}")
        # Check attribute lookup via getattr
        try:
            val = getattr(mind, 'run_cognitive_cycle')
            console.log(f"[DEBUG] getattr(mind,'run_cognitive_cycle') -> {val}")
        except Exception as e:
            console.log(f"[DEBUG] getattr failed: {e}")
    except Exception as e:
        console.log(f"[DEBUG] listing methods failed: {e}")

    # Methods previously defined at module level have been moved into E8Mind class.
    console.log("[INFO] Using source-level E8Mind methods; dynamic binding removed.")
    # Safety fallback: if a module-level function exists (leftover), bind it to the class so
    # older releases that reference the function still work until source-level changes settle.
    try:
        if not hasattr(mind, 'run_cognitive_cycle'):
            module_fn = globals().get('run_cognitive_cycle')
            if callable(module_fn):
                setattr(type(mind), 'run_cognitive_cycle', module_fn)
                console.log('[FIX] Bound module-level run_cognitive_cycle to E8Mind (fallback)')
        if not hasattr(mind, '_ensure_console_export_state'):
            module_fn2 = globals().get('_ensure_console_export_state')
            if callable(module_fn2):
                setattr(type(mind), '_ensure_console_export_state', module_fn2)
                console.log('[FIX] Bound module-level _ensure_console_export_state to E8Mind (fallback)')
        # Targeted bind for other known helpers used by the cognitive loop
        for helper_name in ('_reset_tick_train_counter', 'seed_domain_if_empty', '_export_console_chunk'):
            try:
                if not hasattr(type(mind), helper_name):
                    h = globals().get(helper_name)
                    if callable(h):
                        setattr(type(mind), helper_name, h)
                        console.log(f'[FIX] Bound module-level {helper_name} to E8Mind (fallback)')
            except Exception:
                pass
    except Exception:
        pass

    # Additional debug: check reset counter availability
    try:
        cls = type(mind)
        console.log(f"[DEBUG] class has _reset_tick_train_counter: {hasattr(cls, '_reset_tick_train_counter')}")
        console.log(f"[DEBUG] instance has _reset_tick_train_counter: {hasattr(mind, '_reset_tick_train_counter')}")
        if hasattr(cls, '_reset_tick_train_counter'):
            try:
                code_obj = getattr(cls, '_reset_tick_train_counter').__code__
                console.log(f"[DEBUG] _reset_tick_train_counter defined at line: {code_obj.co_firstlineno}")
            except Exception:
                pass
    except Exception:
        pass

    # Start curvature streamer in background
    try:
        curvature_task = asyncio.create_task(mind._run_curvature_stream())
    except Exception:
        curvature_task = None

    cycle_task = asyncio.create_task(mind.run_cognitive_cycle(max_steps=max_steps))
    try:
        await cycle_task
    finally:
        # Stop curvature streamer
        try:
            if curvature_task:
                curvature_task.cancel()
        except Exception:
            pass
        await runner.cleanup()


if __name__ == "__main__":
    try:
        # Optional one-shot demo for Ray Alert UI (no side effects on normal runs)
        try:
            if os.getenv("E8_UI_RAY_ALERTS_DEMO", "0") == "1":
                demo = build_alert_entry(
                    source_label="Quantum Phase",
                    target_label="Causal Lattice",
                    source_dim=16,
                    target_dim=8,
                    distance=0.3421,
                    tier="strong",
                    score=0.88,
                    hypothesis="Energy gradients align with E8 lattice orientation.",
                    bulk_src=[0.1, 0.0, -0.2, 0.05, 0.02, -0.08, 0.11, -0.03],
                    bulk_tgt=[0.12, -0.02, -0.1, 0.03, 0.01, -0.09, 0.10, -0.02],
                )
                render_ray_alert(demo, console=console)
        except Exception:
            pass
        asyncio.run(main())
    except KeyboardInterrupt:
        console.log("\n[bold yellow]Keyboard interrupt received. Shutting down.[/bold yellow]")
    except Exception as e:
        console.log(f"[bold red]CRITICAL ERROR in main: {e}[/bold red]")
        console.print_exception()
